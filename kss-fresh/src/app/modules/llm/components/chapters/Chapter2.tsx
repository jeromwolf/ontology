'use client';

import Link from 'next/link';
import { Code, FlaskConical } from 'lucide-react';
import References from '@/components/common/References';

export default function Chapter2() {
  return (
    <div className="space-y-8">
      <section>
        <h2 className="text-2xl font-bold text-indigo-800 dark:text-indigo-200 mb-4 flex items-center gap-2">
          <Code className="w-6 h-6" />
          Transformer ì•„í‚¤í…ì²˜ ì™„ì „ ë¶„ì„
        </h2>
        <div className="bg-indigo-50 dark:bg-indigo-900/20 rounded-xl p-6 mb-6">
          <p className="text-lg text-gray-700 dark:text-gray-300 mb-4">
            TransformerëŠ” "Attention Is All You Need" ë…¼ë¬¸ì—ì„œ ì†Œê°œëœ í˜ì‹ ì ì¸ ì•„í‚¤í…ì²˜ë¡œ, 
            í˜„ì¬ ëª¨ë“  LLMì˜ ê¸°ë°˜ì´ ë˜ê³  ìˆìŠµë‹ˆë‹¤.
          </p>
        </div>
      </section>

      <section>
        <h3 className="text-xl font-semibold text-gray-900 dark:text-white mb-4">Transformer ì „ì²´ êµ¬ì¡°</h3>
        
        {/* Transformer Architecture Diagram */}
        <div className="bg-white dark:bg-gray-800 p-6 rounded-xl border mb-8">
          <img 
            src="/images/llm/transformer-architecture.png" 
            alt="Transformer Architecture" 
            className="w-full max-w-3xl mx-auto rounded-lg shadow-lg"
          />
          <p className="text-sm text-gray-600 dark:text-gray-400 text-center mt-4">
            Transformer ì•„í‚¤í…ì²˜ (Attention Is All You Need, 2017)
          </p>
        </div>
        
        {/* Transformer 3D ì‹œë®¬ë ˆì´í„° ë§í¬ */}
        <div className="mb-6 p-4 bg-gradient-to-r from-purple-50 to-pink-50 dark:from-purple-900/20 dark:to-pink-900/20 rounded-xl">
          <div className="flex items-center justify-between">
            <div>
              <h4 className="font-semibold text-purple-900 dark:text-purple-200 mb-1">ğŸ® Transformer 3D ì‹œë®¬ë ˆì´í„°</h4>
              <p className="text-sm text-gray-600 dark:text-gray-400">
                Transformer ì•„í‚¤í…ì²˜ì˜ êµ¬ì¡°ë¥¼ 3Dë¡œ ì‹œê°í™”í•˜ê³  íƒêµ¬í•´ë³´ì„¸ìš”
              </p>
            </div>
            <Link 
              href="/modules/llm/simulators/transformer-architecture"
              className="inline-flex items-center gap-2 px-4 py-2 bg-purple-600 text-white rounded-lg hover:bg-purple-700 transition-colors"
            >
              <FlaskConical className="w-4 h-4" />
              ì‹œë®¬ë ˆì´í„° ì‹¤í–‰
            </Link>
          </div>
        </div>
        
        <h3 className="text-xl font-semibold text-gray-900 dark:text-white mb-4 mt-8">í•µì‹¬ êµ¬ì„± ìš”ì†Œ</h3>
        
        <div className="grid md:grid-cols-2 gap-6 mb-8">
          <div className="bg-blue-50 dark:bg-blue-900/20 p-6 rounded-xl">
            <h4 className="font-bold text-blue-800 dark:text-blue-200 mb-3">ğŸ”µ Encoder (ì™¼ìª½)</h4>
            <ul className="space-y-2 text-sm text-gray-700 dark:text-gray-300">
              <li>â€¢ ì…ë ¥ ë¬¸ì¥ì„ ì´í•´í•˜ê³  ì¸ì½”ë”©</li>
              <li>â€¢ Self-Attentionìœ¼ë¡œ ë¬¸ë§¥ íŒŒì•…</li>
              <li>â€¢ ê° í† í°ì´ ë‹¤ë¥¸ ëª¨ë“  í† í°ì„ ë³¼ ìˆ˜ ìˆìŒ</li>
              <li>â€¢ 6ê°œ ë ˆì´ì–´ë¡œ êµ¬ì„± (NÃ—)</li>
            </ul>
          </div>
          
          <div className="bg-green-50 dark:bg-green-900/20 p-6 rounded-xl">
            <h4 className="font-bold text-green-800 dark:text-green-200 mb-3">ğŸŸ¢ Decoder (ì˜¤ë¥¸ìª½)</h4>
            <ul className="space-y-2 text-sm text-gray-700 dark:text-gray-300">
              <li>â€¢ ì¶œë ¥ ë¬¸ì¥ì„ ìƒì„±</li>
              <li>â€¢ Masked Self-Attention ì‚¬ìš©</li>
              <li>â€¢ ë¯¸ë˜ í† í°ì€ ë³¼ ìˆ˜ ì—†ìŒ (ìê¸°íšŒê·€ì )</li>
              <li>â€¢ Encoder ì¶œë ¥ì„ ì°¸ì¡° (Cross-Attention)</li>
            </ul>
          </div>
        </div>
        
        <div className="grid md:grid-cols-2 gap-6">
          <div className="space-y-4">
            <div className="bg-white dark:bg-gray-800 p-4 rounded-lg border">
              <h4 className="font-semibold text-indigo-600 dark:text-indigo-400 mb-2">Self-Attention</h4>
              <p className="text-sm text-gray-600 dark:text-gray-400">
                ê° í† í°ì´ ë‹¤ë¥¸ ëª¨ë“  í† í°ê³¼ì˜ ê´€ê³„ë¥¼ ë™ì‹œì— ê³„ì‚°
              </p>
            </div>
            <div className="bg-white dark:bg-gray-800 p-4 rounded-lg border">
              <h4 className="font-semibold text-indigo-600 dark:text-indigo-400 mb-2">Multi-Head Attention</h4>
              <p className="text-sm text-gray-600 dark:text-gray-400">
                ì—¬ëŸ¬ ê°œì˜ attention headë¡œ ë‹¤ì–‘í•œ ê´€ê³„ íŒ¨í„´ í¬ì°©
              </p>
            </div>
            <div className="bg-white dark:bg-gray-800 p-4 rounded-lg border">
              <h4 className="font-semibold text-indigo-600 dark:text-indigo-400 mb-2">Feed Forward Network</h4>
              <p className="text-sm text-gray-600 dark:text-gray-400">
                ê° ìœ„ì¹˜ì—ì„œ ë…ë¦½ì ìœ¼ë¡œ ì ìš©ë˜ëŠ” ì™„ì „ì—°ê²°ì¸µ
              </p>
            </div>
          </div>
          <div className="space-y-4">
            <div className="bg-white dark:bg-gray-800 p-4 rounded-lg border">
              <h4 className="font-semibold text-indigo-600 dark:text-indigo-400 mb-2">Layer Normalization</h4>
              <p className="text-sm text-gray-600 dark:text-gray-400">
                ê° ì¸µì˜ ì¶œë ¥ì„ ì•ˆì •í™”í•˜ì—¬ ê¹Šì€ ë„¤íŠ¸ì›Œí¬ í•™ìŠµ ê°€ëŠ¥
              </p>
            </div>
            <div className="bg-white dark:bg-gray-800 p-4 rounded-lg border">
              <h4 className="font-semibold text-indigo-600 dark:text-indigo-400 mb-2">Residual Connection</h4>
              <p className="text-sm text-gray-600 dark:text-gray-400">
                ê·¸ë˜ë””ì–¸íŠ¸ ì†Œì‹¤ ë¬¸ì œ í•´ê²°ê³¼ í•™ìŠµ ì•ˆì •ì„± í–¥ìƒ
              </p>
            </div>
            <div className="bg-white dark:bg-gray-800 p-4 rounded-lg border">
              <h4 className="font-semibold text-indigo-600 dark:text-indigo-400 mb-2">Positional Encoding</h4>
              <p className="text-sm text-gray-600 dark:text-gray-400">
                ìˆœì„œ ì •ë³´ë¥¼ ëª¨ë¸ì— ì œê³µí•˜ëŠ” ìœ„ì¹˜ ì¸ì½”ë”©
              </p>
            </div>
          </div>
        </div>
      </section>

      <section>
        <h3 className="text-xl font-semibold text-gray-900 dark:text-white mb-4">Attention ë©”ì»¤ë‹ˆì¦˜ ìˆ˜ì‹</h3>
        
        {/* Attention Visualizer ì‹œë®¬ë ˆì´í„° ë§í¬ */}
        <div className="mb-6 p-4 bg-gradient-to-r from-blue-50 to-indigo-50 dark:from-blue-900/20 dark:to-indigo-900/20 rounded-xl">
          <div className="flex items-center justify-between">
            <div>
              <h4 className="font-semibold text-blue-900 dark:text-blue-200 mb-1">ğŸ® Attention ë©”ì»¤ë‹ˆì¦˜ ì‹œê°í™”</h4>
              <p className="text-sm text-gray-600 dark:text-gray-400">
                Self-Attentionì´ ë‹¨ì–´ ê°„ì˜ ê´€ê³„ë¥¼ ì–´ë–»ê²Œ í•™ìŠµí•˜ëŠ”ì§€ ì‹œê°ì ìœ¼ë¡œ íƒêµ¬í•´ë³´ì„¸ìš”
              </p>
            </div>
            <Link 
              href="/modules/llm/simulators/attention-visualizer"
              className="inline-flex items-center gap-2 px-4 py-2 bg-blue-600 text-white rounded-lg hover:bg-blue-700 transition-colors"
            >
              <FlaskConical className="w-4 h-4" />
              ì‹œë®¬ë ˆì´í„° ì‹¤í–‰
            </Link>
          </div>
        </div>
        
        <div className="bg-gray-50 dark:bg-gray-800 p-6 rounded-lg">
          <div className="space-y-4">
            <div>
              <h4 className="font-semibold mb-2">Self-Attention ê³„ì‚°</h4>
              <div className="bg-white dark:bg-gray-900 p-4 rounded border font-mono text-sm">
                Attention(Q, K, V) = softmax(QK^T / âˆšd_k)V
              </div>
            </div>
            <div>
              <h4 className="font-semibold mb-2">Multi-Head Attention</h4>
              <div className="bg-white dark:bg-gray-900 p-4 rounded border font-mono text-sm">
                MultiHead(Q, K, V) = Concat(head_1, ..., head_h)W^O
              </div>
            </div>
          </div>
        </div>
      </section>

      <References
        sections={[
          {
            title: 'ì›ë³¸ ë…¼ë¬¸ (Original Papers)',
            icon: 'paper',
            color: 'border-indigo-500',
            items: [
              {
                title: 'Attention Is All You Need',
                authors: 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin',
                year: '2017',
                description: 'Transformer ì•„í‚¤í…ì²˜ë¥¼ ìµœì´ˆë¡œ ì œì•ˆí•œ ì—­ì‚¬ì  ë…¼ë¬¸',
                link: 'https://arxiv.org/abs/1706.03762'
              },
              {
                title: 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding',
                authors: 'Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova',
                year: '2018',
                description: 'Transformer Encoder ê¸°ë°˜ ì–‘ë°©í–¥ ì‚¬ì „í•™ìŠµ ëª¨ë¸',
                link: 'https://arxiv.org/abs/1810.04805'
              },
              {
                title: 'Language Models are Unsupervised Multitask Learners',
                authors: 'Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever',
                year: '2019',
                description: 'GPT-2: Transformer Decoder ê¸°ë°˜ ìƒì„± ëª¨ë¸ì˜ ë°œì „',
                link: 'https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf'
              },
              {
                title: 'Language Models are Few-Shot Learners',
                authors: 'Tom B. Brown et al.',
                year: '2020',
                description: 'GPT-3: 175B íŒŒë¼ë¯¸í„° ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸',
                link: 'https://arxiv.org/abs/2005.14165'
              }
            ]
          },
          {
            title: 'ê¸°ìˆ  ë¶„ì„ ìë£Œ (Technical Resources)',
            icon: 'book',
            color: 'border-blue-500',
            items: [
              {
                title: 'The Illustrated Transformer',
                authors: 'Jay Alammar',
                year: '2018',
                description: 'Transformerë¥¼ ì‹œê°ì ìœ¼ë¡œ ì´í•´í•˜ê¸° ì‰½ê²Œ ì„¤ëª…í•œ í•„ë… ìë£Œ',
                link: 'https://jalammar.github.io/illustrated-transformer/'
              },
              {
                title: 'The Annotated Transformer',
                authors: 'Harvard NLP Group',
                year: '2018',
                description: 'ë…¼ë¬¸ì˜ ì½”ë“œ êµ¬í˜„ì„ ë¼ì¸ë³„ë¡œ ì„¤ëª…í•œ ìƒì„¸ ê°€ì´ë“œ',
                link: 'http://nlp.seas.harvard.edu/annotated-transformer/'
              },
              {
                title: 'Formal Algorithms for Transformers',
                authors: 'Mary Phuong, Marcus Hutter',
                year: '2022',
                description: 'Transformerì˜ ìˆ˜í•™ì  ì•Œê³ ë¦¬ì¦˜ì„ ì—„ë°€í•˜ê²Œ ì •ë¦¬í•œ ë…¼ë¬¸',
                link: 'https://arxiv.org/abs/2207.09238'
              }
            ]
          },
          {
            title: 'í•™ìŠµ ìë£Œ (Learning Resources)',
            icon: 'web',
            color: 'border-purple-500',
            items: [
              {
                title: 'Stanford CS224N: Natural Language Processing with Deep Learning',
                description: 'Transformerì™€ NLPì˜ ê¸°ì´ˆë¶€í„° ê³ ê¸‰ê¹Œì§€ ë‹¤ë£¨ëŠ” ìŠ¤íƒ í¬ë“œ ê°•ì˜',
                link: 'http://web.stanford.edu/class/cs224n/'
              },
              {
                title: 'Hugging Face Transformers Documentation',
                description: 'ì‹¤ë¬´ì—ì„œ ê°€ì¥ ë§ì´ ì‚¬ìš©ë˜ëŠ” Transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ ê³µì‹ ë¬¸ì„œ',
                link: 'https://huggingface.co/docs/transformers/index'
              },
              {
                title: 'Transformers from Scratch',
                authors: 'Peter Bloem',
                description: 'Transformerë¥¼ ì²˜ìŒë¶€í„° êµ¬í˜„í•˜ë©° ë°°ìš°ëŠ” íŠœí† ë¦¬ì–¼',
                link: 'https://peterbloem.nl/blog/transformers'
              },
              {
                title: 'Attention? Attention!',
                authors: 'Lilian Weng',
                description: 'Attention ë©”ì»¤ë‹ˆì¦˜ì˜ ë°œì „ ê³¼ì •ì„ ì •ë¦¬í•œ í¬ê´„ì  ë¸”ë¡œê·¸',
                link: 'https://lilianweng.github.io/posts/2018-06-24-attention/'
              }
            ]
          }
        ]}
      />
    </div>
  )
}