import { Module, Chapter } from '@/types/module'

export const llmModule: Module = {
  id: 'llm',
  name: 'Large Language Models',
  nameKo: 'LLM ëŒ€í˜• ì–¸ì–´ ëª¨ë¸',
  description: 'ëŒ€í˜• ì–¸ì–´ ëª¨ë¸ì˜ í•µì‹¬ ê°œë…ë¶€í„° ì‹¤ì „ í™œìš©ê¹Œì§€',
  version: '1.0.0',
  difficulty: 'intermediate',
  estimatedHours: 20,
  icon: 'ðŸ¤–',
  color: '#6366f1',
  
  prerequisites: [],
  
  chapters: [
    {
      id: '01-introduction',
      title: 'LLM ê°œìš”ì™€ ì—­ì‚¬',
      description: 'LLMì˜ íƒ„ìƒ ë°°ê²½ê³¼ ë°œì „ ê³¼ì •',
      estimatedMinutes: 45,
      keywords: ['transformer', 'GPT', 'BERT', 'ì–¸ì–´ëª¨ë¸', 'NLP'],
      learningObjectives: [
        'LLMì˜ ì •ì˜ì™€ íŠ¹ì§• ì™„ì „ ì´í•´',
        'Transformer ì•„í‚¤í…ì²˜ ê¸°ë³¸ ê°œë… íŒŒì•…',
        'ì£¼ìš” LLM ëª¨ë¸ë“¤ì˜ ì°¨ì´ì ê³¼ íŠ¹ì§• ë¶„ì„',
        'ì–¸ì–´ëª¨ë¸ì˜ ì—­ì‚¬ì  ë°œì „ ê³¼ì • ì´í•´',
        'LLMì´ ê°€ì ¸ì˜¨ AI íŒ¨ëŸ¬ë‹¤ìž„ ë³€í™” ì¸ì‹'
      ]
    },
    {
      id: '02-architecture',
      title: 'Transformer ì•„í‚¤í…ì²˜ ì™„ì „ ë¶„ì„',
      description: 'LLMì˜ í•µì‹¬ì¸ Transformer êµ¬ì¡°ì™€ ë™ìž‘ì›ë¦¬ ì™„ì „ í•´ë¶€',
      estimatedMinutes: 75,
      keywords: ['attention', 'encoder', 'decoder', 'self-attention', 'multi-head', 'positional-encoding'],
      learningObjectives: [
        'Attention ë©”ì»¤ë‹ˆì¦˜ì˜ ìˆ˜í•™ì  ì›ë¦¬ ì´í•´',
        'Multi-Head Attentionì˜ ë³‘ë ¬ ì²˜ë¦¬ ë°©ì‹',
        'Encoder-Decoder êµ¬ì¡°ì˜ ì •ë³´ íë¦„ íŒŒì•…',
        'Positional Encodingì˜ í•„ìš”ì„±ê³¼ êµ¬í˜„',
        'Layer Normalizationê³¼ Residual Connection',
        'Feed Forward Networkì˜ ì—­í• ê³¼ êµ¬ì¡°'
      ]
    },
    {
      id: '03-training',
      title: 'ëª¨ë¸ í•™ìŠµê³¼ì •ê³¼ ìµœì í™”',
      description: 'ì‚¬ì „í›ˆë ¨, íŒŒì¸íŠœë‹, RLHFê¹Œì§€ ì „ì²´ í•™ìŠµ íŒŒì´í”„ë¼ì¸',
      estimatedMinutes: 65,
      keywords: ['pretraining', 'fine-tuning', 'RLHF', 'tokenization', 'optimization', 'scaling-law'],
      learningObjectives: [
        'ì‚¬ì „í›ˆë ¨ ê³¼ì •ê³¼ ëŒ€ê·œëª¨ ë°ì´í„°ì…‹ ì²˜ë¦¬',
        'ë‹¤ì–‘í•œ íŒŒì¸íŠœë‹ ê¸°ë²• (Supervised, In-context)',
        'RLHF (ì¸ê°„ í”¼ë“œë°± ê°•í™”í•™ìŠµ) ì›ë¦¬ì™€ ì ìš©',
        'Tokenization ì „ëžµê³¼ Vocabulary êµ¬ì„±',
        'Scaling Lawì™€ ëª¨ë¸ í¬ê¸°ì˜ ê´€ê³„',
        'í•™ìŠµ íš¨ìœ¨ì„±ì„ ìœ„í•œ ìµœì í™” ê¸°ë²•ë“¤'
      ]
    },
    {
      id: '04-prompt-engineering',  
      title: 'í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ ë§ˆìŠ¤í„°',
      description: 'LLMê³¼ íš¨ê³¼ì ìœ¼ë¡œ ì†Œí†µí•˜ëŠ” í”„ë¡¬í”„íŠ¸ ì„¤ê³„ ê¸°ë²•',
      estimatedMinutes: 55,
      keywords: ['prompt', 'few-shot', 'chain-of-thought', 'role-playing', 'context'],
      learningObjectives: [
        'Zero-shot vs Few-shot vs Many-shot í”„ë¡¬í”„íŒ…',
        'Chain-of-Thought (CoT) ì¶”ë¡  ê¸°ë²•',
        'Role-playingê³¼ Persona ì„¤ì •',
        'Context Length ìµœì í™” ì „ëžµ',
        'í”„ë¡¬í”„íŠ¸ ì¸ì ì…˜ ë°©ì–´ì™€ ì•ˆì „ì„±',
        'ë„ë©”ì¸ë³„ í”„ë¡¬í”„íŠ¸ íŒ¨í„´ê³¼ í…œí”Œë¦¿'
      ]
    },
    {
      id: '05-applications-1',
      title: 'LLM ì‹¤ì „ í™œìš©: RAGì™€ ì±—ë´‡',
      description: 'RAG ì‹œìŠ¤í…œê³¼ ê³ ê¸‰ ì±—ë´‡ ê°œë°œ ì‹¤ìŠµ',
      estimatedMinutes: 45,
      keywords: ['RAG', 'chatbot', 'vector-db', 'conversation', 'langchain'],
      learningObjectives: [
        'RAG (Retrieval Augmented Generation) ì‹œìŠ¤í…œ ì´í•´',
        'ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ì™€ ìž„ë² ë”© í™œìš©',
        'LangChainì„ ì´ìš©í•œ RAG êµ¬í˜„',
        'ê³ ë„í™”ëœ ì±—ë´‡ ì•„í‚¤í…ì²˜ ì„¤ê³„',
        'ëŒ€í™” ìƒíƒœ ê´€ë¦¬ì™€ ê°œì¸í™”',
        'ì±—ë´‡ ì˜¤ë¥˜ ì²˜ë¦¬ ë° ì—ìŠ¤ì»¬ë ˆì´ì…˜'
      ]
    },
    {
      id: '05-applications-2',
      title: 'LLM ì‹¤ì „ í™œìš©: ì½”ë“œì™€ ë¬¸ì„œ',
      description: 'ì½”ë“œ ìƒì„± ìžë™í™”ì™€ ë¬¸ì„œ ì²˜ë¦¬ ê¸°ë²•',
      estimatedMinutes: 40,
      keywords: ['code-generation', 'copilot', 'summarization', 'extraction', 'parsing'],
      learningObjectives: [
        'AI íŽ˜ì–´ í”„ë¡œê·¸ëž˜ë° êµ¬í˜„',
        'ì½”ë“œ ë¦¬ë·°ì™€ ë¦¬íŒ©í† ë§ ìžë™í™”',
        'ë¬¸ì„œ ìš”ì•½ê³¼ í•µì‹¬ ì •ë³´ ì¶”ì¶œ',
        'êµ¬ì¡°í™”ëœ ë°ì´í„° íŒŒì‹±',
        'ëŒ€ëŸ‰ ë¬¸ì„œ ë°°ì¹˜ ì²˜ë¦¬',
        'PDF, Word ë“± ë‹¤ì–‘í•œ í˜•ì‹ ì²˜ë¦¬'
      ]
    },
    {
      id: '05-applications-3',
      title: 'LLM ì‹¤ì „ í™œìš©: ë²ˆì—­ê³¼ ì½˜í…ì¸ ',
      description: 'ë‹¤êµ­ì–´ ë²ˆì—­ê³¼ ì°½ìž‘ ì½˜í…ì¸  ìƒì„±',
      estimatedMinutes: 35,
      keywords: ['translation', 'localization', 'content-generation', 'marketing', 'creative'],
      learningObjectives: [
        'ê³ í’ˆì§ˆ ë‹¤êµ­ì–´ ë²ˆì—­ ì‹œìŠ¤í…œ',
        'ë¬¸í™”ì  ë§¥ë½ì„ ê³ ë ¤í•œ í˜„ì§€í™”',
        'ë§ˆì¼€íŒ… ì¹´í”¼ ìžë™ ìƒì„±',
        'SEO ìµœì í™” ì½˜í…ì¸  ìž‘ì„±',
        'ì°½ìž‘ ì½˜í…ì¸  íŒŒì´í”„ë¼ì¸',
        'A/B í…ŒìŠ¤íŠ¸ìš© ë³€í˜• ìƒì„±'
      ]
    },
    {
      id: '06-advanced',
      title: 'ê³ ê¸‰ ê¸°ë²•ê³¼ ìµœì‹  ë™í–¥',
      description: 'ìµœì‹  LLM ì—°êµ¬ì™€ ê³ ê¸‰ í™œìš© ê¸°ë²•ë“¤',
      estimatedMinutes: 80,
      keywords: ['multimodal', 'diffusion', 'mamba', 'huggingface', 'efficiency'],
      learningObjectives: [
        'Multimodal LLM (GPT-4V, Claude Vision, Gemini)',
        'Diffusion Models (Stable Diffusion, DALL-E 3)',
        'ì°¨ì„¸ëŒ€ ì•„í‚¤í…ì²˜ (Mamba, RWKV, Flash Attention)',
        'Hugging Face ìƒíƒœê³„ í™œìš©',
        'Parameter Efficient Fine-tuning (LoRA, QLoRA)',
        'AI ì„œë¹„ìŠ¤ ìƒíƒœê³„ì™€ ê¸°ì—… API'
      ]
    },
    {
      id: '07-huggingface',
      title: 'Hugging Face ì‹¤ì „ í™œìš©',
      description: 'í—ˆê¹…íŽ˜ì´ìŠ¤ í”Œëž«í¼ìœ¼ë¡œ ëª¨ë¸ ê°œë°œë¶€í„° ë°°í¬ê¹Œì§€',
      estimatedMinutes: 60,
      keywords: ['transformers', 'datasets', 'spaces', 'autotrain', 'inference-api'],
      learningObjectives: [
        'Transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ë¡œ ëª¨ë¸ ë¡œë“œ/íŒŒì¸íŠœë‹',
        'Datasetsë¡œ ë°ì´í„°ì…‹ ì²˜ë¦¬',
        'Tokenizersë¡œ ì»¤ìŠ¤í…€ í† í¬ë‚˜ì´ì € êµ¬ì¶•',
        'Spacesë¡œ ë°ëª¨ ì•± ë°°í¬',
        'AutoTrainìœ¼ë¡œ No-code í•™ìŠµ',
        'Inference API í™œìš©ë²•'
      ]
    },
    {
      id: '08-ai-services',  
      title: 'AI ì„œë¹„ìŠ¤ì™€ API í™œìš©',
      description: 'ì£¼ìš” AI ê¸°ì—… APIì™€ ì„œë¹„ìŠ¤ í™œìš©ë²•',
      estimatedMinutes: 50,
      keywords: ['openai-api', 'claude-api', 'vertex-ai', 'bedrock', 'langchain'],
      learningObjectives: [
        'OpenAI API (GPT-4, DALL-E, Whisper)',
        'Anthropic Claude API í™œìš©',
        'Google Vertex AI & Gemini API',
        'AWS Bedrock ë©€í‹°ëª¨ë¸ í”Œëž«í¼',
        'Azure OpenAI Service',
        'LangChainìœ¼ë¡œ API í†µí•©'
      ]
    }
  ],
  
  simulators: [
    {
      id: 'tokenizer',
      name: 'í† í¬ë‚˜ì´ì € ì‹œë®¬ë ˆì´í„°',
      description: 'ë‹¤ì–‘í•œ í† í¬ë‚˜ì´ì €(GPT, Claude, Gemini)ì˜ í…ìŠ¤íŠ¸ ë¶„í•  ê³¼ì • ë¹„êµ',
      component: 'TokenizerSimulator'
    },
    {
      id: 'attention',
      name: 'Attention ë©”ì»¤ë‹ˆì¦˜ ì‹œê°í™”',
      description: 'Self-Attentionê³¼ Multi-Head Attentionì˜ ì‹¤ì‹œê°„ ë™ìž‘ ê³¼ì •',
      component: 'AttentionVisualizer'
    },
    {
      id: 'transformer',
      name: 'Transformer ì•„í‚¤í…ì²˜ 3D',
      description: 'ì¸ì½”ë”-ë””ì½”ë” êµ¬ì¡°ë¥¼ 3Dë¡œ íƒí—˜í•˜ë©° ë°ì´í„° íë¦„ ì¶”ì ',
      component: 'TransformerArchitecture3D'
    },
    {
      id: 'training',
      name: 'ëª¨ë¸ í•™ìŠµ ì‹œë®¬ë ˆì´í„°',
      description: 'ì‚¬ì „í›ˆë ¨ë¶€í„° íŒŒì¸íŠœë‹ê¹Œì§€ ì „ì²´ í•™ìŠµ ê³¼ì •ì„ ë‹¨ê³„ë³„ë¡œ ì²´í—˜',
      component: 'TrainingSimulator'
    },
    {
      id: 'prompt-playground',
      name: 'í”„ë¡¬í”„íŠ¸ í”Œë ˆì´ê·¸ë¼ìš´ë“œ',
      description: 'ë‹¤ì–‘í•œ í”„ë¡¬í”„íŠ¸ ê¸°ë²•ì„ ì‹¤ì‹œê°„ìœ¼ë¡œ í…ŒìŠ¤íŠ¸í•˜ê³  ê²°ê³¼ ë¹„êµ',
      component: 'PromptPlayground'
    },
    {
      id: 'model-comparison',
      name: 'LLM ëª¨ë¸ ë¹„êµê¸°',
      description: 'Claude Opus 4, GPT-4o, Grok 4, Gemini 2.5, Llama 3.3 ë“± ìµœì‹  ëª¨ë¸ ë¹„êµ',
      component: 'ModelComparison'
    }
  ],
  
  tools: [
    {
      id: 'prompt-playground',
      name: 'í”„ë¡¬í”„íŠ¸ í”Œë ˆì´ê·¸ë¼ìš´ë“œ',
      description: 'ë‹¤ì–‘í•œ í”„ë¡¬í”„íŠ¸ ê¸°ë²• ì‹¤ìŠµ',
      url: '/modules/llm/tools/prompt-playground'
    }
  ]
}

export const getChapter = (chapterId: string): Chapter | undefined => {
  return llmModule.chapters.find(chapter => chapter.id === chapterId)
}

export const getNextChapter = (currentChapterId: string): Chapter | undefined => {
  const currentIndex = llmModule.chapters.findIndex(ch => ch.id === currentChapterId)
  return currentIndex < llmModule.chapters.length - 1 ? llmModule.chapters[currentIndex + 1] : undefined
}

export const getPrevChapter = (currentChapterId: string): Chapter | undefined => {
  const currentIndex = llmModule.chapters.findIndex(ch => ch.id === currentChapterId)
  return currentIndex > 0 ? llmModule.chapters[currentIndex - 1] : undefined
}