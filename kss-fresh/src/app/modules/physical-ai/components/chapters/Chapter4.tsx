'use client';

import React from 'react';
import { Brain, Gamepad2, Target, Zap, TrendingUp, RefreshCw, Award } from 'lucide-react';

export default function Chapter4() {
  return (
    <div className="prose prose-lg dark:prose-invert max-w-none">
      {/* Hero Section */}
      <div className="bg-gradient-to-r from-orange-50 to-red-50 dark:from-orange-900/20 dark:to-red-900/20 rounded-2xl p-8 mb-8 border border-orange-200 dark:border-orange-800">
        <div className="flex items-center gap-4 mb-4">
          <div className="w-12 h-12 bg-orange-500 rounded-xl flex items-center justify-center">
            <Brain className="w-6 h-6 text-white" />
          </div>
          <h1 className="text-3xl font-bold text-gray-900 dark:text-white m-0">ê°•í™”í•™ìŠµê³¼ ë¡œë´‡ ì œì–´</h1>
        </div>
        <p className="text-xl text-gray-700 dark:text-gray-300 m-0">
          ì‹œí–‰ì°©ì˜¤ë¥¼ í†µí•œ í•™ìŠµ - ë¡œë´‡ì´ ìŠ¤ìŠ¤ë¡œ ìµœì ì˜ í–‰ë™ì„ ì°¾ëŠ”ë‹¤
        </p>
      </div>

      {/* Introduction */}
      <section className="my-8">
        <h2 className="flex items-center gap-2">
          <Gamepad2 className="text-orange-600" />
          ê°•í™”í•™ìŠµì´ë€?
        </h2>

        <div className="bg-gradient-to-r from-orange-50 to-yellow-50 dark:from-orange-900/20 dark:to-yellow-900/20 p-6 rounded-lg border-l-4 border-orange-500 mb-6">
          <h3 className="text-xl font-bold mb-4">ğŸ® ê²Œì„ì²˜ëŸ¼ í•™ìŠµí•˜ëŠ” AI</h3>
          <p className="mb-4">
            <strong>ê°•í™”í•™ìŠµ (Reinforcement Learning)</strong>ì€ AIê°€ <strong>ë³´ìƒê³¼ ë²Œì </strong>ì„ í†µí•´
            ìŠ¤ìŠ¤ë¡œ ìµœì ì˜ ì „ëµì„ ì°¾ì•„ê°€ëŠ” í•™ìŠµ ë°©ë²•ì…ë‹ˆë‹¤. ë§ˆì¹˜ ê²Œì„ì„ ë°˜ë³µí•˜ë©° ê³ ìˆ˜ê°€ ë˜ëŠ” ê²ƒê³¼ ê°™ìŠµë‹ˆë‹¤.
          </p>

          <div className="grid md:grid-cols-3 gap-4">
            <div className="bg-white dark:bg-gray-800 p-4 rounded-lg text-center">
              <div className="text-3xl mb-2">ğŸ¯</div>
              <h4 className="font-bold text-sm mb-2">ëª©í‘œ (Goal)</h4>
              <p className="text-xs">ìµœëŒ€í•œ ë†’ì€ ë³´ìƒ íšë“</p>
            </div>
            <div className="bg-white dark:bg-gray-800 p-4 rounded-lg text-center">
              <div className="text-3xl mb-2">ğŸ”„</div>
              <h4 className="font-bold text-sm mb-2">ì‹œí–‰ì°©ì˜¤ (Trial & Error)</h4>
              <p className="text-xs">ì‹¤íŒ¨í•˜ë©° ë°°ìš´ë‹¤</p>
            </div>
            <div className="bg-white dark:bg-gray-800 p-4 rounded-lg text-center">
              <div className="text-3xl mb-2">ğŸ†</div>
              <h4 className="font-bold text-sm mb-2">ìµœì í™” (Optimization)</h4>
              <p className="text-xs">ì ì  ë‚˜ì•„ì§€ëŠ” ì „ëµ</p>
            </div>
          </div>
        </div>

        <div className="bg-white dark:bg-gray-800 rounded-lg p-6 shadow-lg mb-6">
          <h4 className="font-bold mb-3">ğŸ“š ì§€ë„í•™ìŠµ vs ê°•í™”í•™ìŠµ</h4>
          <div className="grid md:grid-cols-2 gap-6">
            <div className="bg-blue-50 dark:bg-blue-900/20 p-4 rounded-lg">
              <h5 className="font-bold mb-2 text-blue-600">ì§€ë„í•™ìŠµ (Supervised Learning)</h5>
              <ul className="text-sm space-y-2">
                <li>âœ… ì •ë‹µì´ ì£¼ì–´ì§„ ë°ì´í„°ë¡œ í•™ìŠµ</li>
                <li>âœ… "ì´ ì´ë¯¸ì§€ëŠ” ê³ ì–‘ì´ì…ë‹ˆë‹¤"</li>
                <li>âŒ ìƒˆë¡œìš´ ìƒí™©ì— ì•½í•¨</li>
                <li>ğŸ¯ ìš©ë„: ì´ë¯¸ì§€ ë¶„ë¥˜, ë²ˆì—­</li>
              </ul>
            </div>
            <div className="bg-orange-50 dark:bg-orange-900/20 p-4 rounded-lg border-2 border-orange-500">
              <h5 className="font-bold mb-2 text-orange-600">ê°•í™”í•™ìŠµ (Reinforcement Learning)</h5>
              <ul className="text-sm space-y-2">
                <li>âœ… ë³´ìƒìœ¼ë¡œë§Œ í•™ìŠµ (ì •ë‹µ ì—†ìŒ)</li>
                <li>âœ… "ì´ í–‰ë™ì´ ì¢‹ì•˜ë‚˜? ë‚˜ë¹´ë‚˜?"</li>
                <li>âœ… ìƒˆë¡œìš´ ìƒí™©ì— ì ì‘ ê°€ëŠ¥</li>
                <li>ğŸ¯ ìš©ë„: ë¡œë´‡ ì œì–´, ê²Œì„ AI</li>
              </ul>
            </div>
          </div>
        </div>
      </section>

      {/* 1. Q-Learning */}
      <section className="my-8">
        <h2 className="flex items-center gap-2">
          <Target className="text-blue-600" />
          1. Q-Learning - ê°€ì¥ ê¸°ë³¸ì ì¸ ê°•í™”í•™ìŠµ
        </h2>

        <div className="bg-white dark:bg-gray-800 rounded-lg p-6 shadow-lg mb-6">
          <h3 className="text-xl font-bold mb-4">ğŸ§® Q-Table: ëª¨ë“  ìƒí™©ë³„ ìµœì„ ì˜ í–‰ë™ ì €ì¥</h3>

          <div className="bg-blue-50 dark:bg-blue-900/20 p-5 rounded-lg mb-6">
            <h4 className="font-bold mb-3">Q-Learning í•µì‹¬ ê°œë…</h4>
            <div className="space-y-3 text-sm">
              <div>
                <strong className="text-blue-600">â€¢ State (ìƒíƒœ)</strong>: ë¡œë´‡ì´ ì²˜í•œ í˜„ì¬ ìƒí™©
                <p className="mt-1 ml-4">ì˜ˆ: "ë¡œë´‡ íŒ”ì´ ë¬¼ì²´ë¡œë¶€í„° 10cm ë–¨ì–´ì ¸ ìˆìŒ"</p>
              </div>
              <div>
                <strong className="text-green-600">â€¢ Action (í–‰ë™)</strong>: ë¡œë´‡ì´ ì·¨í•  ìˆ˜ ìˆëŠ” ë™ì‘
                <p className="mt-1 ml-4">ì˜ˆ: "ì™¼ìª½ìœ¼ë¡œ 5cm ì´ë™", "ê·¸ë¦¬í¼ ë‹«ê¸°"</p>
              </div>
              <div>
                <strong className="text-purple-600">â€¢ Reward (ë³´ìƒ)</strong>: í–‰ë™ì˜ ê²°ê³¼ì— ëŒ€í•œ ì ìˆ˜
                <p className="mt-1 ml-4">ì˜ˆ: ë¬¼ì²´ë¥¼ ì„±ê³µì ìœ¼ë¡œ ì¡ìœ¼ë©´ +10, ë–¨ì–´ëœ¨ë¦¬ë©´ -5</p>
              </div>
              <div>
                <strong className="text-orange-600">â€¢ Q-Value</strong>: íŠ¹ì • ìƒíƒœì—ì„œ íŠ¹ì • í–‰ë™ì˜ ê¸°ëŒ€ ë³´ìƒ
                <p className="mt-1 ml-4">Q(ìƒíƒœ, í–‰ë™) = "ì´ ìƒí™©ì—ì„œ ì´ í–‰ë™ì´ ì–¼ë§ˆë‚˜ ì¢‹ì€ê°€?"</p>
              </div>
            </div>
          </div>

          <div className="bg-gray-900 text-gray-100 p-4 rounded-lg overflow-x-auto mb-4">
            <pre className="text-sm">
{`# Q-Learning: ë¡œë´‡ íŒ”ì´ ë¬¼ì²´ ì¡ê¸° í•™ìŠµ
import numpy as np

class RobotArmQLearning:
    def __init__(self, num_positions=10, num_actions=4):
        # Q-Table ì´ˆê¸°í™” (ìƒíƒœ x í–‰ë™)
        self.q_table = np.zeros((num_positions, num_actions))

        # í•˜ì´í¼íŒŒë¼ë¯¸í„°
        self.learning_rate = 0.1  # Î±: ì–¼ë§ˆë‚˜ ë¹¨ë¦¬ í•™ìŠµ?
        self.discount_factor = 0.99  # Î³: ë¯¸ë˜ ë³´ìƒì„ ì–¼ë§ˆë‚˜ ì¤‘ì‹œ?
        self.epsilon = 1.0  # íƒìƒ‰ í™•ë¥  (ì´ˆê¸°ì—” ëœë¤)
        self.epsilon_decay = 0.995
        self.epsilon_min = 0.01

        # í–‰ë™: 0=ì™¼ìª½, 1=ì˜¤ë¥¸ìª½, 2=ì¡ê¸°, 3=ë†“ê¸°
        self.actions = ['LEFT', 'RIGHT', 'GRASP', 'RELEASE']

    def choose_action(self, state):
        # Îµ-greedy ì „ëµ: íƒìƒ‰ vs í™œìš©
        if np.random.random() < self.epsilon:
            # íƒìƒ‰: ëœë¤ í–‰ë™ (ìƒˆë¡œìš´ ì „ëµ ì‹œë„)
            return np.random.randint(0, len(self.actions))
        else:
            # í™œìš©: Q-Tableì—ì„œ ìµœì„ ì˜ í–‰ë™ ì„ íƒ
            return np.argmax(self.q_table[state])

    def update_q_value(self, state, action, reward, next_state):
        # Bellman ë°©ì •ì‹: Q-Learningì˜ í•µì‹¬
        # Q(s,a) = Q(s,a) + Î±[r + Î³*max(Q(s',a')) - Q(s,a)]

        current_q = self.q_table[state, action]
        max_next_q = np.max(self.q_table[next_state])

        # TD Error (Temporal Difference)
        td_target = reward + self.discount_factor * max_next_q
        td_error = td_target - current_q

        # Q-Value ì—…ë°ì´íŠ¸
        new_q = current_q + self.learning_rate * td_error
        self.q_table[state, action] = new_q

        # Îµ ê°ì†Œ (ì ì  íƒìƒ‰ ì¤„ì´ê³  í™œìš© ì¦ê°€)
        if self.epsilon > self.epsilon_min:
            self.epsilon *= self.epsilon_decay

    def train(self, episodes=1000):
        for episode in range(episodes):
            state = 0  # ì‹œì‘ ìœ„ì¹˜
            total_reward = 0

            for step in range(50):  # ìµœëŒ€ 50ìŠ¤í…
                # í–‰ë™ ì„ íƒ
                action = self.choose_action(state)

                # í™˜ê²½ì—ì„œ í–‰ë™ ì‹¤í–‰
                next_state, reward, done = self.env_step(state, action)

                # Q-Value ì—…ë°ì´íŠ¸
                self.update_q_value(state, action, reward, next_state)

                total_reward += reward
                state = next_state

                if done:
                    break

            if episode % 100 == 0:
                print(f"Episode {episode}: Total Reward = {total_reward:.2f}, Îµ = {self.epsilon:.3f}")

    def env_step(self, state, action):
        # í™˜ê²½ ì‹œë®¬ë ˆì´ì…˜ (ì‹¤ì œë¡œëŠ” ë¬¼ë¦¬ ì—”ì§„ ë˜ëŠ” ì‹¤ì œ ë¡œë´‡)
        target_position = 7  # ë¬¼ì²´ ìœ„ì¹˜

        if action == 0:  # LEFT
            next_state = max(0, state - 1)
        elif action == 1:  # RIGHT
            next_state = min(9, state + 1)
        elif action == 2:  # GRASP
            if state == target_position:
                return state, 10.0, True  # ì„±ê³µ!
            else:
                return state, -1.0, False  # ì‹¤íŒ¨
        else:  # RELEASE
            return state, -0.1, False

        # ëª©í‘œì— ê°€ê¹Œì›Œì§ˆìˆ˜ë¡ ì‘ì€ ë³´ìƒ
        distance_reward = -abs(next_state - target_position) * 0.1

        return next_state, distance_reward, False

# í•™ìŠµ ì‹¤í–‰
agent = RobotArmQLearning()
agent.train(episodes=1000)

# í•™ìŠµëœ ì •ì±…ìœ¼ë¡œ ì‹¤í–‰
state = 0
for step in range(10):
    action = agent.choose_action(state)
    print(f"State {state}: Action = {agent.actions[action]}")
    state, _, done = agent.env_step(state, action)
    if done:
        print("âœ… ë¬¼ì²´ë¥¼ ì„±ê³µì ìœ¼ë¡œ ì¡ì•˜ìŠµë‹ˆë‹¤!")
        break`}
            </pre>
          </div>

          <div className="bg-green-50 dark:bg-green-900/20 p-4 rounded-lg border-l-4 border-green-500">
            <h4 className="font-bold mb-2">ğŸ“ˆ í•™ìŠµ ê³¼ì •</h4>
            <div className="space-y-2 text-sm">
              <div className="flex items-start gap-2">
                <span className="font-bold text-green-600">Episode 1-100:</span>
                <span>ëœë¤í•˜ê²Œ ì›€ì§ì´ë©° í™˜ê²½ íƒìƒ‰ (Îµ â‰ˆ 1.0)</span>
              </div>
              <div className="flex items-start gap-2">
                <span className="font-bold text-blue-600">Episode 100-500:</span>
                <span>ì¢‹ì€ í–‰ë™ íŒ¨í„´ ë°œê²¬ ì‹œì‘ (Îµ â‰ˆ 0.5)</span>
              </div>
              <div className="flex items-start gap-2">
                <span className="font-bold text-purple-600">Episode 500-1000:</span>
                <span>ìµœì  ì „ëµ í™œìš©, ê±°ì˜ í•­ìƒ ì„±ê³µ (Îµ â‰ˆ 0.01)</span>
              </div>
            </div>
          </div>
        </div>
      </section>

      {/* 2. Deep Q-Network (DQN) */}
      <section className="my-8">
        <h2 className="flex items-center gap-2">
          <Zap className="text-purple-600" />
          2. Deep Q-Network (DQN) - ë”¥ëŸ¬ë‹ + ê°•í™”í•™ìŠµ
        </h2>

        <div className="bg-white dark:bg-gray-800 rounded-lg p-6 shadow-lg mb-6">
          <h3 className="text-xl font-bold mb-4">ğŸ§  ì‹ ê²½ë§ìœ¼ë¡œ Q-Value ì˜ˆì¸¡</h3>

          <div className="bg-purple-50 dark:bg-purple-900/20 p-5 rounded-lg mb-6">
            <h4 className="font-bold mb-3">Q-Learningì˜ í•œê³„ì™€ DQNì˜ í•´ê²°ì±…</h4>
            <div className="grid md:grid-cols-2 gap-4">
              <div>
                <h5 className="font-bold text-sm mb-2 text-red-600">Q-Learning í•œê³„</h5>
                <ul className="text-sm space-y-1">
                  <li>âŒ ìƒíƒœê°€ ë§ìœ¼ë©´ Q-Table í­ë°œ</li>
                  <li>âŒ ì—°ì†ì  ìƒíƒœ ì²˜ë¦¬ ë¶ˆê°€</li>
                  <li>âŒ ì˜ˆ: ë¡œë´‡ ê´€ì ˆ ê°ë„ (ë¬´í•œëŒ€)</li>
                </ul>
              </div>
              <div className="border-l-2 border-purple-300 pl-4">
                <h5 className="font-bold text-sm mb-2 text-green-600">DQN í•´ê²°ì±…</h5>
                <ul className="text-sm space-y-1">
                  <li>âœ… ì‹ ê²½ë§ì´ Q-Value ê·¼ì‚¬</li>
                  <li>âœ… ì—°ì† ìƒíƒœë„ ì²˜ë¦¬ ê°€ëŠ¥</li>
                  <li>âœ… ì´ë¯¸ì§€ ì…ë ¥ë„ ê°€ëŠ¥ (CNN)</li>
                </ul>
              </div>
            </div>
          </div>

          <div className="bg-gray-900 text-gray-100 p-4 rounded-lg overflow-x-auto mb-4">
            <pre className="text-sm">
{`# DQN: ì´ë¯¸ì§€ë¡œ ë¡œë´‡ ì œì–´ í•™ìŠµ
import torch
import torch.nn as nn
import torch.optim as optim
from collections import deque
import random

class DQNetwork(nn.Module):
    """ì‹ ê²½ë§ìœ¼ë¡œ Q-Value ì˜ˆì¸¡"""
    def __init__(self, state_size, action_size):
        super(DQNetwork, self).__init__()

        # 3ì¸µ ì‹ ê²½ë§
        self.fc1 = nn.Linear(state_size, 128)
        self.fc2 = nn.Linear(128, 128)
        self.fc3 = nn.Linear(128, action_size)

    def forward(self, state):
        x = torch.relu(self.fc1(state))
        x = torch.relu(self.fc2(x))
        q_values = self.fc3(x)  # ê° í–‰ë™ì˜ Q-Value
        return q_values

class DQNAgent:
    def __init__(self, state_size, action_size):
        self.state_size = state_size
        self.action_size = action_size

        # ì£¼ ë„¤íŠ¸ì›Œí¬ì™€ íƒ€ê²Ÿ ë„¤íŠ¸ì›Œí¬ (Double DQN)
        self.q_network = DQNetwork(state_size, action_size)
        self.target_network = DQNetwork(state_size, action_size)
        self.target_network.load_state_dict(self.q_network.state_dict())

        # ê²½í—˜ ì¬í˜„ ë©”ëª¨ë¦¬ (Experience Replay)
        self.memory = deque(maxlen=10000)
        self.batch_size = 64

        # í•™ìŠµ íŒŒë¼ë¯¸í„°
        self.optimizer = optim.Adam(self.q_network.parameters(), lr=0.001)
        self.gamma = 0.99
        self.epsilon = 1.0
        self.epsilon_decay = 0.995
        self.epsilon_min = 0.01

    def remember(self, state, action, reward, next_state, done):
        """ê²½í—˜ì„ ë©”ëª¨ë¦¬ì— ì €ì¥"""
        self.memory.append((state, action, reward, next_state, done))

    def choose_action(self, state):
        if np.random.random() < self.epsilon:
            return random.randrange(self.action_size)

        with torch.no_grad():
            state_tensor = torch.FloatTensor(state).unsqueeze(0)
            q_values = self.q_network(state_tensor)
            return torch.argmax(q_values).item()

    def replay(self):
        """ê²½í—˜ ì¬í˜„ìœ¼ë¡œ í•™ìŠµ"""
        if len(self.memory) < self.batch_size:
            return

        # ëœë¤ ìƒ˜í”Œë§
        batch = random.sample(self.memory, self.batch_size)

        for state, action, reward, next_state, done in batch:
            state_tensor = torch.FloatTensor(state).unsqueeze(0)
            next_state_tensor = torch.FloatTensor(next_state).unsqueeze(0)

            # í˜„ì¬ Q-Value
            q_values = self.q_network(state_tensor)
            q_value = q_values[0][action]

            # íƒ€ê²Ÿ Q-Value (Double DQN)
            with torch.no_grad():
                next_q_values = self.target_network(next_state_tensor)
                max_next_q = torch.max(next_q_values)
                target_q = reward if done else reward + self.gamma * max_next_q

            # ì†ì‹¤ ê³„ì‚° ë° ì—­ì „íŒŒ
            loss = nn.MSELoss()(q_value, torch.FloatTensor([target_q]))

            self.optimizer.zero_grad()
            loss.backward()
            self.optimizer.step()

        # Îµ ê°ì†Œ
        if self.epsilon > self.epsilon_min:
            self.epsilon *= self.epsilon_decay

    def update_target_network(self):
        """íƒ€ê²Ÿ ë„¤íŠ¸ì›Œí¬ ì—…ë°ì´íŠ¸ (ë§¤ N ì—í”¼ì†Œë“œë§ˆë‹¤)"""
        self.target_network.load_state_dict(self.q_network.state_dict())

# ì‚¬ìš© ì˜ˆì‹œ: ë¡œë´‡ íŒ” ì œì–´
state_size = 6  # 6 DOF ë¡œë´‡ íŒ” ê´€ì ˆ ê°ë„
action_size = 4  # ìƒí•˜ì¢Œìš° ì´ë™

agent = DQNAgent(state_size, action_size)

for episode in range(1000):
    state = env.reset()
    total_reward = 0

    for step in range(200):
        action = agent.choose_action(state)
        next_state, reward, done, _ = env.step(action)

        # ê²½í—˜ ì €ì¥
        agent.remember(state, action, reward, next_state, done)

        # í•™ìŠµ
        agent.replay()

        state = next_state
        total_reward += reward

        if done:
            break

    # íƒ€ê²Ÿ ë„¤íŠ¸ì›Œí¬ ì—…ë°ì´íŠ¸ (ë§¤ 10 ì—í”¼ì†Œë“œ)
    if episode % 10 == 0:
        agent.update_target_network()

    print(f"Episode {episode}: Reward = {total_reward:.2f}")`}
            </pre>
          </div>

          <div className="bg-cyan-50 dark:bg-cyan-900/20 p-4 rounded-lg border-l-4 border-cyan-500">
            <h4 className="font-bold mb-2">ğŸ® DQNì˜ í˜ì‹ ì  ê¸°ìˆ </h4>
            <ul className="text-sm space-y-2">
              <li>
                <strong className="text-cyan-700 dark:text-cyan-300">â€¢ Experience Replay</strong>
                <p className="mt-1">ê³¼ê±° ê²½í—˜ì„ ë©”ëª¨ë¦¬ì— ì €ì¥í•´ ë°˜ë³µ í•™ìŠµ â†’ ë°ì´í„° íš¨ìœ¨ì„± í–¥ìƒ</p>
              </li>
              <li>
                <strong className="text-cyan-700 dark:text-cyan-300">â€¢ Target Network</strong>
                <p className="mt-1">ë³„ë„ì˜ íƒ€ê²Ÿ ë„¤íŠ¸ì›Œí¬ë¡œ í•™ìŠµ ì•ˆì •í™” â†’ ì§„ë™ ë°©ì§€</p>
              </li>
              <li>
                <strong className="text-cyan-700 dark:text-cyan-300">â€¢ CNN í†µí•©</strong>
                <p className="mt-1">ì´ë¯¸ì§€ë¥¼ ì§ì ‘ ì…ë ¥ë°›ì•„ ì‹œê° ì •ë³´ë¡œ í•™ìŠµ ê°€ëŠ¥</p>
              </li>
            </ul>
          </div>
        </div>
      </section>

      {/* 3. PPO (Proximal Policy Optimization) */}
      <section className="my-8">
        <h2 className="flex items-center gap-2">
          <Award className="text-green-600" />
          3. PPO - í˜„ëŒ€ ë¡œë´‡ì˜ í‘œì¤€ ì•Œê³ ë¦¬ì¦˜
        </h2>

        <div className="bg-white dark:bg-gray-800 rounded-lg p-6 shadow-lg mb-6">
          <h3 className="text-xl font-bold mb-4">ğŸ† OpenAIê°€ ì„ íƒí•œ ì•Œê³ ë¦¬ì¦˜</h3>

          <div className="bg-green-50 dark:bg-green-900/20 p-5 rounded-lg mb-6">
            <h4 className="font-bold mb-3">ì™œ PPOê°€ ë¡œë´‡ ì œì–´ì˜ í‘œì¤€ì¸ê°€?</h4>
            <p className="text-sm mb-4">
              <strong>PPO (Proximal Policy Optimization)</strong>ëŠ” OpenAIê°€ ê°œë°œí•œ ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ,
              <strong>ì•ˆì •ì„±ê³¼ ì„±ëŠ¥</strong>ì„ ë™ì‹œì— ê°–ì¶˜ ìµœê³ ì˜ ê°•í™”í•™ìŠµ ë°©ë²•ì…ë‹ˆë‹¤.
            </p>
            <div className="grid md:grid-cols-2 gap-4">
              <div className="bg-white dark:bg-gray-800 p-4 rounded">
                <h5 className="font-bold text-sm mb-2">âœ… ì¥ì </h5>
                <ul className="text-sm space-y-1">
                  <li>â€¢ í•™ìŠµ ì•ˆì •ì  (í­ë°œ ì—†ìŒ)</li>
                  <li>â€¢ ìƒ˜í”Œ íš¨ìœ¨ì </li>
                  <li>â€¢ êµ¬í˜„ ê°„ë‹¨</li>
                  <li>â€¢ ë‹¤ì–‘í•œ ì‘ì—…ì— ì ìš© ê°€ëŠ¥</li>
                </ul>
              </div>
              <div className="bg-white dark:bg-gray-800 p-4 rounded">
                <h5 className="font-bold text-sm mb-2">ğŸ¯ ì‚¬ìš© ì‚¬ë¡€</h5>
                <ul className="text-sm space-y-1">
                  <li>â€¢ Tesla Bot ë³´í–‰ í•™ìŠµ</li>
                  <li>â€¢ Boston Dynamics ë™ì‘</li>
                  <li>â€¢ OpenAI Dota 2 ì±”í”¼ì–¸</li>
                  <li>â€¢ ChatGPT RLHF í•™ìŠµ</li>
                </ul>
              </div>
            </div>
          </div>

          <div className="bg-gray-900 text-gray-100 p-4 rounded-lg overflow-x-auto">
            <pre className="text-sm">
{`# PPOë¥¼ ì´ìš©í•œ íœ´ë¨¸ë…¸ì´ë“œ ë³´í–‰ í•™ìŠµ
import torch
import torch.nn as nn
from torch.distributions import Normal

class ActorCritic(nn.Module):
    """Actor (ì •ì±…)ì™€ Critic (ê°€ì¹˜) ë„¤íŠ¸ì›Œí¬"""
    def __init__(self, state_size, action_size):
        super(ActorCritic, self).__init__()

        # ê³µìœ  ë ˆì´ì–´
        self.shared = nn.Sequential(
            nn.Linear(state_size, 256),
            nn.ReLU(),
            nn.Linear(256, 256),
            nn.ReLU()
        )

        # Actor: í–‰ë™ í™•ë¥  ë¶„í¬ ì¶œë ¥
        self.actor_mean = nn.Linear(256, action_size)
        self.actor_std = nn.Parameter(torch.ones(action_size) * 0.1)

        # Critic: ìƒíƒœ ê°€ì¹˜ ì¶œë ¥
        self.critic = nn.Linear(256, 1)

    def forward(self, state):
        shared_features = self.shared(state)

        # Actor ì¶œë ¥
        action_mean = self.actor_mean(shared_features)
        action_std = self.actor_std.expand_as(action_mean)
        action_dist = Normal(action_mean, action_std)

        # Critic ì¶œë ¥
        state_value = self.critic(shared_features)

        return action_dist, state_value

class PPOAgent:
    def __init__(self, state_size, action_size):
        self.model = ActorCritic(state_size, action_size)
        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=3e-4)

        self.clip_epsilon = 0.2  # PPO í´ë¦¬í•‘ íŒŒë¼ë¯¸í„°
        self.gamma = 0.99
        self.gae_lambda = 0.95  # GAE (Generalized Advantage Estimation)

    def get_action(self, state):
        """ì •ì±…ì— ë”°ë¼ í–‰ë™ ìƒ˜í”Œë§"""
        state_tensor = torch.FloatTensor(state)
        action_dist, _ = self.model(state_tensor)

        action = action_dist.sample()
        log_prob = action_dist.log_prob(action).sum()

        return action.numpy(), log_prob

    def update(self, states, actions, old_log_probs, rewards, dones):
        """PPO í•µì‹¬: Clipped Surrogate Objective"""
        states = torch.FloatTensor(states)
        actions = torch.FloatTensor(actions)
        old_log_probs = torch.FloatTensor(old_log_probs)

        # Advantage ê³„ì‚° (GAE)
        advantages = self.compute_gae(states, rewards, dones)
        returns = advantages + self.compute_value(states)

        # PPO ì—…ë°ì´íŠ¸ (ì—¬ëŸ¬ ì—í¬í¬ ë°˜ë³µ)
        for _ in range(10):
            # í˜„ì¬ ì •ì±…ìœ¼ë¡œ log_prob ì¬ê³„ì‚°
            action_dist, values = self.model(states)
            new_log_probs = action_dist.log_prob(actions).sum(dim=1)

            # Importance Sampling Ratio
            ratio = torch.exp(new_log_probs - old_log_probs)

            # Clipped Surrogate Loss (PPO í•µì‹¬!)
            surr1 = ratio * advantages
            surr2 = torch.clamp(ratio, 1 - self.clip_epsilon, 1 + self.clip_epsilon) * advantages
            actor_loss = -torch.min(surr1, surr2).mean()

            # Critic Loss
            critic_loss = nn.MSELoss()(values.squeeze(), returns)

            # ì´ ì†ì‹¤
            loss = actor_loss + 0.5 * critic_loss

            # ì—­ì „íŒŒ
            self.optimizer.zero_grad()
            loss.backward()
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), 0.5)
            self.optimizer.step()

    def compute_gae(self, states, rewards, dones):
        """Generalized Advantage Estimation"""
        with torch.no_grad():
            _, values = self.model(states)
            values = values.squeeze()

        advantages = []
        gae = 0

        for t in reversed(range(len(rewards))):
            if t == len(rewards) - 1:
                next_value = 0
            else:
                next_value = values[t + 1]

            delta = rewards[t] + self.gamma * next_value * (1 - dones[t]) - values[t]
            gae = delta + self.gamma * self.gae_lambda * (1 - dones[t]) * gae
            advantages.insert(0, gae)

        return torch.FloatTensor(advantages)

# íœ´ë¨¸ë…¸ì´ë“œ ë³´í–‰ í•™ìŠµ
state_size = 44  # ê´€ì ˆ ê°ë„, ì†ë„, ì„¼ì„œ ë°ì´í„°
action_size = 17  # 17ê°œ ê´€ì ˆ í† í¬ ì œì–´

agent = PPOAgent(state_size, action_size)

for iteration in range(1000):
    states, actions, log_probs, rewards, dones = collect_trajectories(agent)
    agent.update(states, actions, log_probs, rewards, dones)

    print(f"Iteration {iteration}: Avg Reward = {np.mean(rewards):.2f}")`}
            </pre>
          </div>
        </div>
      </section>

      {/* 4. Model Predictive Control (MPC) */}
      <section className="my-8">
        <h2 className="flex items-center gap-2">
          <RefreshCw className="text-indigo-600" />
          4. MPC - ì •ë°€í•œ ë¡œë´‡ ì œì–´
        </h2>

        <div className="bg-white dark:bg-gray-800 rounded-lg p-6 shadow-lg mb-6">
          <h3 className="text-xl font-bold mb-4">ğŸ¯ ë¯¸ë˜ë¥¼ ì˜ˆì¸¡í•˜ë©° ìµœì  ê²½ë¡œ ê³„ì‚°</h3>

          <div className="bg-indigo-50 dark:bg-indigo-900/20 p-5 rounded-lg mb-6">
            <h4 className="font-bold mb-3">MPCë€?</h4>
            <p className="text-sm mb-4">
              <strong>Model Predictive Control (ëª¨ë¸ ì˜ˆì¸¡ ì œì–´)</strong>ëŠ” ë¡œë´‡ì˜ ë¬¼ë¦¬ ëª¨ë¸ì„ ì‚¬ìš©í•´
              <strong>ë¯¸ë˜ ê¶¤ì ì„ ì‹œë®¬ë ˆì´ì…˜</strong>í•˜ê³ , ê°€ì¥ ì¢‹ì€ ê²½ë¡œë¥¼ ì„ íƒí•˜ëŠ” ì œì–´ ë°©ë²•ì…ë‹ˆë‹¤.
            </p>
            <div className="bg-white dark:bg-gray-800 p-4 rounded text-sm">
              <strong>ì‘ë™ ì›ë¦¬</strong>:
              <ol className="mt-2 space-y-1">
                <li>1. í˜„ì¬ ìƒíƒœì—ì„œ ë¯¸ë˜ NìŠ¤í… ì‹œë®¬ë ˆì´ì…˜</li>
                <li>2. ìµœì í™”ë¡œ ìµœì„ ì˜ í–‰ë™ ì‹œí€€ìŠ¤ ì°¾ê¸°</li>
                <li>3. ì²« ë²ˆì§¸ í–‰ë™ë§Œ ì‹¤í–‰</li>
                <li>4. ë°˜ë³µ (Receding Horizon)</li>
              </ol>
            </div>
          </div>

          <div className="bg-gray-900 text-gray-100 p-4 rounded-lg overflow-x-auto mb-4">
            <pre className="text-sm">
{`# MPC for ë¡œë´‡ íŒ” ê¶¤ì  ìµœì í™”
from scipy.optimize import minimize
import numpy as np

class MPCController:
    def __init__(self, horizon=10, dt=0.1):
        self.horizon = horizon  # ì˜ˆì¸¡ ë²”ìœ„
        self.dt = dt  # ì‹œê°„ ê°„ê²©

    def predict_trajectory(self, current_state, actions):
        """ë¬¼ë¦¬ ëª¨ë¸ë¡œ ë¯¸ë˜ ê¶¤ì  ì˜ˆì¸¡"""
        trajectory = [current_state]
        state = current_state.copy()

        for action in actions:
            # ë¡œë´‡ ë™ì—­í•™ ëª¨ë¸ (ê°„ë‹¨í•œ ì˜ˆì‹œ)
            velocity = state[3:6]  # ì†ë„
            acceleration = action  # ì œì–´ ì…ë ¥

            # ì˜¤ì¼ëŸ¬ ì ë¶„
            new_velocity = velocity + acceleration * self.dt
            new_position = state[0:3] + new_velocity * self.dt

            state = np.concatenate([new_position, new_velocity])
            trajectory.append(state)

        return np.array(trajectory)

    def cost_function(self, actions, current_state, target_state):
        """ë¹„ìš© í•¨ìˆ˜: ëª©í‘œì™€ì˜ ì°¨ì´ + ì œì–´ ì—ë„ˆì§€"""
        actions = actions.reshape(self.horizon, 3)  # (N, 3) í˜•íƒœë¡œ ë³€í™˜

        # ë¯¸ë˜ ê¶¤ì  ì˜ˆì¸¡
        trajectory = self.predict_trajectory(current_state, actions)

        # ëª©í‘œ ë„ë‹¬ ë¹„ìš©
        final_state = trajectory[-1]
        position_error = np.linalg.norm(final_state[0:3] - target_state[0:3])

        # ì œì–´ ì—ë„ˆì§€ ë¹„ìš© (ì‘ì€ í˜ ì„ í˜¸)
        control_cost = np.sum(actions**2) * 0.01

        # ê²½ë¡œ ìŠ¤ë¬´ìŠ¤ë‹ˆìŠ¤ (ê¸‰ê²©í•œ ë³€í™” ë°©ì§€)
        smoothness_cost = np.sum(np.diff(actions, axis=0)**2) * 0.1

        total_cost = position_error + control_cost + smoothness_cost
        return total_cost

    def compute_optimal_action(self, current_state, target_state):
        """ìµœì í™”ë¡œ ìµœì„ ì˜ í–‰ë™ ì‹œí€€ìŠ¤ ê³„ì‚°"""
        # ì´ˆê¸° ì¶”ì • (ì˜ ì…ë ¥)
        initial_guess = np.zeros(self.horizon * 3)

        # ìµœì í™” ì‹¤í–‰
        result = minimize(
            self.cost_function,
            initial_guess,
            args=(current_state, target_state),
            method='SLSQP',
            bounds=[(-1, 1)] * (self.horizon * 3)  # ì•¡ì…˜ ë²”ìœ„
        )

        # ì²« ë²ˆì§¸ í–‰ë™ë§Œ ë°˜í™˜ (Receding Horizon)
        optimal_actions = result.x.reshape(self.horizon, 3)
        return optimal_actions[0]

# ì‚¬ìš© ì˜ˆì‹œ: ë¡œë´‡ íŒ”ì„ ëª©í‘œ ìœ„ì¹˜ë¡œ ì´ë™
mpc = MPCController(horizon=10, dt=0.1)

current_state = np.array([0, 0, 0, 0, 0, 0])  # [x, y, z, vx, vy, vz]
target_state = np.array([1, 1, 1, 0, 0, 0])

for step in range(100):
    # ìµœì  ì œì–´ ê³„ì‚°
    action = mpc.compute_optimal_action(current_state, target_state)

    # ì‹¤ì œ ë¡œë´‡ì— ëª…ë ¹ ì „ë‹¬
    robot.apply_force(action)

    # ìƒíƒœ ì—…ë°ì´íŠ¸ (ì‹¤ì œ ì„¼ì„œ ì¸¡ì •)
    current_state = robot.get_state()

    # ëª©í‘œ ë„ë‹¬ í™•ì¸
    if np.linalg.norm(current_state[0:3] - target_state[0:3]) < 0.01:
        print("âœ… ëª©í‘œ ë„ë‹¬!")
        break`}
            </pre>
          </div>

          <div className="bg-blue-50 dark:bg-blue-900/20 p-4 rounded-lg border-l-4 border-blue-500">
            <h4 className="font-bold mb-2">ğŸ­ MPC ì‘ìš© ì‚¬ë¡€</h4>
            <ul className="text-sm space-y-2">
              <li>
                <strong>â€¢ Boston Dynamics Atlas</strong>
                <p className="mt-1">ì „ì‹  ì œì–´: 100Hzë¡œ ë¯¸ë˜ ê¶¤ì  ìµœì í™”í•´ íŒŒì¿ ë¥´ ë™ì‘ ìˆ˜í–‰</p>
              </li>
              <li>
                <strong>â€¢ Tesla Autopilot</strong>
                <p className="mt-1">ì°¨ì„  ìœ ì§€: ì• ì°¨ëŸ‰ê³¼ì˜ ê°„ê²©ì„ ì˜ˆì¸¡í•˜ë©° ìµœì  ì†ë„ ê³„ì‚°</p>
              </li>
              <li>
                <strong>â€¢ ì‚°ì—… ë¡œë´‡ íŒ”</strong>
                <p className="mt-1">ì •ë°€ ì¡°ë¦½: ì§„ë™ ìµœì†Œí™”í•˜ë©° ë¹ ë¥¸ ì´ë™ ì‹¤í˜„</p>
              </li>
            </ul>
          </div>
        </div>
      </section>

      {/* 5. Sim2Real */}
      <section className="my-8">
        <h2 className="flex items-center gap-2">
          <TrendingUp className="text-pink-600" />
          5. Sim2Real - ê°€ìƒì—ì„œ í•™ìŠµ, í˜„ì‹¤ì—ì„œ ì‹¤í–‰
        </h2>

        <div className="bg-white dark:bg-gray-800 rounded-lg p-6 shadow-lg">
          <h3 className="text-xl font-bold mb-4">ğŸŒ NVIDIA Isaac Gymì˜ í˜ëª…</h3>

          <div className="bg-pink-50 dark:bg-pink-900/20 p-5 rounded-lg mb-6">
            <h4 className="font-bold mb-3">ì™œ ì‹œë®¬ë ˆì´ì…˜ì´ í•„ìˆ˜ì¸ê°€?</h4>
            <p className="text-sm mb-4">
              ì‹¤ì œ ë¡œë´‡ìœ¼ë¡œ í•™ìŠµí•˜ë ¤ë©´ <strong>ì‹œê°„ê³¼ ë¹„ìš©ì´ ì—„ì²­ë‚©ë‹ˆë‹¤</strong>.
              ì‹œë®¬ë ˆì´ì…˜ì—ì„œëŠ” <strong>ìˆ˜ì²œ ë°° ë¹ ë¥´ê²Œ, ë¬´í•œ ë°˜ë³µ</strong> ê°€ëŠ¥í•©ë‹ˆë‹¤.
            </p>
            <div className="grid md:grid-cols-2 gap-4">
              <div className="bg-white dark:bg-gray-800 p-4 rounded">
                <h5 className="font-bold text-sm mb-2 text-red-600">ì‹¤ì œ ë¡œë´‡ í•™ìŠµ</h5>
                <ul className="text-sm space-y-1">
                  <li>âŒ 1ì‹œê°„ = 1ì‹œê°„ (ì‹¤ì‹œê°„)</li>
                  <li>âŒ ë¡œë´‡ ê³ ì¥ ìœ„í—˜</li>
                  <li>âŒ ìœ„í—˜í•œ ìƒí™© í…ŒìŠ¤íŠ¸ ë¶ˆê°€</li>
                  <li>âŒ ë¹„ìš©: $1M+ (ë¡œë´‡ + ì¸ë ¥)</li>
                </ul>
              </div>
              <div className="bg-white dark:bg-gray-800 p-4 rounded border-2 border-green-500">
                <h5 className="font-bold text-sm mb-2 text-green-600">ì‹œë®¬ë ˆì´ì…˜ í•™ìŠµ</h5>
                <ul className="text-sm space-y-1">
                  <li>âœ… 1ì‹œê°„ = 1000ì‹œê°„ (ë³‘ë ¬)</li>
                  <li>âœ… ë¬´í•œ ì‹œí–‰ì°©ì˜¤</li>
                  <li>âœ… ê·¹í•œ ìƒí™© í…ŒìŠ¤íŠ¸ ê°€ëŠ¥</li>
                  <li>âœ… ë¹„ìš©: GPU ë Œíƒˆ ë¹„ìš©ë§Œ</li>
                </ul>
              </div>
            </div>
          </div>

          <div className="bg-gradient-to-r from-green-50 to-cyan-50 dark:from-green-900/20 dark:to-cyan-900/20 p-5 rounded-lg">
            <h4 className="font-bold mb-3">ğŸ¯ Sim2Real ì„±ê³µ ì‚¬ë¡€</h4>
            <div className="space-y-3 text-sm">
              <div>
                <strong className="text-green-700 dark:text-green-300">â€¢ NVIDIA Isaac Gym</strong>
                <p className="mt-1">GPUë¡œ 4096ê°œ ë¡œë´‡ì„ ë™ì‹œì— ì‹œë®¬ë ˆì´ì…˜. í•˜ë£¨ì— ìˆ˜ë°± ë…„ì¹˜ ê²½í—˜ í•™ìŠµ.</p>
              </div>
              <div>
                <strong className="text-blue-700 dark:text-blue-300">â€¢ OpenAI Dactyl</strong>
                <p className="mt-1">ë¡œë´‡ ì†ì´ ë£¨ë¹…ìŠ¤ íë¸Œë¥¼ í‘¸ëŠ” ë²•ì„ ì‹œë®¬ë ˆì´ì…˜ì—ì„œ í•™ìŠµ í›„ ì‹¤ì œë¡œ ì„±ê³µ.</p>
              </div>
              <div>
                <strong className="text-purple-700 dark:text-purple-300">â€¢ Tesla Bot</strong>
                <p className="mt-1">ê°€ìƒ ê³µì¥ì—ì„œ ìˆ˜ì²œ ë²ˆ ë„˜ì–´ì§€ë©° ë³´í–‰ í•™ìŠµ. ì‹¤ì œ ë¡œë´‡ì€ ì¦‰ì‹œ ê±¸ìŒ.</p>
              </div>
            </div>
          </div>
        </div>
      </section>

      {/* Summary */}
      <div className="bg-gradient-to-r from-orange-50 to-red-50 dark:from-orange-900/20 dark:to-red-900/20 border-l-4 border-orange-500 p-6 rounded-lg my-8">
        <h3 className="text-xl font-bold mb-3">ğŸ“Œ í•µì‹¬ ìš”ì•½</h3>
        <ul className="space-y-2 text-sm">
          <li>âœ… <strong>Q-Learning</strong>: ê¸°ë³¸ ê°•í™”í•™ìŠµ, Q-Tableë¡œ ìµœì  í–‰ë™ ì €ì¥</li>
          <li>âœ… <strong>DQN</strong>: ì‹ ê²½ë§ìœ¼ë¡œ Q-Value ê·¼ì‚¬, ì´ë¯¸ì§€ ì…ë ¥ ê°€ëŠ¥</li>
          <li>âœ… <strong>PPO</strong>: í˜„ëŒ€ ë¡œë´‡ì˜ í‘œì¤€, ì•ˆì •ì ì´ê³  íš¨ìœ¨ì </li>
          <li>âœ… <strong>MPC</strong>: ë¯¸ë˜ ì˜ˆì¸¡ìœ¼ë¡œ ì •ë°€ ì œì–´, Boston Dynamics í•µì‹¬</li>
          <li>âœ… <strong>Sim2Real</strong>: ì‹œë®¬ë ˆì´ì…˜ í•™ìŠµ â†’ ì‹¤ì œ ë¡œë´‡ ë°°í¬</li>
        </ul>
      </div>

      {/* Next Chapter */}
      <div className="bg-blue-50 dark:bg-blue-900/20 border-l-4 border-blue-500 p-6 rounded-lg">
        <h3 className="text-xl font-bold mb-2">ë‹¤ìŒ ë‹¨ê³„: IoT & Edge Computing</h3>
        <p className="text-gray-700 dark:text-gray-300">
          ë‹¤ìŒ ì±•í„°ì—ì„œëŠ” ë¡œë´‡ì´ <strong>ì‹¤ì‹œê°„ìœ¼ë¡œ íŒë‹¨</strong>í•˜ê¸° ìœ„í•œ
          IoT ì„¼ì„œ ë„¤íŠ¸ì›Œí¬ì™€ ì—£ì§€ AI ì¹© ê¸°ìˆ ì„ í•™ìŠµí•©ë‹ˆë‹¤.
        </p>
      </div>
    </div>
  )
}