'use client'

import Link from 'next/link'
import { ArrowLeft, ArrowRight, Target } from 'lucide-react'
import References from '@/components/common/References'

export default function Section4() {
  return (
    <>
      <section className="bg-white dark:bg-gray-800 rounded-2xl p-8 shadow-sm border border-gray-200 dark:border-gray-700">
        <div className="flex items-center gap-3 mb-6">
          <div className="w-12 h-12 rounded-xl bg-blue-100 dark:bg-blue-900/20 flex items-center justify-center">
            <Target className="text-blue-600" size={24} />
          </div>
          <div>
            <h2 className="text-2xl font-bold text-gray-900 dark:text-white">4.4 Learning to Rank (LTR) for RAG</h2>
            <p className="text-gray-600 dark:text-gray-400">ê¸°ê³„í•™ìŠµì„ í™œìš©í•œ ìµœì  ìˆœìœ„ í•™ìŠµ</p>
          </div>
        </div>

        <div className="space-y-6">
          <div className="bg-blue-50 dark:bg-blue-900/20 p-6 rounded-xl border border-blue-200 dark:border-blue-700">
            <h3 className="font-bold text-blue-800 dark:text-blue-200 mb-4">LambdaMARTë¥¼ í™œìš©í•œ RAG ìµœì í™”</h3>

            <div className="prose prose-sm dark:prose-invert mb-4">
              <p className="text-gray-700 dark:text-gray-300">
                <strong>Learning to RankëŠ” ê²€ìƒ‰ í’ˆì§ˆì„ ê·¹ëŒ€í™”í•˜ëŠ” í•µì‹¬ ê¸°ìˆ ì…ë‹ˆë‹¤.</strong>
                íŠ¹íˆ RAG ì‹œìŠ¤í…œì—ì„œëŠ” ë‹¨ìˆœí•œ ë²¡í„° ìœ ì‚¬ë„ë¥¼ ë„˜ì–´ì„œ ë‹¤ì–‘í•œ íŠ¹ì§•ì„ ì¢…í•©ì ìœ¼ë¡œ
                ê³ ë ¤í•˜ì—¬ ìµœì ì˜ ìˆœìœ„ë¥¼ í•™ìŠµí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
              </p>
            </div>

            <div className="bg-slate-50 dark:bg-slate-800 p-4 rounded-lg border border-slate-200 dark:border-slate-700 overflow-x-auto">
              <pre className="text-sm text-slate-800 dark:text-slate-200 font-mono">
{`import xgboost as xgb
import numpy as np
from sklearn.preprocessing import StandardScaler
from typing import List, Dict, Tuple, Optional
import pandas as pd
from dataclasses import dataclass
from collections import defaultdict

@dataclass
class RankingFeatures:
    """ë­í‚¹ì„ ìœ„í•œ íŠ¹ì§•ë“¤"""
    # í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ íŠ¹ì§•
    bm25_score: float
    vector_similarity: float
    cross_encoder_score: float

    # ë¬¸ì„œ íŠ¹ì§•
    doc_length: int
    doc_freshness: float  # ìµœì‹ ì„± (ì¼ ë‹¨ìœ„)
    doc_popularity: float  # í´ë¦­ë¥ , ì¡°íšŒìˆ˜ ë“±

    # ì¿¼ë¦¬-ë¬¸ì„œ ë§¤ì¹­ íŠ¹ì§•
    exact_match_count: int
    synonym_match_count: int
    named_entity_overlap: float

    # ì˜ë¯¸ì  íŠ¹ì§•
    topic_similarity: float
    sentiment_alignment: float

    # ì‚¬ìš©ì íŠ¹ì§•
    user_click_history: float  # ì´ ë¬¸ì„œ íƒ€ì…ì— ëŒ€í•œ ê³¼ê±° í´ë¦­ë¥ 
    session_dwell_time: float  # ì„¸ì…˜ ë‚´ í‰ê·  ì²´ë¥˜ì‹œê°„

class LearningToRankRAG:
    def __init__(self, model_path: Optional[str] = None):
        """
        Learning to Rank ê¸°ë°˜ RAG ì¬ìˆœìœ„í™”
        - LambdaMART (XGBoost) ì‚¬ìš©
        - ì˜¨ë¼ì¸ í•™ìŠµ ì§€ì›
        - Feature importance ë¶„ì„
        """
        self.scaler = StandardScaler()
        self.feature_names = [
            'bm25_score', 'vector_similarity', 'cross_encoder_score',
            'doc_length_norm', 'doc_freshness', 'doc_popularity',
            'exact_match_count', 'synonym_match_count', 'named_entity_overlap',
            'topic_similarity', 'sentiment_alignment',
            'user_click_history', 'session_dwell_time'
        ]

        if model_path:
            self.model = xgb.Booster()
            self.model.load_model(model_path)
        else:
            self.model = None

        # ì˜¨ë¼ì¸ í•™ìŠµì„ ìœ„í•œ ë²„í¼
        self.training_buffer = []
        self.buffer_size = 1000

    def extract_features(self, query: str, document: Dict,
                        user_context: Optional[Dict] = None) -> RankingFeatures:
        """
        ì¿¼ë¦¬-ë¬¸ì„œ ìŒì—ì„œ ë­í‚¹ íŠ¹ì§• ì¶”ì¶œ
        """
        # ê¸°ë³¸ ì ìˆ˜ë“¤ (ì´ë¯¸ ê³„ì‚°ë˜ì–´ ìˆë‹¤ê³  ê°€ì •)
        features = RankingFeatures(
            bm25_score=document.get('bm25_score', 0.0),
            vector_similarity=document.get('vector_score', 0.0),
            cross_encoder_score=document.get('ce_score', 0.0),
            doc_length=len(document['content'].split()),
            doc_freshness=self._calculate_freshness(document),
            doc_popularity=document.get('popularity', 0.0),
            exact_match_count=self._count_exact_matches(query, document['content']),
            synonym_match_count=self._count_synonym_matches(query, document['content']),
            named_entity_overlap=self._calculate_entity_overlap(query, document['content']),
            topic_similarity=document.get('topic_similarity', 0.0),
            sentiment_alignment=self._calculate_sentiment_alignment(query, document),
            user_click_history=0.0,
            session_dwell_time=0.0
        )

        # ì‚¬ìš©ì ì»¨í…ìŠ¤íŠ¸ê°€ ìˆìœ¼ë©´ ê°œì¸í™” íŠ¹ì§• ì¶”ê°€
        if user_context:
            features.user_click_history = user_context.get('doc_type_ctr', {}).get(
                document.get('type', 'general'), 0.0
            )
            features.session_dwell_time = user_context.get('avg_dwell_time', 0.0)

        return features

    def _calculate_freshness(self, document: Dict) -> float:
        """ë¬¸ì„œ ìµœì‹ ì„± ê³„ì‚° (0-1)"""
        from datetime import datetime, timedelta

        if 'created_at' not in document:
            return 0.5

        created = datetime.fromisoformat(document['created_at'])
        age_days = (datetime.now() - created).days

        # ì§€ìˆ˜ì  ê°ì‡  (30ì¼ ë°˜ê°ê¸°)
        return np.exp(-age_days / 30.0)

    def _count_exact_matches(self, query: str, content: str) -> int:
        """ì •í™•í•œ ë‹¨ì–´ ë§¤ì¹­ ìˆ˜"""
        query_terms = set(query.lower().split())
        content_terms = set(content.lower().split())
        return len(query_terms.intersection(content_terms))

    def _count_synonym_matches(self, query: str, content: str) -> int:
        """ë™ì˜ì–´ ë§¤ì¹­ ìˆ˜ (ê°„ë‹¨í•œ ì˜ˆì œ)"""
        # ì‹¤ì œë¡œëŠ” WordNetì´ë‚˜ ì‚¬ì „ í•™ìŠµëœ ë™ì˜ì–´ ì‚¬ì „ ì‚¬ìš©
        synonym_dict = {
            'python': ['íŒŒì´ì¬', 'python3', 'py'],
            'machine learning': ['ml', 'ë¨¸ì‹ ëŸ¬ë‹', 'ê¸°ê³„í•™ìŠµ'],
            'deep learning': ['dl', 'ë”¥ëŸ¬ë‹', 'ì‹¬ì¸µí•™ìŠµ']
        }

        count = 0
        query_lower = query.lower()
        content_lower = content.lower()

        for term, synonyms in synonym_dict.items():
            if term in query_lower:
                for syn in synonyms:
                    if syn in content_lower:
                        count += 1

        return count

    def _calculate_entity_overlap(self, query: str, content: str) -> float:
        """ëª…ëª…ëœ ê°œì²´ ì¤‘ì²©ë„ (ê°„ë‹¨í•œ ë²„ì „)"""
        # ì‹¤ì œë¡œëŠ” NER ëª¨ë¸ ì‚¬ìš©
        # ì—¬ê¸°ì„œëŠ” ëŒ€ë¬¸ìë¡œ ì‹œì‘í•˜ëŠ” ë‹¨ì–´ë¥¼ ê°œì²´ë¡œ ê°€ì •
        query_entities = set(w for w in query.split() if w[0].isupper())
        content_entities = set(w for w in content.split() if w[0].isupper())

        if not query_entities:
            return 0.0

        overlap = len(query_entities.intersection(content_entities))
        return overlap / len(query_entities)

    def _calculate_sentiment_alignment(self, query: str, document: Dict) -> float:
        """ê°ì„± ì •ë ¬ë„ (ì§ˆë¬¸ê³¼ ë‹µë³€ì˜ í†¤ ì¼ì¹˜)"""
        # ê°„ë‹¨í•œ ê·œì¹™ ê¸°ë°˜ (ì‹¤ì œë¡œëŠ” ê°ì„± ë¶„ì„ ëª¨ë¸ ì‚¬ìš©)
        positive_words = {'good', 'best', 'excellent', 'great', 'ì¢‹ì€', 'ìµœê³ '}
        negative_words = {'bad', 'worst', 'poor', 'terrible', 'ë‚˜ìœ', 'ìµœì•…'}

        query_sentiment = sum(1 for w in query.split() if w in positive_words) - \
                         sum(1 for w in query.split() if w in negative_words)

        doc_sentiment = sum(1 for w in document['content'].split() if w in positive_words) - \
                       sum(1 for w in document['content'].split() if w in negative_words)

        # ê°™ì€ ë¶€í˜¸ë©´ 1, ë‹¤ë¥¸ ë¶€í˜¸ë©´ 0
        if query_sentiment * doc_sentiment >= 0:
            return 1.0
        return 0.0

    def features_to_vector(self, features: RankingFeatures) -> np.ndarray:
        """íŠ¹ì§•ì„ ë²¡í„°ë¡œ ë³€í™˜"""
        return np.array([
            features.bm25_score,
            features.vector_similarity,
            features.cross_encoder_score,
            features.doc_length / 1000.0,  # ì •ê·œí™”
            features.doc_freshness,
            features.doc_popularity,
            features.exact_match_count / 10.0,  # ì •ê·œí™”
            features.synonym_match_count / 5.0,  # ì •ê·œí™”
            features.named_entity_overlap,
            features.topic_similarity,
            features.sentiment_alignment,
            features.user_click_history,
            features.session_dwell_time / 60.0  # ë¶„ ë‹¨ìœ„ë¡œ ì •ê·œí™”
        ])

    def train_model(self, training_data: List[Tuple[str, List[Dict], List[int]]],
                   validation_data: Optional[List] = None):
        """
        LambdaMART ëª¨ë¸ í•™ìŠµ

        Args:
            training_data: [(query, documents, relevance_labels)]
            validation_data: ê²€ì¦ ë°ì´í„° (ì„ íƒì‚¬í•­)
        """
        # íŠ¹ì§• ì¶”ì¶œ
        X_train = []
        y_train = []
        qids = []

        for qid, (query, documents, labels) in enumerate(training_data):
            for doc, label in zip(documents, labels):
                features = self.extract_features(query, doc)
                X_train.append(self.features_to_vector(features))
                y_train.append(label)
                qids.append(qid)

        X_train = np.array(X_train)
        y_train = np.array(y_train)
        qids = np.array(qids)

        # íŠ¹ì§• ìŠ¤ì¼€ì¼ë§
        X_train = self.scaler.fit_transform(X_train)

        # XGBoost DMatrix ìƒì„±
        dtrain = xgb.DMatrix(X_train, label=y_train)
        dtrain.set_group(np.bincount(qids))

        # LambdaMART íŒŒë¼ë¯¸í„°
        params = {
            'objective': 'rank:ndcg',
            'eval_metric': ['ndcg@10', 'map@10'],
            'eta': 0.1,
            'max_depth': 6,
            'min_child_weight': 1,
            'subsample': 0.8,
            'colsample_bytree': 0.8,
            'lambda': 1.0,
            'alpha': 0.0
        }

        # ëª¨ë¸ í•™ìŠµ
        self.model = xgb.train(
            params,
            dtrain,
            num_boost_round=300,
            early_stopping_rounds=50,
            verbose_eval=50
        )

        # Feature importance ë¶„ì„
        self._analyze_feature_importance()

    def _analyze_feature_importance(self):
        """íŠ¹ì§• ì¤‘ìš”ë„ ë¶„ì„"""
        if self.model is None:
            return

        importance = self.model.get_score(importance_type='gain')

        # Feature name ë§¤í•‘
        feature_importance = {}
        for i, name in enumerate(self.feature_names):
            key = f'f{i}'
            if key in importance:
                feature_importance[name] = importance[key]

        # ì¤‘ìš”ë„ ìˆœìœ¼ë¡œ ì •ë ¬
        sorted_importance = sorted(
            feature_importance.items(),
            key=lambda x: x[1],
            reverse=True
        )

        print("=== Feature Importance ===")
        for feature, score in sorted_importance[:10]:
            print(f"{feature}: {score:.2f}")

    def predict_ranking_scores(self, query: str, documents: List[Dict],
                             user_context: Optional[Dict] = None) -> List[float]:
        """
        ë¬¸ì„œë“¤ì˜ ë­í‚¹ ì ìˆ˜ ì˜ˆì¸¡
        """
        if self.model is None:
            # ëª¨ë¸ì´ ì—†ìœ¼ë©´ ê¸°ë³¸ ì ìˆ˜ ì‚¬ìš©
            return [doc.get('score', 0.0) for doc in documents]

        # íŠ¹ì§• ì¶”ì¶œ
        features = []
        for doc in documents:
            feat = self.extract_features(query, doc, user_context)
            features.append(self.features_to_vector(feat))

        # ìŠ¤ì¼€ì¼ë§
        X = self.scaler.transform(np.array(features))

        # ì˜ˆì¸¡
        dtest = xgb.DMatrix(X)
        scores = self.model.predict(dtest)

        return scores.tolist()

    def rerank_with_ltr(self, query: str, documents: List[Dict],
                       user_context: Optional[Dict] = None) -> List[Dict]:
        """
        Learning to Rankë¥¼ ì‚¬ìš©í•œ ì¬ìˆœìœ„í™”
        """
        # ì ìˆ˜ ì˜ˆì¸¡
        scores = self.predict_ranking_scores(query, documents, user_context)

        # ë¬¸ì„œì™€ ì ìˆ˜ ê²°í•©
        for doc, score in zip(documents, scores):
            doc['ltr_score'] = score

        # ì ìˆ˜ìˆœ ì •ë ¬
        reranked = sorted(documents, key=lambda x: x['ltr_score'], reverse=True)

        return reranked

    def collect_feedback(self, query: str, documents: List[Dict],
                        clicks: List[int], dwell_times: List[float]):
        """
        ì‚¬ìš©ì í”¼ë“œë°± ìˆ˜ì§‘ (ì˜¨ë¼ì¸ í•™ìŠµìš©)
        """
        # í´ë¦­ê³¼ ì²´ë¥˜ì‹œê°„ì„ ê¸°ë°˜ìœ¼ë¡œ ê´€ë ¨ì„± ë ˆì´ë¸” ìƒì„±
        relevance_labels = []
        for click, dwell_time in zip(clicks, dwell_times):
            if not click:
                label = 0
            elif dwell_time < 10:  # 10ì´ˆ ë¯¸ë§Œ
                label = 1
            elif dwell_time < 30:  # 30ì´ˆ ë¯¸ë§Œ
                label = 2
            else:
                label = 3  # ë†’ì€ ê´€ë ¨ì„±

            relevance_labels.append(label)

        # í•™ìŠµ ë²„í¼ì— ì¶”ê°€
        self.training_buffer.append((query, documents, relevance_labels))

        # ë²„í¼ê°€ ê°€ë“ ì°¨ë©´ ì¬í•™ìŠµ
        if len(self.training_buffer) >= self.buffer_size:
            self._retrain_online()

    def _retrain_online(self):
        """ì˜¨ë¼ì¸ ì¬í•™ìŠµ"""
        print("Online retraining with {} examples...".format(len(self.training_buffer)))

        # ê¸°ì¡´ ëª¨ë¸ì„ ë² ì´ìŠ¤ë¡œ ì¶”ê°€ í•™ìŠµ
        self.train_model(self.training_buffer)

        # ë²„í¼ ì´ˆê¸°í™”
        self.training_buffer = []

# í†µí•© ì¬ìˆœìœ„í™” ì‹œìŠ¤í…œ
class UnifiedRerankingSystem:
    def __init__(self):
        """
        ëª¨ë“  ì¬ìˆœìœ„í™” ê¸°ë²•ì„ í†µí•©í•œ ì‹œìŠ¤í…œ
        """
        self.cross_encoder = AdvancedCrossEncoderReranker()
        self.colbert = ColBERTv2(ColBERTConfig())
        self.diversity_reranker = DiversityAwareReranker()
        self.ltr_model = LearningToRankRAG()

    def rerank(self, query: str, initial_results: List[Dict],
              reranking_strategy: str = 'hybrid',
              user_context: Optional[Dict] = None) -> List[Dict]:
        """
        í†µí•© ì¬ìˆœìœ„í™” ìˆ˜í–‰

        Strategies:
        - 'cross_encoder': Cross-Encoderë§Œ ì‚¬ìš©
        - 'colbert': ColBERTë§Œ ì‚¬ìš©
        - 'diversity': MMR ë‹¤ì–‘ì„± ì¬ìˆœìœ„í™”
        - 'ltr': Learning to Rank
        - 'hybrid': ëª¨ë“  ê¸°ë²• ì¡°í•© (ê¸°ë³¸ê°’)
        """
        if reranking_strategy == 'cross_encoder':
            return self._rerank_cross_encoder(query, initial_results)

        elif reranking_strategy == 'colbert':
            return self._rerank_colbert(query, initial_results)

        elif reranking_strategy == 'diversity':
            return self.diversity_reranker.mmr_rerank(query, initial_results)

        elif reranking_strategy == 'ltr':
            return self.ltr_model.rerank_with_ltr(query, initial_results, user_context)

        elif reranking_strategy == 'hybrid':
            # 1ë‹¨ê³„: Cross-Encoderë¡œ Top-30 ì¬ìˆœìœ„í™”
            ce_results = self._rerank_cross_encoder(query, initial_results[:50])[:30]

            # 2ë‹¨ê³„: LTR ëª¨ë¸ë¡œ ì ìˆ˜ ì¡°ì •
            if self.ltr_model.model is not None:
                ltr_results = self.ltr_model.rerank_with_ltr(query, ce_results, user_context)
            else:
                ltr_results = ce_results

            # 3ë‹¨ê³„: MMRë¡œ ìµœì¢… ë‹¤ì–‘ì„± í™•ë³´
            final_results = self.diversity_reranker.mmr_rerank(
                query, ltr_results[:20], top_k=10
            )

            return final_results

        else:
            raise ValueError(f"Unknown strategy: {reranking_strategy}")

    def _rerank_cross_encoder(self, query: str, documents: List[Dict]) -> List[Dict]:
        """Cross-Encoder ì¬ìˆœìœ„í™” ë˜í¼"""
        results = self.cross_encoder.rerank(query, documents)
        return [
            {
                'id': r.doc_id,
                'content': r.content,
                'score': r.reranked_score
            }
            for r in results
        ]

    def _rerank_colbert(self, query: str, documents: List[Dict]) -> List[Dict]:
        """ColBERT ì¬ìˆœìœ„í™”"""
        # ColBERT ì¸ë±ìŠ¤ êµ¬ì¶• (ì‹¤ì œë¡œëŠ” ì‚¬ì „ êµ¬ì¶•)
        doc_contents = [d['content'] for d in documents]
        index, boundaries = self.colbert.build_index(doc_contents)

        # ê²€ìƒ‰ ìˆ˜í–‰
        results = self.colbert.retrieve(query, index, boundaries)

        # ê²°ê³¼ í¬ë§·íŒ…
        reranked = []
        for doc_id, score in results:
            reranked.append({
                'id': documents[doc_id]['id'],
                'content': documents[doc_id]['content'],
                'score': score
            })

        return reranked

# ì‚¬ìš© ì˜ˆì œ
print("=== ê³ ê¸‰ ì¬ìˆœìœ„í™” ì‹œìŠ¤í…œ ë°ëª¨ ===\\n")

# ì´ˆê¸° ê²€ìƒ‰ ê²°ê³¼
initial_results = [
    {'id': '1', 'content': 'Cross-EncoderëŠ” ì¿¼ë¦¬ì™€ ë¬¸ì„œë¥¼ í•¨ê»˜ ì¸ì½”ë”©í•©ë‹ˆë‹¤.', 'score': 0.89},
    {'id': '2', 'content': 'ColBERTëŠ” í† í° ë ˆë²¨ ìƒí˜¸ì‘ìš©ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.', 'score': 0.87},
    {'id': '3', 'content': 'MMRì€ ë‹¤ì–‘ì„±ì„ ìœ„í•œ ì¬ìˆœìœ„í™” ì•Œê³ ë¦¬ì¦˜ì…ë‹ˆë‹¤.', 'score': 0.85},
    {'id': '4', 'content': 'Learning to RankëŠ” ê¸°ê³„í•™ìŠµ ê¸°ë°˜ ìˆœìœ„ ìµœì í™”ì…ë‹ˆë‹¤.', 'score': 0.83},
    {'id': '5', 'content': 'BERT ê¸°ë°˜ ê²€ìƒ‰ ëª¨ë¸ë“¤ì˜ ì„±ëŠ¥ ë¹„êµ.', 'score': 0.82},
]

# í†µí•© ì‹œìŠ¤í…œ ì´ˆê¸°í™”
unified_system = UnifiedRerankingSystem()

# ë‹¤ì–‘í•œ ì „ëµìœ¼ë¡œ ì¬ìˆœìœ„í™”
strategies = ['cross_encoder', 'diversity', 'hybrid']

for strategy in strategies:
    print(f"\\n=== {strategy.upper()} ì „ëµ ê²°ê³¼ ===")
    results = unified_system.rerank(
        "ì¬ìˆœìœ„í™” ì•Œê³ ë¦¬ì¦˜ì˜ ì¢…ë¥˜",
        initial_results,
        reranking_strategy=strategy
    )

    for i, doc in enumerate(results[:5], 1):
        print(f"{i}. [{doc['id']}] {doc['content'][:50]}... (ì ìˆ˜: {doc.get('score', 0):.3f})")`}
              </pre>
            </div>
          </div>
        </div>
      </section>

      {/* Practical Exercise */}
      <section className="bg-gradient-to-r from-orange-500 to-red-600 rounded-2xl p-8 text-white">
        <h2 className="text-2xl font-bold mb-6">ì‹¤ìŠµ ê³¼ì œ</h2>

        <div className="bg-white/10 rounded-xl p-6 backdrop-blur">
          <h3 className="font-bold mb-4">ê³ ê¸‰ ì¬ìˆœìœ„í™” ì‹œìŠ¤í…œ êµ¬ì¶•</h3>

          <div className="space-y-4">
            <div className="bg-white/10 p-4 rounded-lg">
              <h4 className="font-medium mb-2">ğŸ“‹ ìš”êµ¬ì‚¬í•­</h4>
              <ol className="space-y-2 text-sm">
                <li>1. Cross-Encoderì™€ ColBERTë¥¼ ëª¨ë‘ í™œìš©í•œ í•˜ì´ë¸Œë¦¬ë“œ ì‹œìŠ¤í…œ êµ¬í˜„</li>
                <li>2. ì¿¼ë¦¬ ìœ í˜•ë³„ ìµœì  ì¬ìˆœìœ„í™” ì „ëµ ìë™ ì„ íƒ</li>
                <li>3. A/B í…ŒìŠ¤íŠ¸ë¥¼ í†µí•œ Î» íŒŒë¼ë¯¸í„° ìµœì í™”</li>
                <li>4. ì‹¤ì‹œê°„ ì‚¬ìš©ì í”¼ë“œë°± ê¸°ë°˜ ì˜¨ë¼ì¸ í•™ìŠµ</li>
                <li>5. ì¬ìˆœìœ„í™” ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ êµ¬ì¶•</li>
              </ol>
            </div>

            <div className="bg-white/10 p-4 rounded-lg">
              <h4 className="font-medium mb-2">ğŸ¯ ì„±ëŠ¥ ëª©í‘œ</h4>
              <ul className="space-y-1 text-sm">
                <li>â€¢ MRR@10: 0.4 ì´ìƒ</li>
                <li>â€¢ NDCG@10: 0.6 ì´ìƒ</li>
                <li>â€¢ ì¬ìˆœìœ„í™” ë ˆì´í„´ì‹œ: &lt; 100ms (P95)</li>
                <li>â€¢ ë‹¤ì–‘ì„± ì ìˆ˜: 0.7 ì´ìƒ</li>
              </ul>
            </div>

            <div className="bg-white/10 p-4 rounded-lg">
              <h4 className="font-medium mb-2">ğŸ’¡ ë„ì „ ê³¼ì œ</h4>
              <p className="text-sm">
                ë‹¤êµ­ì–´ ì¬ìˆœìœ„í™” ì§€ì›ì„ ì¶”ê°€í•˜ì—¬ í•œêµ­ì–´, ì˜ì–´, ì¼ë³¸ì–´ ì¿¼ë¦¬ì— ëŒ€í•´
                ì–¸ì–´ë³„ ìµœì í™”ëœ ì¬ìˆœìœ„í™”ë¥¼ ìˆ˜í–‰í•˜ëŠ” ì‹œìŠ¤í…œìœ¼ë¡œ í™•ì¥í•´ë³´ì„¸ìš”.
                íŠ¹íˆ Cross-lingual ê²€ìƒ‰ì—ì„œì˜ ì¬ìˆœìœ„í™” ì „ëµì„ ê³ ë¯¼í•´ë³´ì„¸ìš”.
              </p>
            </div>
          </div>
        </div>
      </section>

      {/* References */}
      <References
        sections={[
          {
            title: 'ğŸ“š Cross-Encoder & Reranking ëª¨ë¸',
            icon: 'web' as const,
            color: 'border-orange-500',
            items: [
              {
                title: 'Sentence-Transformers Reranking',
                authors: 'UKPLab',
                year: '2024',
                description: 'Cross-Encoder ê³µì‹ ë¼ì´ë¸ŒëŸ¬ë¦¬ - ms-marco-MiniLM, mxbai-rerank',
                link: 'https://www.sbert.net/docs/pretrained_cross-encoders.html'
              },
              {
                title: 'Cohere Rerank API',
                authors: 'Cohere',
                year: '2025',
                description: 'State-of-the-art ì¬ìˆœìœ„í™” - ë‹¤êµ­ì–´ ì§€ì›, ì‹¤ì‹œê°„ API',
                link: 'https://docs.cohere.com/docs/rerank'
              },
              {
                title: 'Jina Reranker Models',
                authors: 'Jina AI',
                year: '2024',
                description: 'ê²½ëŸ‰ ì¬ìˆœìœ„í™” ëª¨ë¸ - jina-reranker-v1-base-en',
                link: 'https://jina.ai/reranker'
              },
              {
                title: 'RankGPT: LLM-based Reranking',
                authors: 'Microsoft Research',
                year: '2023',
                description: 'GPTë¥¼ í™œìš©í•œ ì¬ìˆœìœ„í™” - Zero-shot Relevance íŒë‹¨',
                link: 'https://github.com/sunnweiwei/RankGPT'
              },
              {
                title: 'BGE Reranker by BAAI',
                authors: 'Beijing Academy of AI',
                year: '2024',
                description: 'ê³ ì„±ëŠ¥ ì¤‘êµ­ì–´/ì˜ì–´ ì¬ìˆœìœ„í™” - bge-reranker-large',
                link: 'https://huggingface.co/BAAI/bge-reranker-large'
              }
            ]
          },
          {
            title: 'ğŸ“– Learning-to-Rank ì—°êµ¬',
            icon: 'research' as const,
            color: 'border-red-500',
            items: [
              {
                title: 'Sentence-BERT: Cross-Encoders for Semantic Search',
                authors: 'Reimers & Gurevych',
                year: '2019',
                description: 'Cross-Encoder ê¸°ë³¸ ë…¼ë¬¸ - BERT ê¸°ë°˜ ë¬¸ì¥ ìŒ ë¶„ë¥˜',
                link: 'https://arxiv.org/abs/1908.10084'
              },
              {
                title: 'ColBERT: Efficient and Effective Passage Search',
                authors: 'Khattab & Zaharia, Stanford',
                year: '2020',
                description: 'Late Interaction - Bi-Encoder + Cross-Encoder ì¥ì  ê²°í•©',
                link: 'https://arxiv.org/abs/2004.12832'
              },
              {
                title: 'RankNet to LambdaRank to LambdaMART',
                authors: 'Burges et al., Microsoft Research',
                year: '2010',
                description: 'Learning-to-Rank ì•Œê³ ë¦¬ì¦˜ ë°œì „ì‚¬ - Pairwise to Listwise',
                link: 'https://www.microsoft.com/en-us/research/publication/from-ranknet-to-lambdarank-to-lambdamart-an-overview/'
              },
              {
                title: 'MonoT5 & DuoT5: T5-based Reranking',
                authors: 'Nogueira et al., University of Waterloo',
                year: '2020',
                description: 'T5ë¡œ ì¬ìˆœìœ„í™” - Text-to-Text Relevance íŒë‹¨',
                link: 'https://arxiv.org/abs/2003.06713'
              }
            ]
          },
          {
            title: 'ğŸ› ï¸ ì‹¤ë¬´ ì¬ìˆœìœ„í™” ë„êµ¬',
            icon: 'tools' as const,
            color: 'border-green-500',
            items: [
              {
                title: 'Haystack Ranker Component',
                authors: 'deepset',
                year: '2024',
                description: 'RAG íŒŒì´í”„ë¼ì¸ ì¬ìˆœìœ„í™” - SentenceTransformers í†µí•©',
                link: 'https://docs.haystack.deepset.ai/docs/ranker'
              },
              {
                title: 'LlamaIndex Reranking Postprocessor',
                authors: 'LlamaIndex',
                year: '2024',
                description: 'Query Engine ì¬ìˆœìœ„í™” - Cohere, SentenceTransformers ì§€ì›',
                link: 'https://docs.llamaindex.ai/en/stable/examples/node_postprocessor/CohereRerank.html'
              },
              {
                title: 'Vespa Ranking Framework',
                authors: 'Vespa.ai',
                year: '2024',
                description: 'ëŒ€ê·œëª¨ ì¬ìˆœìœ„í™” ì—”ì§„ - ONNX ëª¨ë¸, LightGBM, XGBoost ì§€ì›',
                link: 'https://docs.vespa.ai/en/ranking.html'
              },
              {
                title: 'FlashRank: Fast Reranking',
                authors: 'PrithivirajDamodaran',
                year: '2024',
                description: 'ì´ˆê²½ëŸ‰ ì¬ìˆœìœ„í™” - CPUì—ì„œ ë¹ ë¥¸ ì¶”ë¡ , 40MB ëª¨ë¸',
                link: 'https://github.com/PrithivirajDamodaran/FlashRank'
              },
              {
                title: 'Rank-BM25 Python Library',
                authors: 'dorianbrown',
                year: '2024',
                description: 'BM25 ì¬ìˆœìœ„í™” - ê²½ëŸ‰ í‚¤ì›Œë“œ ê¸°ë°˜ ì¬ì ìˆ˜í™”',
                link: 'https://github.com/dorianbrown/rank_bm25'
              }
            ]
          }
        ]}
      />

      {/* Navigation */}
      <div className="mt-12 bg-white dark:bg-gray-800 rounded-2xl p-6 shadow-sm border border-gray-200 dark:border-gray-700">
        <div className="flex justify-between items-center">
          <Link
            href="/modules/rag/advanced/chapter3"
            className="inline-flex items-center gap-2 text-gray-600 dark:text-gray-400 hover:text-gray-900 dark:hover:text-white transition-colors"
          >
            <ArrowLeft size={16} />
            ì´ì „: ë¶„ì‚° RAG ì‹œìŠ¤í…œ
          </Link>

          <Link
            href="/modules/rag/advanced/chapter5"
            className="inline-flex items-center gap-2 bg-orange-500 text-white px-6 py-3 rounded-lg font-medium hover:bg-orange-600 transition-colors"
          >
            ë‹¤ìŒ: RAG í‰ê°€ì™€ ëª¨ë‹ˆí„°ë§
            <ArrowRight size={16} />
          </Link>
        </div>
      </div>
    </>
  )
}
