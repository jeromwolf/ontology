'use client'

import { Gauge } from 'lucide-react'

export default function Section1() {
  return (
    <section className="bg-white dark:bg-gray-800 rounded-2xl p-8 shadow-sm border border-gray-200 dark:border-gray-700">
      <div className="flex items-center gap-3 mb-6">
        <div className="w-12 h-12 rounded-xl bg-emerald-100 dark:bg-emerald-900/20 flex items-center justify-center">
          <Gauge className="text-emerald-600" size={24} />
        </div>
        <div>
          <h2 className="text-2xl font-bold text-gray-900 dark:text-white">5.1 RAG ì‹œìŠ¤í…œ í‰ê°€ ë©”íŠ¸ë¦­ ì²´ê³„</h2>
          <p className="text-gray-600 dark:text-gray-400">ê²€ìƒ‰ê³¼ ìƒì„±ì„ ëª¨ë‘ ê³ ë ¤í•œ ì¢…í•©ì  í‰ê°€ í”„ë ˆì„ì›Œí¬</p>
        </div>
      </div>

      <div className="space-y-6">
        <div className="bg-emerald-50 dark:bg-emerald-900/20 p-6 rounded-xl border border-emerald-200 dark:border-emerald-700">
          <h3 className="font-bold text-emerald-800 dark:text-emerald-200 mb-4">RAG í‰ê°€ì˜ 3ì°¨ì› ì ‘ê·¼ë²•</h3>

          <div className="prose prose-sm dark:prose-invert mb-4">
            <p className="text-gray-700 dark:text-gray-300">
              <strong>RAG ì‹œìŠ¤í…œì€ ê²€ìƒ‰(Retrieval)ê³¼ ìƒì„±(Generation) ë‘ ê°€ì§€ í•µì‹¬ ì»´í¬ë„ŒíŠ¸ë¡œ êµ¬ì„±ë˜ë©°,
              ê°ê°ì„ ë…ë¦½ì ìœ¼ë¡œ í‰ê°€í•˜ëŠ” ë™ì‹œì— ì „ì²´ ì‹œìŠ¤í…œì˜ ì„±ëŠ¥ë„ ì¸¡ì •í•´ì•¼ í•©ë‹ˆë‹¤.</strong>
              Microsoft, Google, OpenAIì˜ ì‹¤ì œ í”„ë¡œë•ì…˜ ê²½í—˜ì„ ë°”íƒ•ìœ¼ë¡œ í•œ í¬ê´„ì  í‰ê°€ ì²´ê³„ë¥¼ ì†Œê°œí•©ë‹ˆë‹¤.
            </p>
          </div>

          <div className="grid md:grid-cols-3 gap-4">
            <div className="bg-white dark:bg-gray-800 p-4 rounded-lg border">
              <h4 className="font-medium text-emerald-600 dark:text-emerald-400 mb-2">ğŸ” Retrieval Quality</h4>
              <ul className="text-sm text-gray-700 dark:text-gray-300 space-y-1">
                <li>â€¢ Precision@K</li>
                <li>â€¢ Recall@K</li>
                <li>â€¢ MRR (Mean Reciprocal Rank)</li>
                <li>â€¢ NDCG (Normalized DCG)</li>
                <li>â€¢ Coverage & Diversity</li>
              </ul>
            </div>

            <div className="bg-white dark:bg-gray-800 p-4 rounded-lg border">
              <h4 className="font-medium text-blue-600 dark:text-blue-400 mb-2">ğŸ“ Generation Quality</h4>
              <ul className="text-sm text-gray-700 dark:text-gray-300 space-y-1">
                <li>â€¢ Relevance Score</li>
                <li>â€¢ Faithfulness</li>
                <li>â€¢ Answer Correctness</li>
                <li>â€¢ Coherence & Fluency</li>
                <li>â€¢ Hallucination Rate</li>
              </ul>
            </div>

            <div className="bg-white dark:bg-gray-800 p-4 rounded-lg border">
              <h4 className="font-medium text-purple-600 dark:text-purple-400 mb-2">âš¡ System Performance</h4>
              <ul className="text-sm text-gray-700 dark:text-gray-300 space-y-1">
                <li>â€¢ End-to-end Latency</li>
                <li>â€¢ Throughput (QPS)</li>
                <li>â€¢ Resource Utilization</li>
                <li>â€¢ Cost per Query</li>
                <li>â€¢ Error Rate</li>
              </ul>
            </div>
          </div>
        </div>

        <div className="bg-blue-50 dark:bg-blue-900/20 p-6 rounded-xl border border-blue-200 dark:border-blue-700">
          <h3 className="font-bold text-blue-800 dark:text-blue-200 mb-4">RAGAS Framework êµ¬í˜„</h3>

          <div className="bg-slate-50 dark:bg-slate-800 p-4 rounded-lg border border-slate-200 dark:border-slate-700 overflow-x-auto">
            <pre className="text-sm text-slate-800 dark:text-slate-200 font-mono">
{`from dataclasses import dataclass
from typing import List, Dict, Tuple, Optional, Any
import numpy as np
from sklearn.metrics import ndcg_score
import torch
from transformers import AutoTokenizer, AutoModel
from sentence_transformers import CrossEncoder
import asyncio
from datetime import datetime
import json

@dataclass
class RAGEvaluationSample:
    """RAG í‰ê°€ë¥¼ ìœ„í•œ ë°ì´í„° ìƒ˜í”Œ"""
    query: str
    true_answer: str
    retrieved_documents: List[Dict[str, Any]]
    generated_answer: str
    ground_truth_docs: List[str]  # ì •ë‹µ ë¬¸ì„œ ID
    metadata: Dict[str, Any] = None

class ComprehensiveRAGEvaluator:
    def __init__(self,
                 embedding_model: str = 'intfloat/multilingual-e5-large',
                 cross_encoder_model: str = 'cross-encoder/ms-marco-MiniLM-L-6-v2',
                 nli_model: str = 'microsoft/deberta-v3-base-nli'):
        """
        í¬ê´„ì  RAG í‰ê°€ ì‹œìŠ¤í…œ
        - Retrieval, Generation, System ë©”íŠ¸ë¦­ í†µí•©
        - ë‹¤êµ­ì–´ ì§€ì›
        - ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ í†µí•©
        """
        # ëª¨ë¸ ì´ˆê¸°í™”
        self.tokenizer = AutoTokenizer.from_pretrained(embedding_model)
        self.embedding_model = AutoModel.from_pretrained(embedding_model)
        self.cross_encoder = CrossEncoder(cross_encoder_model)
        self.nli_model = CrossEncoder(nli_model)

        # í‰ê°€ ê²°ê³¼ ì €ì¥
        self.evaluation_history = []

    def evaluate_sample(self, sample: RAGEvaluationSample) -> Dict[str, float]:
        """ë‹¨ì¼ ìƒ˜í”Œì— ëŒ€í•œ ì „ì²´ í‰ê°€"""
        metrics = {}

        # 1. Retrieval í‰ê°€
        retrieval_metrics = self._evaluate_retrieval(sample)
        metrics.update(retrieval_metrics)

        # 2. Generation í‰ê°€
        generation_metrics = self._evaluate_generation(sample)
        metrics.update(generation_metrics)

        # 3. ì¢…í•© ì ìˆ˜ ê³„ì‚°
        metrics['overall_score'] = self._calculate_overall_score(metrics)

        return metrics

    def _evaluate_retrieval(self, sample: RAGEvaluationSample) -> Dict[str, float]:
        """ê²€ìƒ‰ í’ˆì§ˆ í‰ê°€"""
        retrieved_ids = [doc['id'] for doc in sample.retrieved_documents]
        ground_truth_ids = sample.ground_truth_docs

        # Precision@K
        k_values = [1, 3, 5, 10]
        precisions = {}
        for k in k_values:
            if len(retrieved_ids) >= k:
                relevant_at_k = sum(1 for doc_id in retrieved_ids[:k]
                                   if doc_id in ground_truth_ids)
                precisions[f'precision@{k}'] = relevant_at_k / k
            else:
                precisions[f'precision@{k}'] = 0.0

        # Recall@K
        recalls = {}
        for k in k_values:
            if len(ground_truth_ids) > 0:
                relevant_at_k = sum(1 for doc_id in retrieved_ids[:k]
                                   if doc_id in ground_truth_ids)
                recalls[f'recall@{k}'] = relevant_at_k / len(ground_truth_ids)
            else:
                recalls[f'recall@{k}'] = 0.0

        # MRR (Mean Reciprocal Rank)
        mrr = 0.0
        for i, doc_id in enumerate(retrieved_ids):
            if doc_id in ground_truth_ids:
                mrr = 1.0 / (i + 1)
                break

        # NDCG
        relevance_scores = []
        for doc in sample.retrieved_documents:
            if doc['id'] in ground_truth_ids:
                relevance_scores.append(1.0)
            else:
                # Semantic similarity as soft relevance
                sim = self._calculate_semantic_similarity(
                    sample.query, doc['content']
                )
                relevance_scores.append(sim)

        ideal_scores = sorted(relevance_scores, reverse=True)
        if len(relevance_scores) > 0 and sum(ideal_scores) > 0:
            ndcg = ndcg_score([ideal_scores], [relevance_scores])
        else:
            ndcg = 0.0

        # Coverage (ê²€ìƒ‰ëœ ê´€ë ¨ ë¬¸ì„œì˜ ë¹„ìœ¨)
        coverage = len(set(retrieved_ids) & set(ground_truth_ids)) / \
                  max(len(ground_truth_ids), 1)

        # Diversity (ê²€ìƒ‰ ê²°ê³¼ì˜ ë‹¤ì–‘ì„±)
        diversity = self._calculate_diversity(sample.retrieved_documents)

        return {
            **precisions,
            **recalls,
            'mrr': mrr,
            'ndcg': ndcg,
            'coverage': coverage,
            'diversity': diversity
        }

    def _evaluate_generation(self, sample: RAGEvaluationSample) -> Dict[str, float]:
        """ìƒì„± í’ˆì§ˆ í‰ê°€"""
        metrics = {}

        # 1. Answer Relevance (ì¿¼ë¦¬ì™€ ë‹µë³€ì˜ ê´€ë ¨ì„±)
        relevance = self._calculate_semantic_similarity(
            sample.query, sample.generated_answer
        )
        metrics['answer_relevance'] = relevance

        # 2. Faithfulness (ê²€ìƒ‰ëœ ë¬¸ì„œì— ëŒ€í•œ ì¶©ì‹¤ë„)
        faithfulness = self._evaluate_faithfulness(
            sample.generated_answer,
            sample.retrieved_documents
        )
        metrics['faithfulness'] = faithfulness

        # 3. Answer Correctness (ì •ë‹µê³¼ì˜ ì¼ì¹˜ë„)
        correctness = self._evaluate_correctness(
            sample.generated_answer,
            sample.true_answer
        )
        metrics['answer_correctness'] = correctness

        # 4. Hallucination Detection
        hallucination_score = self._detect_hallucination(
            sample.generated_answer,
            sample.retrieved_documents
        )
        metrics['hallucination_score'] = hallucination_score

        # 5. Coherence and Fluency
        coherence = self._evaluate_coherence(sample.generated_answer)
        metrics['coherence'] = coherence

        return metrics

    def _calculate_semantic_similarity(self, text1: str, text2: str) -> float:
        """ì˜ë¯¸ì  ìœ ì‚¬ë„ ê³„ì‚°"""
        # ì„ë² ë”© ìƒì„±
        inputs1 = self.tokenizer(text1, return_tensors='pt',
                                padding=True, truncation=True)
        inputs2 = self.tokenizer(text2, return_tensors='pt',
                                padding=True, truncation=True)

        with torch.no_grad():
            emb1 = self.embedding_model(**inputs1).last_hidden_state.mean(dim=1)
            emb2 = self.embedding_model(**inputs2).last_hidden_state.mean(dim=1)

        # Cosine similarity
        similarity = torch.cosine_similarity(emb1, emb2).item()
        return (similarity + 1) / 2  # Normalize to [0, 1]

    def _evaluate_faithfulness(self, answer: str, documents: List[Dict]) -> float:
        """ë‹µë³€ì´ ê²€ìƒ‰ëœ ë¬¸ì„œì— ì¶©ì‹¤í•œì§€ í‰ê°€"""
        # ë‹µë³€ì„ ë¬¸ì¥ìœ¼ë¡œ ë¶„ë¦¬
        answer_sentences = answer.split('. ')

        # ê° ë¬¸ì¥ì´ ë¬¸ì„œì—ì„œ ì§€ì›ë˜ëŠ”ì§€ í™•ì¸
        supported_count = 0
        total_sentences = len(answer_sentences)

        for sentence in answer_sentences:
            if not sentence.strip():
                continue

            max_support = 0.0
            for doc in documents:
                # NLI ëª¨ë¸ì„ ì‚¬ìš©í•œ entailment ê²€ì‚¬
                premise = doc['content']
                hypothesis = sentence

                # Cross-encoderë¡œ entailment score ê³„ì‚°
                score = self.nli_model.predict([[premise, hypothesis]])[0]
                max_support = max(max_support, score)

            # Threshold ì´ìƒì´ë©´ ì§€ì›ë¨
            if max_support > 0.5:
                supported_count += 1

        return supported_count / max(total_sentences, 1)

    def _detect_hallucination(self, answer: str, documents: List[Dict]) -> float:
        """í™˜ê°(hallucination) íƒì§€"""
        # ë‹µë³€ì—ì„œ êµ¬ì²´ì ì¸ ì‚¬ì‹¤/ìˆ˜ì¹˜ ì¶”ì¶œ (ê°„ë‹¨í•œ ê·œì¹™ ê¸°ë°˜)
        import re

        # ìˆ«ì, ë‚ ì§œ, ê³ ìœ ëª…ì‚¬ íŒ¨í„´
        fact_patterns = [
            r'\d+(?:\.\d+)?%?',  # ìˆ«ìì™€ ë°±ë¶„ìœ¨
            r'\d{4}ë…„',  # ì—°ë„
            r'[A-Z][a-z]+(?:\s[A-Z][a-z]+)*',  # ê³ ìœ ëª…ì‚¬
        ]

        facts = []
        for pattern in fact_patterns:
            facts.extend(re.findall(pattern, answer))

        if not facts:
            return 0.0  # êµ¬ì²´ì  ì‚¬ì‹¤ì´ ì—†ìœ¼ë©´ í™˜ê° ì—†ìŒ

        # ê° ì‚¬ì‹¤ì´ ë¬¸ì„œì—ì„œ ì–¸ê¸‰ë˜ëŠ”ì§€ í™•ì¸
        hallucinated = 0
        for fact in facts:
            found = False
            for doc in documents:
                if fact in doc['content']:
                    found = True
                    break

            if not found:
                hallucinated += 1

        return hallucinated / len(facts) if facts else 0.0

    def _evaluate_correctness(self, generated: str, ground_truth: str) -> float:
        """ì •ë‹µê³¼ì˜ ì¼ì¹˜ë„ í‰ê°€"""
        # 1. Semantic similarity
        semantic_score = self._calculate_semantic_similarity(generated, ground_truth)

        # 2. Token overlap (F1 score)
        generated_tokens = set(generated.lower().split())
        truth_tokens = set(ground_truth.lower().split())

        if not truth_tokens:
            return semantic_score

        overlap = generated_tokens.intersection(truth_tokens)
        precision = len(overlap) / len(generated_tokens) if generated_tokens else 0
        recall = len(overlap) / len(truth_tokens)
        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0

        # 3. Combined score
        return 0.7 * semantic_score + 0.3 * f1

    def _evaluate_coherence(self, text: str) -> float:
        """í…ìŠ¤íŠ¸ ì¼ê´€ì„± í‰ê°€"""
        sentences = text.split('. ')
        if len(sentences) < 2:
            return 1.0

        # ì—°ì†ëœ ë¬¸ì¥ ê°„ ì˜ë¯¸ì  ìœ ì‚¬ë„
        coherence_scores = []
        for i in range(len(sentences) - 1):
            if sentences[i].strip() and sentences[i+1].strip():
                sim = self._calculate_semantic_similarity(
                    sentences[i], sentences[i+1]
                )
                coherence_scores.append(sim)

        return np.mean(coherence_scores) if coherence_scores else 1.0

    def _calculate_diversity(self, documents: List[Dict]) -> float:
        """ë¬¸ì„œ ì§‘í•©ì˜ ë‹¤ì–‘ì„± ê³„ì‚°"""
        if len(documents) < 2:
            return 0.0

        # ëª¨ë“  ë¬¸ì„œ ìŒì˜ ìœ ì‚¬ë„ ê³„ì‚°
        similarities = []
        for i in range(len(documents)):
            for j in range(i + 1, len(documents)):
                sim = self._calculate_semantic_similarity(
                    documents[i]['content'],
                    documents[j]['content']
                )
                similarities.append(sim)

        # í‰ê·  ìœ ì‚¬ë„ê°€ ë‚®ì„ìˆ˜ë¡ ë‹¤ì–‘ì„±ì´ ë†’ìŒ
        avg_similarity = np.mean(similarities)
        diversity = 1 - avg_similarity

        return diversity

    def _calculate_overall_score(self, metrics: Dict[str, float]) -> float:
        """ì¢…í•© ì ìˆ˜ ê³„ì‚°"""
        # ê°€ì¤‘ì¹˜ ì„¤ì •
        weights = {
            'retrieval': 0.4,
            'generation': 0.5,
            'system': 0.1
        }

        # ì¹´í…Œê³ ë¦¬ë³„ ì ìˆ˜ ê³„ì‚°
        retrieval_score = np.mean([
            metrics.get('precision@10', 0),
            metrics.get('recall@10', 0),
            metrics.get('mrr', 0),
            metrics.get('ndcg', 0)
        ])

        generation_score = np.mean([
            metrics.get('answer_relevance', 0),
            metrics.get('faithfulness', 0),
            metrics.get('answer_correctness', 0),
            1 - metrics.get('hallucination_score', 0),  # í™˜ê°ì´ ì ì„ìˆ˜ë¡ ì¢‹ìŒ
            metrics.get('coherence', 0)
        ])

        # ì¢…í•© ì ìˆ˜
        overall = (weights['retrieval'] * retrieval_score +
                  weights['generation'] * generation_score)

        return overall

    def evaluate_dataset(self, samples: List[RAGEvaluationSample],
                        batch_size: int = 32) -> Dict[str, Any]:
        """ì „ì²´ ë°ì´í„°ì…‹ í‰ê°€"""
        all_metrics = []

        # ë°°ì¹˜ ì²˜ë¦¬
        for i in range(0, len(samples), batch_size):
            batch = samples[i:i+batch_size]

            # ë³‘ë ¬ ì²˜ë¦¬ë¥¼ ìœ„í•œ ë¹„ë™ê¸° ì‹¤í–‰
            batch_metrics = []
            for sample in batch:
                metrics = self.evaluate_sample(sample)
                batch_metrics.append(metrics)

            all_metrics.extend(batch_metrics)

        # ì§‘ê³„
        aggregated = self._aggregate_metrics(all_metrics)

        # ìƒì„¸ ë¶„ì„
        analysis = self._analyze_results(all_metrics, samples)

        return {
            'aggregated_metrics': aggregated,
            'detailed_analysis': analysis,
            'sample_metrics': all_metrics
        }

    def _aggregate_metrics(self, metrics_list: List[Dict[str, float]]) -> Dict[str, Dict[str, float]]:
        """ë©”íŠ¸ë¦­ ì§‘ê³„"""
        aggregated = {}

        # ëª¨ë“  ë©”íŠ¸ë¦­ ìˆ˜ì§‘
        all_metric_names = set()
        for metrics in metrics_list:
            all_metric_names.update(metrics.keys())

        # ê° ë©”íŠ¸ë¦­ì— ëŒ€í•´ í†µê³„ ê³„ì‚°
        for metric_name in all_metric_names:
            values = [m.get(metric_name, 0) for m in metrics_list]

            aggregated[metric_name] = {
                'mean': np.mean(values),
                'std': np.std(values),
                'min': np.min(values),
                'max': np.max(values),
                'median': np.median(values),
                'p95': np.percentile(values, 95)
            }

        return aggregated

    def _analyze_results(self, metrics_list: List[Dict[str, float]],
                        samples: List[RAGEvaluationSample]) -> Dict[str, Any]:
        """ê²°ê³¼ ìƒì„¸ ë¶„ì„"""
        analysis = {}

        # 1. ì„±ëŠ¥ ì €í•˜ ìƒ˜í”Œ ì‹ë³„
        overall_scores = [m['overall_score'] for m in metrics_list]
        threshold = np.percentile(overall_scores, 25)  # í•˜ìœ„ 25%

        poor_performing = []
        for i, (sample, metrics) in enumerate(zip(samples, metrics_list)):
            if metrics['overall_score'] < threshold:
                poor_performing.append({
                    'index': i,
                    'query': sample.query,
                    'score': metrics['overall_score'],
                    'issues': self._identify_issues(metrics)
                })

        analysis['poor_performing_samples'] = poor_performing[:10]  # Top 10

        # 2. ì¹´í…Œê³ ë¦¬ë³„ ê°•ì•½ì  ë¶„ì„
        retrieval_avg = np.mean([
            m.get('mrr', 0) for m in metrics_list
        ])
        generation_avg = np.mean([
            m.get('answer_correctness', 0) for m in metrics_list
        ])

        if retrieval_avg < 0.5:
            analysis['weaknesses'] = ['retrieval']
        if generation_avg < 0.7:
            analysis['weaknesses'] = analysis.get('weaknesses', []) + ['generation']

        # 3. íŒ¨í„´ ë¶„ì„
        analysis['patterns'] = {
            'avg_hallucination_rate': np.mean([
                m.get('hallucination_score', 0) for m in metrics_list
            ]),
            'low_diversity_rate': sum(
                1 for m in metrics_list if m.get('diversity', 1) < 0.3
            ) / len(metrics_list)
        }

        return analysis

    def _identify_issues(self, metrics: Dict[str, float]) -> List[str]:
        """ë©”íŠ¸ë¦­ ê¸°ë°˜ ë¬¸ì œì  ì‹ë³„"""
        issues = []

        if metrics.get('precision@10', 1) < 0.3:
            issues.append('Low retrieval precision')
        if metrics.get('faithfulness', 1) < 0.5:
            issues.append('Low faithfulness to sources')
        if metrics.get('hallucination_score', 0) > 0.3:
            issues.append('High hallucination rate')
        if metrics.get('coherence', 1) < 0.7:
            issues.append('Poor answer coherence')

        return issues

# ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ
class RAGMonitoringSystem:
    def __init__(self, evaluator: ComprehensiveRAGEvaluator):
        """
        ì‹¤ì‹œê°„ RAG ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ
        - ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì¶”ì 
        - ì´ìƒ íƒì§€
        - ìë™ ì•Œë¦¼
        """
        self.evaluator = evaluator
        self.metrics_buffer = []
        self.alert_thresholds = {
            'answer_correctness': 0.7,
            'hallucination_score': 0.2,
            'latency_p95': 1000,  # ms
            'error_rate': 0.01
        }
        self.alerts_triggered = []

    async def monitor_request(self, query: str, context: Dict[str, Any],
                            response: Dict[str, Any], latency: float):
        """ì‹¤ì‹œê°„ ìš”ì²­ ëª¨ë‹ˆí„°ë§"""
        # RAG í‰ê°€ ìƒ˜í”Œ ìƒì„±
        sample = RAGEvaluationSample(
            query=query,
            true_answer="",  # ì‹¤ì‹œê°„ì—ì„œëŠ” ground truth ì—†ìŒ
            retrieved_documents=response.get('documents', []),
            generated_answer=response.get('answer', ''),
            ground_truth_docs=[],
            metadata={'latency': latency}
        )

        # í‰ê°€ ìˆ˜í–‰
        metrics = self.evaluator.evaluate_sample(sample)
        metrics['latency'] = latency
        metrics['timestamp'] = datetime.now()

        # ë²„í¼ì— ì¶”ê°€
        self.metrics_buffer.append(metrics)

        # ì´ìƒ íƒì§€
        await self._check_anomalies(metrics)

        # ì£¼ê¸°ì  ì§‘ê³„ (1000ê°œë§ˆë‹¤)
        if len(self.metrics_buffer) >= 1000:
            await self._aggregate_and_report()

    async def _check_anomalies(self, metrics: Dict[str, float]):
        """ì´ìƒ íƒì§€ ë° ì•Œë¦¼"""
        alerts = []

        # Threshold ì²´í¬
        if metrics.get('answer_correctness', 1) < self.alert_thresholds['answer_correctness']:
            alerts.append({
                'type': 'LOW_CORRECTNESS',
                'value': metrics['answer_correctness'],
                'threshold': self.alert_thresholds['answer_correctness']
            })

        if metrics.get('hallucination_score', 0) > self.alert_thresholds['hallucination_score']:
            alerts.append({
                'type': 'HIGH_HALLUCINATION',
                'value': metrics['hallucination_score'],
                'threshold': self.alert_thresholds['hallucination_score']
            })

        if metrics.get('latency', 0) > self.alert_thresholds['latency_p95']:
            alerts.append({
                'type': 'HIGH_LATENCY',
                'value': metrics['latency'],
                'threshold': self.alert_thresholds['latency_p95']
            })

        # ì•Œë¦¼ ë°œì†¡
        for alert in alerts:
            await self._send_alert(alert)

    async def _send_alert(self, alert: Dict[str, Any]):
        """ì•Œë¦¼ ë°œì†¡"""
        self.alerts_triggered.append({
            'alert': alert,
            'timestamp': datetime.now()
        })

        # ì‹¤ì œ í™˜ê²½ì—ì„œëŠ” Slack, Email ë“±ìœ¼ë¡œ ë°œì†¡
        print(f"âš ï¸ ALERT: {alert['type']} - Value: {alert['value']:.3f} "
              f"(Threshold: {alert['threshold']})")

    async def _aggregate_and_report(self):
        """ì£¼ê¸°ì  ì§‘ê³„ ë° ë¦¬í¬íŒ…"""
        if not self.metrics_buffer:
            return

        # ì‹œê°„ëŒ€ë³„ ì§‘ê³„
        aggregated = self.evaluator._aggregate_metrics(self.metrics_buffer)

        # ë¦¬í¬íŠ¸ ìƒì„±
        report = {
            'period': {
                'start': self.metrics_buffer[0]['timestamp'],
                'end': self.metrics_buffer[-1]['timestamp']
            },
            'sample_count': len(self.metrics_buffer),
            'metrics': aggregated,
            'alerts_count': len(self.alerts_triggered),
            'health_score': self._calculate_health_score(aggregated)
        }

        # ì €ì¥ ë° ì „ì†¡
        await self._save_report(report)

        # ë²„í¼ ì´ˆê¸°í™”
        self.metrics_buffer = []

    def _calculate_health_score(self, aggregated: Dict[str, Dict[str, float]]) -> float:
        """ì‹œìŠ¤í…œ ê±´ê°•ë„ ì ìˆ˜ ê³„ì‚°"""
        score = 100.0

        # ê° ë©”íŠ¸ë¦­ë³„ ê°ì 
        if aggregated.get('answer_correctness', {}).get('mean', 1) < 0.8:
            score -= 20
        if aggregated.get('hallucination_score', {}).get('mean', 0) > 0.1:
            score -= 15
        if aggregated.get('latency', {}).get('p95', 0) > 800:
            score -= 10

        return max(0, score)

    async def _save_report(self, report: Dict[str, Any]):
        """ë¦¬í¬íŠ¸ ì €ì¥"""
        filename = f"rag_monitoring_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"

        # ì‹¤ì œë¡œëŠ” S3, BigQuery ë“±ì— ì €ì¥
        with open(filename, 'w') as f:
            json.dump(report, f, indent=2, default=str)

# ì‚¬ìš© ì˜ˆì œ
print("=== RAG í‰ê°€ ì‹œìŠ¤í…œ ë°ëª¨ ===\n")

# í‰ê°€ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
evaluator = ComprehensiveRAGEvaluator()

# ìƒ˜í”Œ ë°ì´í„°
sample = RAGEvaluationSample(
    query="RAG ì‹œìŠ¤í…œì˜ í•µì‹¬ êµ¬ì„±ìš”ì†ŒëŠ” ë¬´ì—‡ì¸ê°€ìš”?",
    true_answer="RAG ì‹œìŠ¤í…œì˜ í•µì‹¬ êµ¬ì„±ìš”ì†ŒëŠ” ê²€ìƒ‰ê¸°(Retriever)ì™€ ìƒì„±ê¸°(Generator)ì…ë‹ˆë‹¤.",
    retrieved_documents=[
        {
            'id': 'doc1',
            'content': 'RAGëŠ” Retrieval-Augmented Generationì˜ ì•½ìë¡œ, ê²€ìƒ‰ê¸°ì™€ ìƒì„±ê¸°ë¡œ êµ¬ì„±ë©ë‹ˆë‹¤.'
        },
        {
            'id': 'doc2',
            'content': 'RAG ì‹œìŠ¤í…œì€ ì™¸ë¶€ ì§€ì‹ì„ í™œìš©í•˜ì—¬ ë” ì •í™•í•œ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.'
        }
    ],
    generated_answer="RAG ì‹œìŠ¤í…œì˜ í•µì‹¬ êµ¬ì„±ìš”ì†ŒëŠ” ê²€ìƒ‰ê¸°(Retriever)ì™€ ìƒì„±ê¸°(Generator)ì…ë‹ˆë‹¤. "
                    "ê²€ìƒ‰ê¸°ëŠ” ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ê³ , ìƒì„±ê¸°ëŠ” ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.",
    ground_truth_docs=['doc1']
)

# í‰ê°€ ìˆ˜í–‰
metrics = evaluator.evaluate_sample(sample)

print("=== í‰ê°€ ê²°ê³¼ ===")
print(f"Overall Score: {metrics['overall_score']:.3f}")
print(f"\nRetrieval Metrics:")
print(f"  - Precision@10: {metrics.get('precision@10', 0):.3f}")
print(f"  - MRR: {metrics['mrr']:.3f}")
print(f"  - NDCG: {metrics['ndcg']:.3f}")
print(f"\nGeneration Metrics:")
print(f"  - Answer Relevance: {metrics['answer_relevance']:.3f}")
print(f"  - Faithfulness: {metrics['faithfulness']:.3f}")
print(f"  - Hallucination Score: {metrics['hallucination_score']:.3f}")
print(f"  - Coherence: {metrics['coherence']:.3f}")`}
            </pre>
          </div>
        </div>
      </div>
    </section>
  )
}
