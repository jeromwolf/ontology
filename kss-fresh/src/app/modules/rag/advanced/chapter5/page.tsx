'use client'

import Link from 'next/link'
import { ArrowLeft, ArrowRight, BarChart3, Activity, AlertTriangle, TrendingUp, Shield, Gauge } from 'lucide-react'

export default function Chapter5Page() {
  return (
    <div className="max-w-4xl mx-auto py-8 px-4">
      {/* Header */}
      <div className="mb-8">
        <Link
          href="/modules/rag/advanced"
          className="inline-flex items-center gap-2 text-emerald-600 hover:text-emerald-700 mb-4 transition-colors"
        >
          <ArrowLeft size={20} />
          ê³ ê¸‰ ê³¼ì •ìœ¼ë¡œ ëŒì•„ê°€ê¸°
        </Link>
        
        <div className="bg-gradient-to-r from-emerald-500 to-teal-600 rounded-2xl p-8 text-white">
          <div className="flex items-center gap-4 mb-4">
            <div className="w-16 h-16 rounded-xl bg-white/20 flex items-center justify-center">
              <BarChart3 size={32} />
            </div>
            <div>
              <h1 className="text-3xl font-bold">Chapter 5: RAG í‰ê°€ì™€ ëª¨ë‹ˆí„°ë§</h1>
              <p className="text-emerald-100 text-lg">í”„ë¡œë•ì…˜ RAG ì‹œìŠ¤í…œì˜ í’ˆì§ˆ ì¸¡ì •ê³¼ ì§€ì†ì  ê°œì„ </p>
            </div>
          </div>
        </div>
      </div>

      {/* Content */}
      <div className="space-y-8">
        {/* Section 1: RAG í‰ê°€ ë©”íŠ¸ë¦­ ì²´ê³„ */}
        <section className="bg-white dark:bg-gray-800 rounded-2xl p-8 shadow-sm border border-gray-200 dark:border-gray-700">
          <div className="flex items-center gap-3 mb-6">
            <div className="w-12 h-12 rounded-xl bg-emerald-100 dark:bg-emerald-900/20 flex items-center justify-center">
              <Gauge className="text-emerald-600" size={24} />
            </div>
            <div>
              <h2 className="text-2xl font-bold text-gray-900 dark:text-white">5.1 RAG ì‹œìŠ¤í…œ í‰ê°€ ë©”íŠ¸ë¦­ ì²´ê³„</h2>
              <p className="text-gray-600 dark:text-gray-400">ê²€ìƒ‰ê³¼ ìƒì„±ì„ ëª¨ë‘ ê³ ë ¤í•œ ì¢…í•©ì  í‰ê°€ í”„ë ˆì„ì›Œí¬</p>
            </div>
          </div>

          <div className="space-y-6">
            <div className="bg-emerald-50 dark:bg-emerald-900/20 p-6 rounded-xl border border-emerald-200 dark:border-emerald-700">
              <h3 className="font-bold text-emerald-800 dark:text-emerald-200 mb-4">RAG í‰ê°€ì˜ 3ì°¨ì› ì ‘ê·¼ë²•</h3>
              
              <div className="prose prose-sm dark:prose-invert mb-4">
                <p className="text-gray-700 dark:text-gray-300">
                  <strong>RAG ì‹œìŠ¤í…œì€ ê²€ìƒ‰(Retrieval)ê³¼ ìƒì„±(Generation) ë‘ ê°€ì§€ í•µì‹¬ ì»´í¬ë„ŒíŠ¸ë¡œ êµ¬ì„±ë˜ë©°,
                  ê°ê°ì„ ë…ë¦½ì ìœ¼ë¡œ í‰ê°€í•˜ëŠ” ë™ì‹œì— ì „ì²´ ì‹œìŠ¤í…œì˜ ì„±ëŠ¥ë„ ì¸¡ì •í•´ì•¼ í•©ë‹ˆë‹¤.</strong>
                  Microsoft, Google, OpenAIì˜ ì‹¤ì œ í”„ë¡œë•ì…˜ ê²½í—˜ì„ ë°”íƒ•ìœ¼ë¡œ í•œ í¬ê´„ì  í‰ê°€ ì²´ê³„ë¥¼ ì†Œê°œí•©ë‹ˆë‹¤.
                </p>
              </div>

              <div className="grid md:grid-cols-3 gap-4">
                <div className="bg-white dark:bg-gray-800 p-4 rounded-lg border">
                  <h4 className="font-medium text-emerald-600 dark:text-emerald-400 mb-2">ğŸ” Retrieval Quality</h4>
                  <ul className="text-sm text-gray-700 dark:text-gray-300 space-y-1">
                    <li>â€¢ Precision@K</li>
                    <li>â€¢ Recall@K</li>
                    <li>â€¢ MRR (Mean Reciprocal Rank)</li>
                    <li>â€¢ NDCG (Normalized DCG)</li>
                    <li>â€¢ Coverage & Diversity</li>
                  </ul>
                </div>
                
                <div className="bg-white dark:bg-gray-800 p-4 rounded-lg border">
                  <h4 className="font-medium text-blue-600 dark:text-blue-400 mb-2">ğŸ“ Generation Quality</h4>
                  <ul className="text-sm text-gray-700 dark:text-gray-300 space-y-1">
                    <li>â€¢ Relevance Score</li>
                    <li>â€¢ Faithfulness</li>
                    <li>â€¢ Answer Correctness</li>
                    <li>â€¢ Coherence & Fluency</li>
                    <li>â€¢ Hallucination Rate</li>
                  </ul>
                </div>
                
                <div className="bg-white dark:bg-gray-800 p-4 rounded-lg border">
                  <h4 className="font-medium text-purple-600 dark:text-purple-400 mb-2">âš¡ System Performance</h4>
                  <ul className="text-sm text-gray-700 dark:text-gray-300 space-y-1">
                    <li>â€¢ End-to-end Latency</li>
                    <li>â€¢ Throughput (QPS)</li>
                    <li>â€¢ Resource Utilization</li>
                    <li>â€¢ Cost per Query</li>
                    <li>â€¢ Error Rate</li>
                  </ul>
                </div>
              </div>
            </div>

            <div className="bg-blue-50 dark:bg-blue-900/20 p-6 rounded-xl border border-blue-200 dark:border-blue-700">
              <h3 className="font-bold text-blue-800 dark:text-blue-200 mb-4">RAGAS Framework êµ¬í˜„</h3>
              
              <div className="bg-slate-50 dark:bg-slate-800 p-4 rounded-lg border border-slate-200 dark:border-slate-700 overflow-x-auto">
                <pre className="text-sm text-slate-800 dark:text-slate-200 font-mono">
{`from dataclasses import dataclass
from typing import List, Dict, Tuple, Optional, Any
import numpy as np
from sklearn.metrics import ndcg_score
import torch
from transformers import AutoTokenizer, AutoModel
from sentence_transformers import CrossEncoder
import asyncio
from datetime import datetime
import json

@dataclass
class RAGEvaluationSample:
    """RAG í‰ê°€ë¥¼ ìœ„í•œ ë°ì´í„° ìƒ˜í”Œ"""
    query: str
    true_answer: str
    retrieved_documents: List[Dict[str, Any]]
    generated_answer: str
    ground_truth_docs: List[str]  # ì •ë‹µ ë¬¸ì„œ ID
    metadata: Dict[str, Any] = None

class ComprehensiveRAGEvaluator:
    def __init__(self, 
                 embedding_model: str = 'intfloat/multilingual-e5-large',
                 cross_encoder_model: str = 'cross-encoder/ms-marco-MiniLM-L-6-v2',
                 nli_model: str = 'microsoft/deberta-v3-base-nli'):
        """
        í¬ê´„ì  RAG í‰ê°€ ì‹œìŠ¤í…œ
        - Retrieval, Generation, System ë©”íŠ¸ë¦­ í†µí•©
        - ë‹¤êµ­ì–´ ì§€ì›
        - ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ í†µí•©
        """
        # ëª¨ë¸ ì´ˆê¸°í™”
        self.tokenizer = AutoTokenizer.from_pretrained(embedding_model)
        self.embedding_model = AutoModel.from_pretrained(embedding_model)
        self.cross_encoder = CrossEncoder(cross_encoder_model)
        self.nli_model = CrossEncoder(nli_model)
        
        # í‰ê°€ ê²°ê³¼ ì €ì¥
        self.evaluation_history = []
        
    def evaluate_sample(self, sample: RAGEvaluationSample) -> Dict[str, float]:
        """ë‹¨ì¼ ìƒ˜í”Œì— ëŒ€í•œ ì „ì²´ í‰ê°€"""
        metrics = {}
        
        # 1. Retrieval í‰ê°€
        retrieval_metrics = self._evaluate_retrieval(sample)
        metrics.update(retrieval_metrics)
        
        # 2. Generation í‰ê°€
        generation_metrics = self._evaluate_generation(sample)
        metrics.update(generation_metrics)
        
        # 3. ì¢…í•© ì ìˆ˜ ê³„ì‚°
        metrics['overall_score'] = self._calculate_overall_score(metrics)
        
        return metrics
    
    def _evaluate_retrieval(self, sample: RAGEvaluationSample) -> Dict[str, float]:
        """ê²€ìƒ‰ í’ˆì§ˆ í‰ê°€"""
        retrieved_ids = [doc['id'] for doc in sample.retrieved_documents]
        ground_truth_ids = sample.ground_truth_docs
        
        # Precision@K
        k_values = [1, 3, 5, 10]
        precisions = {}
        for k in k_values:
            if len(retrieved_ids) >= k:
                relevant_at_k = sum(1 for doc_id in retrieved_ids[:k] 
                                   if doc_id in ground_truth_ids)
                precisions[f'precision@{k}'] = relevant_at_k / k
            else:
                precisions[f'precision@{k}'] = 0.0
        
        # Recall@K
        recalls = {}
        for k in k_values:
            if len(ground_truth_ids) > 0:
                relevant_at_k = sum(1 for doc_id in retrieved_ids[:k] 
                                   if doc_id in ground_truth_ids)
                recalls[f'recall@{k}'] = relevant_at_k / len(ground_truth_ids)
            else:
                recalls[f'recall@{k}'] = 0.0
        
        # MRR (Mean Reciprocal Rank)
        mrr = 0.0
        for i, doc_id in enumerate(retrieved_ids):
            if doc_id in ground_truth_ids:
                mrr = 1.0 / (i + 1)
                break
        
        # NDCG
        relevance_scores = []
        for doc in sample.retrieved_documents:
            if doc['id'] in ground_truth_ids:
                relevance_scores.append(1.0)
            else:
                # Semantic similarity as soft relevance
                sim = self._calculate_semantic_similarity(
                    sample.query, doc['content']
                )
                relevance_scores.append(sim)
        
        ideal_scores = sorted(relevance_scores, reverse=True)
        if len(relevance_scores) > 0 and sum(ideal_scores) > 0:
            ndcg = ndcg_score([ideal_scores], [relevance_scores])
        else:
            ndcg = 0.0
        
        # Coverage (ê²€ìƒ‰ëœ ê´€ë ¨ ë¬¸ì„œì˜ ë¹„ìœ¨)
        coverage = len(set(retrieved_ids) & set(ground_truth_ids)) / \
                  max(len(ground_truth_ids), 1)
        
        # Diversity (ê²€ìƒ‰ ê²°ê³¼ì˜ ë‹¤ì–‘ì„±)
        diversity = self._calculate_diversity(sample.retrieved_documents)
        
        return {
            **precisions,
            **recalls,
            'mrr': mrr,
            'ndcg': ndcg,
            'coverage': coverage,
            'diversity': diversity
        }
    
    def _evaluate_generation(self, sample: RAGEvaluationSample) -> Dict[str, float]:
        """ìƒì„± í’ˆì§ˆ í‰ê°€"""
        metrics = {}
        
        # 1. Answer Relevance (ì¿¼ë¦¬ì™€ ë‹µë³€ì˜ ê´€ë ¨ì„±)
        relevance = self._calculate_semantic_similarity(
            sample.query, sample.generated_answer
        )
        metrics['answer_relevance'] = relevance
        
        # 2. Faithfulness (ê²€ìƒ‰ëœ ë¬¸ì„œì— ëŒ€í•œ ì¶©ì‹¤ë„)
        faithfulness = self._evaluate_faithfulness(
            sample.generated_answer, 
            sample.retrieved_documents
        )
        metrics['faithfulness'] = faithfulness
        
        # 3. Answer Correctness (ì •ë‹µê³¼ì˜ ì¼ì¹˜ë„)
        correctness = self._evaluate_correctness(
            sample.generated_answer,
            sample.true_answer
        )
        metrics['answer_correctness'] = correctness
        
        # 4. Hallucination Detection
        hallucination_score = self._detect_hallucination(
            sample.generated_answer,
            sample.retrieved_documents
        )
        metrics['hallucination_score'] = hallucination_score
        
        # 5. Coherence and Fluency
        coherence = self._evaluate_coherence(sample.generated_answer)
        metrics['coherence'] = coherence
        
        return metrics
    
    def _calculate_semantic_similarity(self, text1: str, text2: str) -> float:
        """ì˜ë¯¸ì  ìœ ì‚¬ë„ ê³„ì‚°"""
        # ì„ë² ë”© ìƒì„±
        inputs1 = self.tokenizer(text1, return_tensors='pt', 
                                padding=True, truncation=True)
        inputs2 = self.tokenizer(text2, return_tensors='pt', 
                                padding=True, truncation=True)
        
        with torch.no_grad():
            emb1 = self.embedding_model(**inputs1).last_hidden_state.mean(dim=1)
            emb2 = self.embedding_model(**inputs2).last_hidden_state.mean(dim=1)
        
        # Cosine similarity
        similarity = torch.cosine_similarity(emb1, emb2).item()
        return (similarity + 1) / 2  # Normalize to [0, 1]
    
    def _evaluate_faithfulness(self, answer: str, documents: List[Dict]) -> float:
        """ë‹µë³€ì´ ê²€ìƒ‰ëœ ë¬¸ì„œì— ì¶©ì‹¤í•œì§€ í‰ê°€"""
        # ë‹µë³€ì„ ë¬¸ì¥ìœ¼ë¡œ ë¶„ë¦¬
        answer_sentences = answer.split('. ')
        
        # ê° ë¬¸ì¥ì´ ë¬¸ì„œì—ì„œ ì§€ì›ë˜ëŠ”ì§€ í™•ì¸
        supported_count = 0
        total_sentences = len(answer_sentences)
        
        for sentence in answer_sentences:
            if not sentence.strip():
                continue
                
            max_support = 0.0
            for doc in documents:
                # NLI ëª¨ë¸ì„ ì‚¬ìš©í•œ entailment ê²€ì‚¬
                premise = doc['content']
                hypothesis = sentence
                
                # Cross-encoderë¡œ entailment score ê³„ì‚°
                score = self.nli_model.predict([[premise, hypothesis]])[0]
                max_support = max(max_support, score)
            
            # Threshold ì´ìƒì´ë©´ ì§€ì›ë¨
            if max_support > 0.5:
                supported_count += 1
        
        return supported_count / max(total_sentences, 1)
    
    def _detect_hallucination(self, answer: str, documents: List[Dict]) -> float:
        """í™˜ê°(hallucination) íƒì§€"""
        # ë‹µë³€ì—ì„œ êµ¬ì²´ì ì¸ ì‚¬ì‹¤/ìˆ˜ì¹˜ ì¶”ì¶œ (ê°„ë‹¨í•œ ê·œì¹™ ê¸°ë°˜)
        import re
        
        # ìˆ«ì, ë‚ ì§œ, ê³ ìœ ëª…ì‚¬ íŒ¨í„´
        fact_patterns = [
            r'\d+(?:\.\d+)?%?',  # ìˆ«ìì™€ ë°±ë¶„ìœ¨
            r'\d{4}ë…„',  # ì—°ë„
            r'[A-Z][a-z]+(?:\s[A-Z][a-z]+)*',  # ê³ ìœ ëª…ì‚¬
        ]
        
        facts = []
        for pattern in fact_patterns:
            facts.extend(re.findall(pattern, answer))
        
        if not facts:
            return 0.0  # êµ¬ì²´ì  ì‚¬ì‹¤ì´ ì—†ìœ¼ë©´ í™˜ê° ì—†ìŒ
        
        # ê° ì‚¬ì‹¤ì´ ë¬¸ì„œì—ì„œ ì–¸ê¸‰ë˜ëŠ”ì§€ í™•ì¸
        hallucinated = 0
        for fact in facts:
            found = False
            for doc in documents:
                if fact in doc['content']:
                    found = True
                    break
            
            if not found:
                hallucinated += 1
        
        return hallucinated / len(facts) if facts else 0.0
    
    def _evaluate_correctness(self, generated: str, ground_truth: str) -> float:
        """ì •ë‹µê³¼ì˜ ì¼ì¹˜ë„ í‰ê°€"""
        # 1. Semantic similarity
        semantic_score = self._calculate_semantic_similarity(generated, ground_truth)
        
        # 2. Token overlap (F1 score)
        generated_tokens = set(generated.lower().split())
        truth_tokens = set(ground_truth.lower().split())
        
        if not truth_tokens:
            return semantic_score
        
        overlap = generated_tokens.intersection(truth_tokens)
        precision = len(overlap) / len(generated_tokens) if generated_tokens else 0
        recall = len(overlap) / len(truth_tokens)
        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0
        
        # 3. Combined score
        return 0.7 * semantic_score + 0.3 * f1
    
    def _evaluate_coherence(self, text: str) -> float:
        """í…ìŠ¤íŠ¸ ì¼ê´€ì„± í‰ê°€"""
        sentences = text.split('. ')
        if len(sentences) < 2:
            return 1.0
        
        # ì—°ì†ëœ ë¬¸ì¥ ê°„ ì˜ë¯¸ì  ìœ ì‚¬ë„
        coherence_scores = []
        for i in range(len(sentences) - 1):
            if sentences[i].strip() and sentences[i+1].strip():
                sim = self._calculate_semantic_similarity(
                    sentences[i], sentences[i+1]
                )
                coherence_scores.append(sim)
        
        return np.mean(coherence_scores) if coherence_scores else 1.0
    
    def _calculate_diversity(self, documents: List[Dict]) -> float:
        """ë¬¸ì„œ ì§‘í•©ì˜ ë‹¤ì–‘ì„± ê³„ì‚°"""
        if len(documents) < 2:
            return 0.0
        
        # ëª¨ë“  ë¬¸ì„œ ìŒì˜ ìœ ì‚¬ë„ ê³„ì‚°
        similarities = []
        for i in range(len(documents)):
            for j in range(i + 1, len(documents)):
                sim = self._calculate_semantic_similarity(
                    documents[i]['content'],
                    documents[j]['content']
                )
                similarities.append(sim)
        
        # í‰ê·  ìœ ì‚¬ë„ê°€ ë‚®ì„ìˆ˜ë¡ ë‹¤ì–‘ì„±ì´ ë†’ìŒ
        avg_similarity = np.mean(similarities)
        diversity = 1 - avg_similarity
        
        return diversity
    
    def _calculate_overall_score(self, metrics: Dict[str, float]) -> float:
        """ì¢…í•© ì ìˆ˜ ê³„ì‚°"""
        # ê°€ì¤‘ì¹˜ ì„¤ì •
        weights = {
            'retrieval': 0.4,
            'generation': 0.5,
            'system': 0.1
        }
        
        # ì¹´í…Œê³ ë¦¬ë³„ ì ìˆ˜ ê³„ì‚°
        retrieval_score = np.mean([
            metrics.get('precision@10', 0),
            metrics.get('recall@10', 0),
            metrics.get('mrr', 0),
            metrics.get('ndcg', 0)
        ])
        
        generation_score = np.mean([
            metrics.get('answer_relevance', 0),
            metrics.get('faithfulness', 0),
            metrics.get('answer_correctness', 0),
            1 - metrics.get('hallucination_score', 0),  # í™˜ê°ì´ ì ì„ìˆ˜ë¡ ì¢‹ìŒ
            metrics.get('coherence', 0)
        ])
        
        # ì¢…í•© ì ìˆ˜
        overall = (weights['retrieval'] * retrieval_score + 
                  weights['generation'] * generation_score)
        
        return overall
    
    def evaluate_dataset(self, samples: List[RAGEvaluationSample], 
                        batch_size: int = 32) -> Dict[str, Any]:
        """ì „ì²´ ë°ì´í„°ì…‹ í‰ê°€"""
        all_metrics = []
        
        # ë°°ì¹˜ ì²˜ë¦¬
        for i in range(0, len(samples), batch_size):
            batch = samples[i:i+batch_size]
            
            # ë³‘ë ¬ ì²˜ë¦¬ë¥¼ ìœ„í•œ ë¹„ë™ê¸° ì‹¤í–‰
            batch_metrics = []
            for sample in batch:
                metrics = self.evaluate_sample(sample)
                batch_metrics.append(metrics)
            
            all_metrics.extend(batch_metrics)
        
        # ì§‘ê³„
        aggregated = self._aggregate_metrics(all_metrics)
        
        # ìƒì„¸ ë¶„ì„
        analysis = self._analyze_results(all_metrics, samples)
        
        return {
            'aggregated_metrics': aggregated,
            'detailed_analysis': analysis,
            'sample_metrics': all_metrics
        }
    
    def _aggregate_metrics(self, metrics_list: List[Dict[str, float]]) -> Dict[str, Dict[str, float]]:
        """ë©”íŠ¸ë¦­ ì§‘ê³„"""
        aggregated = {}
        
        # ëª¨ë“  ë©”íŠ¸ë¦­ ìˆ˜ì§‘
        all_metric_names = set()
        for metrics in metrics_list:
            all_metric_names.update(metrics.keys())
        
        # ê° ë©”íŠ¸ë¦­ì— ëŒ€í•´ í†µê³„ ê³„ì‚°
        for metric_name in all_metric_names:
            values = [m.get(metric_name, 0) for m in metrics_list]
            
            aggregated[metric_name] = {
                'mean': np.mean(values),
                'std': np.std(values),
                'min': np.min(values),
                'max': np.max(values),
                'median': np.median(values),
                'p95': np.percentile(values, 95)
            }
        
        return aggregated
    
    def _analyze_results(self, metrics_list: List[Dict[str, float]], 
                        samples: List[RAGEvaluationSample]) -> Dict[str, Any]:
        """ê²°ê³¼ ìƒì„¸ ë¶„ì„"""
        analysis = {}
        
        # 1. ì„±ëŠ¥ ì €í•˜ ìƒ˜í”Œ ì‹ë³„
        overall_scores = [m['overall_score'] for m in metrics_list]
        threshold = np.percentile(overall_scores, 25)  # í•˜ìœ„ 25%
        
        poor_performing = []
        for i, (sample, metrics) in enumerate(zip(samples, metrics_list)):
            if metrics['overall_score'] < threshold:
                poor_performing.append({
                    'index': i,
                    'query': sample.query,
                    'score': metrics['overall_score'],
                    'issues': self._identify_issues(metrics)
                })
        
        analysis['poor_performing_samples'] = poor_performing[:10]  # Top 10
        
        # 2. ì¹´í…Œê³ ë¦¬ë³„ ê°•ì•½ì  ë¶„ì„
        retrieval_avg = np.mean([
            m.get('mrr', 0) for m in metrics_list
        ])
        generation_avg = np.mean([
            m.get('answer_correctness', 0) for m in metrics_list
        ])
        
        if retrieval_avg < 0.5:
            analysis['weaknesses'] = ['retrieval']
        if generation_avg < 0.7:
            analysis['weaknesses'] = analysis.get('weaknesses', []) + ['generation']
        
        # 3. íŒ¨í„´ ë¶„ì„
        analysis['patterns'] = {
            'avg_hallucination_rate': np.mean([
                m.get('hallucination_score', 0) for m in metrics_list
            ]),
            'low_diversity_rate': sum(
                1 for m in metrics_list if m.get('diversity', 1) < 0.3
            ) / len(metrics_list)
        }
        
        return analysis
    
    def _identify_issues(self, metrics: Dict[str, float]) -> List[str]:
        """ë©”íŠ¸ë¦­ ê¸°ë°˜ ë¬¸ì œì  ì‹ë³„"""
        issues = []
        
        if metrics.get('precision@10', 1) < 0.3:
            issues.append('Low retrieval precision')
        if metrics.get('faithfulness', 1) < 0.5:
            issues.append('Low faithfulness to sources')
        if metrics.get('hallucination_score', 0) > 0.3:
            issues.append('High hallucination rate')
        if metrics.get('coherence', 1) < 0.7:
            issues.append('Poor answer coherence')
        
        return issues

# ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ
class RAGMonitoringSystem:
    def __init__(self, evaluator: ComprehensiveRAGEvaluator):
        """
        ì‹¤ì‹œê°„ RAG ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ
        - ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì¶”ì 
        - ì´ìƒ íƒì§€
        - ìë™ ì•Œë¦¼
        """
        self.evaluator = evaluator
        self.metrics_buffer = []
        self.alert_thresholds = {
            'answer_correctness': 0.7,
            'hallucination_score': 0.2,
            'latency_p95': 1000,  # ms
            'error_rate': 0.01
        }
        self.alerts_triggered = []
    
    async def monitor_request(self, query: str, context: Dict[str, Any],
                            response: Dict[str, Any], latency: float):
        """ì‹¤ì‹œê°„ ìš”ì²­ ëª¨ë‹ˆí„°ë§"""
        # RAG í‰ê°€ ìƒ˜í”Œ ìƒì„±
        sample = RAGEvaluationSample(
            query=query,
            true_answer="",  # ì‹¤ì‹œê°„ì—ì„œëŠ” ground truth ì—†ìŒ
            retrieved_documents=response.get('documents', []),
            generated_answer=response.get('answer', ''),
            ground_truth_docs=[],
            metadata={'latency': latency}
        )
        
        # í‰ê°€ ìˆ˜í–‰
        metrics = self.evaluator.evaluate_sample(sample)
        metrics['latency'] = latency
        metrics['timestamp'] = datetime.now()
        
        # ë²„í¼ì— ì¶”ê°€
        self.metrics_buffer.append(metrics)
        
        # ì´ìƒ íƒì§€
        await self._check_anomalies(metrics)
        
        # ì£¼ê¸°ì  ì§‘ê³„ (1000ê°œë§ˆë‹¤)
        if len(self.metrics_buffer) >= 1000:
            await self._aggregate_and_report()
    
    async def _check_anomalies(self, metrics: Dict[str, float]):
        """ì´ìƒ íƒì§€ ë° ì•Œë¦¼"""
        alerts = []
        
        # Threshold ì²´í¬
        if metrics.get('answer_correctness', 1) < self.alert_thresholds['answer_correctness']:
            alerts.append({
                'type': 'LOW_CORRECTNESS',
                'value': metrics['answer_correctness'],
                'threshold': self.alert_thresholds['answer_correctness']
            })
        
        if metrics.get('hallucination_score', 0) > self.alert_thresholds['hallucination_score']:
            alerts.append({
                'type': 'HIGH_HALLUCINATION',
                'value': metrics['hallucination_score'],
                'threshold': self.alert_thresholds['hallucination_score']
            })
        
        if metrics.get('latency', 0) > self.alert_thresholds['latency_p95']:
            alerts.append({
                'type': 'HIGH_LATENCY',
                'value': metrics['latency'],
                'threshold': self.alert_thresholds['latency_p95']
            })
        
        # ì•Œë¦¼ ë°œì†¡
        for alert in alerts:
            await self._send_alert(alert)
    
    async def _send_alert(self, alert: Dict[str, Any]):
        """ì•Œë¦¼ ë°œì†¡"""
        self.alerts_triggered.append({
            'alert': alert,
            'timestamp': datetime.now()
        })
        
        # ì‹¤ì œ í™˜ê²½ì—ì„œëŠ” Slack, Email ë“±ìœ¼ë¡œ ë°œì†¡
        print(f"âš ï¸ ALERT: {alert['type']} - Value: {alert['value']:.3f} "
              f"(Threshold: {alert['threshold']})")
    
    async def _aggregate_and_report(self):
        """ì£¼ê¸°ì  ì§‘ê³„ ë° ë¦¬í¬íŒ…"""
        if not self.metrics_buffer:
            return
        
        # ì‹œê°„ëŒ€ë³„ ì§‘ê³„
        aggregated = self.evaluator._aggregate_metrics(self.metrics_buffer)
        
        # ë¦¬í¬íŠ¸ ìƒì„±
        report = {
            'period': {
                'start': self.metrics_buffer[0]['timestamp'],
                'end': self.metrics_buffer[-1]['timestamp']
            },
            'sample_count': len(self.metrics_buffer),
            'metrics': aggregated,
            'alerts_count': len(self.alerts_triggered),
            'health_score': self._calculate_health_score(aggregated)
        }
        
        # ì €ì¥ ë° ì „ì†¡
        await self._save_report(report)
        
        # ë²„í¼ ì´ˆê¸°í™”
        self.metrics_buffer = []
    
    def _calculate_health_score(self, aggregated: Dict[str, Dict[str, float]]) -> float:
        """ì‹œìŠ¤í…œ ê±´ê°•ë„ ì ìˆ˜ ê³„ì‚°"""
        score = 100.0
        
        # ê° ë©”íŠ¸ë¦­ë³„ ê°ì 
        if aggregated.get('answer_correctness', {}).get('mean', 1) < 0.8:
            score -= 20
        if aggregated.get('hallucination_score', {}).get('mean', 0) > 0.1:
            score -= 15
        if aggregated.get('latency', {}).get('p95', 0) > 800:
            score -= 10
        
        return max(0, score)
    
    async def _save_report(self, report: Dict[str, Any]):
        """ë¦¬í¬íŠ¸ ì €ì¥"""
        filename = f"rag_monitoring_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        
        # ì‹¤ì œë¡œëŠ” S3, BigQuery ë“±ì— ì €ì¥
        with open(filename, 'w') as f:
            json.dump(report, f, indent=2, default=str)

# ì‚¬ìš© ì˜ˆì œ
print("=== RAG í‰ê°€ ì‹œìŠ¤í…œ ë°ëª¨ ===\n")

# í‰ê°€ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
evaluator = ComprehensiveRAGEvaluator()

# ìƒ˜í”Œ ë°ì´í„°
sample = RAGEvaluationSample(
    query="RAG ì‹œìŠ¤í…œì˜ í•µì‹¬ êµ¬ì„±ìš”ì†ŒëŠ” ë¬´ì—‡ì¸ê°€ìš”?",
    true_answer="RAG ì‹œìŠ¤í…œì˜ í•µì‹¬ êµ¬ì„±ìš”ì†ŒëŠ” ê²€ìƒ‰ê¸°(Retriever)ì™€ ìƒì„±ê¸°(Generator)ì…ë‹ˆë‹¤.",
    retrieved_documents=[
        {
            'id': 'doc1',
            'content': 'RAGëŠ” Retrieval-Augmented Generationì˜ ì•½ìë¡œ, ê²€ìƒ‰ê¸°ì™€ ìƒì„±ê¸°ë¡œ êµ¬ì„±ë©ë‹ˆë‹¤.'
        },
        {
            'id': 'doc2',
            'content': 'RAG ì‹œìŠ¤í…œì€ ì™¸ë¶€ ì§€ì‹ì„ í™œìš©í•˜ì—¬ ë” ì •í™•í•œ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.'
        }
    ],
    generated_answer="RAG ì‹œìŠ¤í…œì˜ í•µì‹¬ êµ¬ì„±ìš”ì†ŒëŠ” ê²€ìƒ‰ê¸°(Retriever)ì™€ ìƒì„±ê¸°(Generator)ì…ë‹ˆë‹¤. "
                    "ê²€ìƒ‰ê¸°ëŠ” ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ê³ , ìƒì„±ê¸°ëŠ” ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.",
    ground_truth_docs=['doc1']
)

# í‰ê°€ ìˆ˜í–‰
metrics = evaluator.evaluate_sample(sample)

print("=== í‰ê°€ ê²°ê³¼ ===")
print(f"Overall Score: {metrics['overall_score']:.3f}")
print(f"\nRetrieval Metrics:")
print(f"  - Precision@10: {metrics.get('precision@10', 0):.3f}")
print(f"  - MRR: {metrics['mrr']:.3f}")
print(f"  - NDCG: {metrics['ndcg']:.3f}")
print(f"\nGeneration Metrics:")
print(f"  - Answer Relevance: {metrics['answer_relevance']:.3f}")
print(f"  - Faithfulness: {metrics['faithfulness']:.3f}")
print(f"  - Hallucination Score: {metrics['hallucination_score']:.3f}")
print(f"  - Coherence: {metrics['coherence']:.3f}")`}
                </pre>
              </div>
            </div>
          </div>
        </section>

        {/* Section 2: A/B í…ŒìŠ¤íŒ… í”„ë ˆì„ì›Œí¬ */}
        <section className="bg-white dark:bg-gray-800 rounded-2xl p-8 shadow-sm border border-gray-200 dark:border-gray-700">
          <div className="flex items-center gap-3 mb-6">
            <div className="w-12 h-12 rounded-xl bg-blue-100 dark:bg-blue-900/20 flex items-center justify-center">
              <TrendingUp className="text-blue-600" size={24} />
            </div>
            <div>
              <h2 className="text-2xl font-bold text-gray-900 dark:text-white">5.2 RAGë¥¼ ìœ„í•œ A/B í…ŒìŠ¤íŒ…</h2>
              <p className="text-gray-600 dark:text-gray-400">ë°ì´í„° ê¸°ë°˜ ì˜ì‚¬ê²°ì •ì„ ìœ„í•œ ì‹¤í—˜ í”„ë ˆì„ì›Œí¬</p>
            </div>
          </div>

          <div className="space-y-6">
            <div className="bg-blue-50 dark:bg-blue-900/20 p-6 rounded-xl border border-blue-200 dark:border-blue-700">
              <h3 className="font-bold text-blue-800 dark:text-blue-200 mb-4">í”„ë¡œë•ì…˜ A/B í…ŒìŠ¤íŒ… ì‹œìŠ¤í…œ</h3>
              
              <div className="bg-slate-50 dark:bg-slate-800 p-4 rounded-lg border border-slate-200 dark:border-slate-700 overflow-x-auto">
                <pre className="text-sm text-slate-800 dark:text-slate-200 font-mono">
{`import hashlib
from typing import List, Dict, Any, Optional, Callable
from enum import Enum
import numpy as np
from scipy import stats
from dataclasses import dataclass, field
import json
from datetime import datetime, timedelta
import asyncio
from collections import defaultdict

class ExperimentVariant(Enum):
    """ì‹¤í—˜ ë³€í˜•"""
    CONTROL = "control"
    TREATMENT_A = "treatment_a"
    TREATMENT_B = "treatment_b"

@dataclass
class ExperimentConfig:
    """ì‹¤í—˜ ì„¤ì •"""
    experiment_id: str
    name: str
    description: str
    start_date: datetime
    end_date: datetime
    traffic_allocation: Dict[ExperimentVariant, float]
    success_metrics: List[str]
    guardrail_metrics: List[str]
    minimum_sample_size: int = 1000
    confidence_level: float = 0.95

@dataclass
class ExperimentResult:
    """ì‹¤í—˜ ê²°ê³¼"""
    variant: ExperimentVariant
    user_id: str
    metrics: Dict[str, float]
    timestamp: datetime
    metadata: Dict[str, Any] = field(default_factory=dict)

class RAGExperimentationFramework:
    def __init__(self):
        """
        RAG A/B í…ŒìŠ¤íŒ… í”„ë ˆì„ì›Œí¬
        - íŠ¸ë˜í”½ ë¶„ë°°
        - ë©”íŠ¸ë¦­ ìˆ˜ì§‘
        - í†µê³„ì  ìœ ì˜ì„± ê²€ì •
        - ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§
        """
        self.experiments: Dict[str, ExperimentConfig] = {}
        self.results: Dict[str, List[ExperimentResult]] = defaultdict(list)
        self.variant_configs = self._init_variant_configs()
        
    def _init_variant_configs(self) -> Dict[ExperimentVariant, Dict[str, Any]]:
        """ê° ë³€í˜•ì˜ RAG ì„¤ì •"""
        return {
            ExperimentVariant.CONTROL: {
                'retrieval': {
                    'model': 'bge-large-en',
                    'top_k': 5,
                    'reranking': False
                },
                'generation': {
                    'model': 'gpt-3.5-turbo',
                    'temperature': 0.7,
                    'max_tokens': 500
                }
            },
            ExperimentVariant.TREATMENT_A: {
                'retrieval': {
                    'model': 'e5-large-v2',
                    'top_k': 10,
                    'reranking': True,
                    'reranker': 'cross-encoder/ms-marco-MiniLM-L-6-v2'
                },
                'generation': {
                    'model': 'gpt-3.5-turbo',
                    'temperature': 0.7,
                    'max_tokens': 500
                }
            },
            ExperimentVariant.TREATMENT_B: {
                'retrieval': {
                    'model': 'bge-large-en',
                    'top_k': 5,
                    'reranking': True,
                    'reranker': 'colbert-v2'
                },
                'generation': {
                    'model': 'gpt-4',
                    'temperature': 0.3,
                    'max_tokens': 800
                }
            }
        }
    
    def create_experiment(self, config: ExperimentConfig):
        """ìƒˆ ì‹¤í—˜ ìƒì„±"""
        # íŠ¸ë˜í”½ í• ë‹¹ í•©ì´ 1ì¸ì§€ í™•ì¸
        total_allocation = sum(config.traffic_allocation.values())
        if abs(total_allocation - 1.0) > 0.001:
            raise ValueError(f"Traffic allocation must sum to 1.0, got {total_allocation}")
        
        self.experiments[config.experiment_id] = config
        print(f"Created experiment: {config.name}")
    
    def get_variant(self, experiment_id: str, user_id: str) -> Optional[ExperimentVariant]:
        """ì‚¬ìš©ìì—ê²Œ í• ë‹¹í•  ì‹¤í—˜ ë³€í˜• ê²°ì •"""
        if experiment_id not in self.experiments:
            return None
        
        config = self.experiments[experiment_id]
        
        # ì‹¤í—˜ ê¸°ê°„ ì²´í¬
        now = datetime.now()
        if now < config.start_date or now > config.end_date:
            return ExperimentVariant.CONTROL
        
        # Deterministic assignment based on user_id
        hash_input = f"{experiment_id}:{user_id}".encode()
        hash_value = int(hashlib.md5(hash_input).hexdigest(), 16)
        bucket = (hash_value % 10000) / 10000.0
        
        # íŠ¸ë˜í”½ í• ë‹¹ì— ë”°ë¼ ë³€í˜• ì„ íƒ
        cumulative = 0.0
        for variant, allocation in config.traffic_allocation.items():
            cumulative += allocation
            if bucket < cumulative:
                return variant
        
        return ExperimentVariant.CONTROL
    
    async def run_experiment_request(self, experiment_id: str, user_id: str,
                                   query: str, context: Dict[str, Any]) -> Dict[str, Any]:
        """ì‹¤í—˜ ìš”ì²­ ì‹¤í–‰"""
        # ë³€í˜• í• ë‹¹
        variant = self.get_variant(experiment_id, user_id)
        if variant is None:
            variant = ExperimentVariant.CONTROL
        
        # ë³€í˜•ë³„ ì„¤ì • ê°€ì ¸ì˜¤ê¸°
        variant_config = self.variant_configs[variant]
        
        # RAG ì‹¤í–‰ (ì‹œë®¬ë ˆì´ì…˜)
        start_time = datetime.now()
        response = await self._execute_rag_variant(query, context, variant_config)
        latency = (datetime.now() - start_time).total_seconds() * 1000
        
        # ë©”íŠ¸ë¦­ ìˆ˜ì§‘
        metrics = {
            'latency': latency,
            'relevance_score': response.get('relevance_score', 0),
            'click_through': 0,  # ë‚˜ì¤‘ì— ì—…ë°ì´íŠ¸
            'dwell_time': 0,  # ë‚˜ì¤‘ì— ì—…ë°ì´íŠ¸
            'thumbs_up': 0,  # ë‚˜ì¤‘ì— ì—…ë°ì´íŠ¸
        }
        
        # ê²°ê³¼ ê¸°ë¡
        result = ExperimentResult(
            variant=variant,
            user_id=user_id,
            metrics=metrics,
            timestamp=datetime.now(),
            metadata={
                'query': query,
                'response': response
            }
        )
        self.results[experiment_id].append(result)
        
        # ì‘ë‹µì— ì‹¤í—˜ ì •ë³´ ì¶”ê°€
        response['experiment'] = {
            'id': experiment_id,
            'variant': variant.value
        }
        
        return response
    
    async def _execute_rag_variant(self, query: str, context: Dict[str, Any],
                                  variant_config: Dict[str, Any]) -> Dict[str, Any]:
        """ë³€í˜•ë³„ RAG ì‹¤í–‰ (ì‹œë®¬ë ˆì´ì…˜)"""
        # ì‹¤ì œë¡œëŠ” ê° ì„¤ì •ì— ë”°ë¼ ë‹¤ë¥¸ RAG íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
        
        # ì‹œë®¬ë ˆì´ì…˜ì„ ìœ„í•œ ê°€ì§œ ì‘ë‹µ
        base_score = 0.8
        if variant_config['retrieval']['reranking']:
            base_score += 0.05
        if variant_config['generation']['model'] == 'gpt-4':
            base_score += 0.1
        
        return {
            'answer': f"Sample answer using {variant_config['generation']['model']}",
            'documents': [{'id': f'doc{i}', 'score': 0.9-i*0.1} 
                         for i in range(variant_config['retrieval']['top_k'])],
            'relevance_score': min(base_score + np.random.normal(0, 0.05), 1.0),
            'config': variant_config
        }
    
    def update_metrics(self, experiment_id: str, user_id: str, 
                      metric_updates: Dict[str, float]):
        """ì‚¬ìš©ì í–‰ë™ ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸"""
        # í•´ë‹¹ ì‚¬ìš©ìì˜ ìµœì‹  ê²°ê³¼ ì°¾ê¸°
        experiment_results = self.results.get(experiment_id, [])
        
        for result in reversed(experiment_results):
            if result.user_id == user_id:
                result.metrics.update(metric_updates)
                break
    
    def analyze_experiment(self, experiment_id: str) -> Dict[str, Any]:
        """ì‹¤í—˜ ê²°ê³¼ ë¶„ì„"""
        if experiment_id not in self.experiments:
            return {'error': 'Experiment not found'}
        
        config = self.experiments[experiment_id]
        results = self.results.get(experiment_id, [])
        
        if len(results) < config.minimum_sample_size:
            return {
                'status': 'insufficient_data',
                'current_sample_size': len(results),
                'required_sample_size': config.minimum_sample_size
            }
        
        # ë³€í˜•ë³„ ê²°ê³¼ ë¶„ë¦¬
        variant_results = defaultdict(list)
        for result in results:
            variant_results[result.variant].append(result)
        
        # ê° ë©”íŠ¸ë¦­ì— ëŒ€í•œ ë¶„ì„
        analysis = {
            'experiment_id': experiment_id,
            'name': config.name,
            'sample_sizes': {v.value: len(results) 
                           for v, results in variant_results.items()},
            'metric_analysis': {}
        }
        
        # Success metrics ë¶„ì„
        for metric in config.success_metrics:
            metric_analysis = self._analyze_metric(
                variant_results, metric, config.confidence_level
            )
            analysis['metric_analysis'][metric] = metric_analysis
        
        # Guardrail metrics ì²´í¬
        guardrail_violations = []
        for metric in config.guardrail_metrics:
            violation = self._check_guardrail(variant_results, metric)
            if violation:
                guardrail_violations.append(violation)
        
        analysis['guardrail_violations'] = guardrail_violations
        
        # ìŠ¹ì ê²°ì •
        analysis['winner'] = self._determine_winner(
            analysis['metric_analysis'], config.success_metrics
        )
        
        return analysis
    
    def _analyze_metric(self, variant_results: Dict[ExperimentVariant, List[ExperimentResult]],
                       metric_name: str, confidence_level: float) -> Dict[str, Any]:
        """ê°œë³„ ë©”íŠ¸ë¦­ ë¶„ì„"""
        metric_values = {}
        for variant, results in variant_results.items():
            values = [r.metrics.get(metric_name, 0) for r in results]
            metric_values[variant] = values
        
        control_values = metric_values.get(ExperimentVariant.CONTROL, [])
        if not control_values:
            return {'error': 'No control data'}
        
        analysis = {
            'means': {},
            'confidence_intervals': {},
            'lifts': {},
            'p_values': {}
        }
        
        # ê° ë³€í˜•ì— ëŒ€í•œ ë¶„ì„
        for variant, values in metric_values.items():
            if not values:
                continue
            
            # ê¸°ë³¸ í†µê³„
            mean = np.mean(values)
            std = np.std(values)
            n = len(values)
            
            analysis['means'][variant.value] = mean
            
            # ì‹ ë¢°êµ¬ê°„
            ci = stats.t.interval(confidence_level, n-1, 
                                 loc=mean, scale=std/np.sqrt(n))
            analysis['confidence_intervals'][variant.value] = ci
            
            # Control ëŒ€ë¹„ ìƒìŠ¹ë¥ 
            if variant != ExperimentVariant.CONTROL:
                control_mean = np.mean(control_values)
                lift = (mean - control_mean) / control_mean * 100
                analysis['lifts'][variant.value] = lift
                
                # T-test
                t_stat, p_value = stats.ttest_ind(values, control_values)
                analysis['p_values'][variant.value] = p_value
        
        return analysis
    
    def _check_guardrail(self, variant_results: Dict[ExperimentVariant, List[ExperimentResult]],
                        metric_name: str) -> Optional[Dict[str, Any]]:
        """Guardrail ë©”íŠ¸ë¦­ ì²´í¬"""
        control_values = [r.metrics.get(metric_name, 0) 
                         for r in variant_results.get(ExperimentVariant.CONTROL, [])]
        
        if not control_values:
            return None
        
        control_mean = np.mean(control_values)
        
        violations = []
        for variant, results in variant_results.items():
            if variant == ExperimentVariant.CONTROL:
                continue
            
            values = [r.metrics.get(metric_name, 0) for r in results]
            if not values:
                continue
            
            variant_mean = np.mean(values)
            
            # 5% ì´ìƒ ì„±ëŠ¥ ì €í•˜ ì²´í¬
            degradation = (control_mean - variant_mean) / control_mean * 100
            if degradation > 5:
                violations.append({
                    'variant': variant.value,
                    'metric': metric_name,
                    'degradation': degradation
                })
        
        return violations[0] if violations else None
    
    def _determine_winner(self, metric_analysis: Dict[str, Dict[str, Any]],
                         success_metrics: List[str]) -> Optional[str]:
        """ìŠ¹ì ê²°ì •"""
        scores = defaultdict(float)
        
        for metric in success_metrics:
            analysis = metric_analysis.get(metric, {})
            p_values = analysis.get('p_values', {})
            lifts = analysis.get('lifts', {})
            
            for variant in [ExperimentVariant.TREATMENT_A, ExperimentVariant.TREATMENT_B]:
                variant_name = variant.value
                
                # í†µê³„ì ìœ¼ë¡œ ìœ ì˜í•˜ê³  ê°œì„ ì´ ìˆëŠ” ê²½ìš°
                if (variant_name in p_values and 
                    p_values[variant_name] < 0.05 and
                    variant_name in lifts and 
                    lifts[variant_name] > 0):
                    scores[variant_name] += lifts[variant_name]
        
        # ê°€ì¥ ë†’ì€ ì ìˆ˜ì˜ ë³€í˜• ì„ íƒ
        if scores:
            winner = max(scores.items(), key=lambda x: x[1])
            return winner[0]
        
        return None
    
    def generate_report(self, experiment_id: str) -> str:
        """ì‹¤í—˜ ë¦¬í¬íŠ¸ ìƒì„±"""
        analysis = self.analyze_experiment(experiment_id)
        
        if 'error' in analysis:
            return f"Error: {analysis['error']}"
        
        report = []
        report.append(f"# Experiment Report: {analysis.get('name', experiment_id)}")
        report.append(f"\n## Sample Sizes")
        for variant, size in analysis['sample_sizes'].items():
            report.append(f"- {variant}: {size}")
        
        report.append(f"\n## Metric Analysis")
        for metric, metric_analysis in analysis['metric_analysis'].items():
            report.append(f"\n### {metric}")
            
            # í‰ê· ê°’
            report.append("**Means:**")
            for variant, mean in metric_analysis['means'].items():
                report.append(f"- {variant}: {mean:.4f}")
            
            # ìƒìŠ¹ë¥ 
            if metric_analysis.get('lifts'):
                report.append("\n**Lifts vs Control:**")
                for variant, lift in metric_analysis['lifts'].items():
                    p_value = metric_analysis['p_values'].get(variant, 1)
                    sig = "âœ…" if p_value < 0.05 else "âŒ"
                    report.append(f"- {variant}: {lift:+.2f}% (p={p_value:.4f}) {sig}")
        
        # Guardrail violations
        if analysis.get('guardrail_violations'):
            report.append("\n## âš ï¸ Guardrail Violations")
            for violation in analysis['guardrail_violations']:
                report.append(f"- {violation['variant']}: {violation['metric']} "
                            f"degraded by {violation['degradation']:.2f}%")
        
        # Winner
        if analysis.get('winner'):
            report.append(f"\n## ğŸ† Winner: {analysis['winner']}")
        else:
            report.append("\n## No clear winner yet")
        
        return "\n".join(report)

# ì‹¤ì‹œê°„ ì‹¤í—˜ ëŒ€ì‹œë³´ë“œ
class ExperimentDashboard:
    def __init__(self, framework: RAGExperimentationFramework):
        """ì‹¤í—˜ ëŒ€ì‹œë³´ë“œ"""
        self.framework = framework
        
    def get_live_metrics(self, experiment_id: str) -> Dict[str, Any]:
        """ì‹¤ì‹œê°„ ë©”íŠ¸ë¦­ ì¡°íšŒ"""
        results = self.framework.results.get(experiment_id, [])
        if not results:
            return {}
        
        # ìµœê·¼ 1ì‹œê°„ ë°ì´í„°
        cutoff = datetime.now() - timedelta(hours=1)
        recent_results = [r for r in results if r.timestamp > cutoff]
        
        # ë³€í˜•ë³„ ì‹¤ì‹œê°„ ë©”íŠ¸ë¦­
        metrics = defaultdict(lambda: defaultdict(list))
        for result in recent_results:
            for metric, value in result.metrics.items():
                metrics[result.variant.value][metric].append(value)
        
        # ì§‘ê³„
        live_metrics = {}
        for variant, variant_metrics in metrics.items():
            live_metrics[variant] = {}
            for metric, values in variant_metrics.items():
                live_metrics[variant][metric] = {
                    'current': values[-1] if values else 0,
                    'mean': np.mean(values) if values else 0,
                    'trend': 'up' if len(values) > 1 and values[-1] > values[-2] else 'down'
                }
        
        return live_metrics

# ì‚¬ìš© ì˜ˆì œ
print("=== RAG A/B í…ŒìŠ¤íŒ… ë°ëª¨ ===\n")

# í”„ë ˆì„ì›Œí¬ ì´ˆê¸°í™”
framework = RAGExperimentationFramework()

# ì‹¤í—˜ ìƒì„±
experiment_config = ExperimentConfig(
    experiment_id="rag_reranking_test",
    name="Reranking Algorithm Comparison",
    description="Compare different reranking strategies",
    start_date=datetime.now(),
    end_date=datetime.now() + timedelta(days=7),
    traffic_allocation={
        ExperimentVariant.CONTROL: 0.33,
        ExperimentVariant.TREATMENT_A: 0.33,
        ExperimentVariant.TREATMENT_B: 0.34
    },
    success_metrics=['relevance_score', 'click_through'],
    guardrail_metrics=['latency'],
    minimum_sample_size=100
)

framework.create_experiment(experiment_config)

# ì‹œë®¬ë ˆì´ì…˜: ì—¬ëŸ¬ ì‚¬ìš©ì ìš”ì²­ ì‹¤í–‰
async def simulate_requests():
    for i in range(150):
        user_id = f"user_{i}"
        query = f"Test query {i % 10}"
        
        response = await framework.run_experiment_request(
            "rag_reranking_test", user_id, query, {}
        )
        
        # ì‹œë®¬ë ˆì´ì…˜: ì‚¬ìš©ì í–‰ë™
        if response.get('relevance_score', 0) > 0.85:
            framework.update_metrics(
                "rag_reranking_test", user_id,
                {'click_through': 1, 'dwell_time': np.random.randint(10, 60)}
            )

# ì‹¤í–‰
asyncio.run(simulate_requests())

# ë¶„ì„
print("\n" + framework.generate_report("rag_reranking_test"))`}
                </pre>
              </div>
            </div>
          </div>
        </section>

        {/* Section 3: ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ */}
        <section className="bg-white dark:bg-gray-800 rounded-2xl p-8 shadow-sm border border-gray-200 dark:border-gray-700">
          <div className="flex items-center gap-3 mb-6">
            <div className="w-12 h-12 rounded-xl bg-purple-100 dark:bg-purple-900/20 flex items-center justify-center">
              <Activity className="text-purple-600" size={24} />
            </div>
            <div>
              <h2 className="text-2xl font-bold text-gray-900 dark:text-white">5.3 ì‹¤ì‹œê°„ RAG ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ</h2>
              <p className="text-gray-600 dark:text-gray-400">Grafana + Prometheus ê¸°ë°˜ ì¢…í•© ëª¨ë‹ˆí„°ë§</p>
            </div>
          </div>

          <div className="space-y-6">
            <div className="bg-purple-50 dark:bg-purple-900/20 p-6 rounded-xl border border-purple-200 dark:border-purple-700">
              <h3 className="font-bold text-purple-800 dark:text-purple-200 mb-4">í”„ë¡œë•ì…˜ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ êµ¬ì¶•</h3>
              
              <div className="bg-slate-50 dark:bg-slate-800 p-4 rounded-lg border border-slate-200 dark:border-slate-700 overflow-x-auto">
                <pre className="text-sm text-slate-800 dark:text-slate-200 font-mono">
{`from prometheus_client import Counter, Histogram, Gauge, Summary
import time
from typing import Dict, List, Any, Optional
from datetime import datetime, timedelta
import asyncio
import aiohttp
from dataclasses import dataclass
import pandas as pd

# Prometheus ë©”íŠ¸ë¦­ ì •ì˜
rag_requests_total = Counter(
    'rag_requests_total',
    'Total number of RAG requests',
    ['endpoint', 'status']
)

rag_latency_seconds = Histogram(
    'rag_latency_seconds',
    'RAG request latency in seconds',
    ['operation'],
    buckets=[0.1, 0.25, 0.5, 0.75, 1.0, 2.5, 5.0, 10.0]
)

rag_document_retrieval = Histogram(
    'rag_document_retrieval_count',
    'Number of documents retrieved per request',
    buckets=[1, 5, 10, 20, 50, 100]
)

rag_relevance_score = Summary(
    'rag_relevance_score',
    'Relevance scores of RAG responses'
)

rag_active_users = Gauge(
    'rag_active_users',
    'Number of active users in the last 5 minutes'
)

rag_cache_hit_rate = Gauge(
    'rag_cache_hit_rate',
    'Cache hit rate percentage'
)

rag_model_latency = Histogram(
    'rag_model_latency_seconds',
    'Model inference latency',
    ['model_name', 'operation'],
    buckets=[0.05, 0.1, 0.25, 0.5, 1.0, 2.5]
)

@dataclass
class RAGMetrics:
    """RAG ìš”ì²­ ë©”íŠ¸ë¦­"""
    request_id: str
    timestamp: datetime
    query: str
    latency_total: float
    latency_retrieval: float
    latency_generation: float
    documents_retrieved: int
    relevance_score: float
    cache_hit: bool
    error: Optional[str] = None
    user_id: Optional[str] = None
    
class RAGMonitoringService:
    def __init__(self):
        """
        ì‹¤ì‹œê°„ RAG ëª¨ë‹ˆí„°ë§ ì„œë¹„ìŠ¤
        - Prometheus ë©”íŠ¸ë¦­ ìˆ˜ì§‘
        - ì‹¤ì‹œê°„ ì´ìƒ íƒì§€
        - ëŒ€ì‹œë³´ë“œ ë°ì´í„° ì œê³µ
        """
        self.metrics_buffer: List[RAGMetrics] = []
        self.active_users: set = set()
        self.alert_rules = self._init_alert_rules()
        
    def _init_alert_rules(self) -> Dict[str, Dict[str, Any]]:
        """ì•Œë¦¼ ê·œì¹™ ì´ˆê¸°í™”"""
        return {
            'high_latency': {
                'condition': lambda m: m.latency_total > 2.0,
                'severity': 'warning',
                'message': 'High latency detected: {latency_total:.2f}s'
            },
            'low_relevance': {
                'condition': lambda m: m.relevance_score < 0.7,
                'severity': 'warning',
                'message': 'Low relevance score: {relevance_score:.2f}'
            },
            'error_rate': {
                'condition': lambda metrics: self._calculate_error_rate(metrics) > 0.05,
                'severity': 'critical',
                'message': 'Error rate exceeds 5%'
            },
            'cache_miss': {
                'condition': lambda metrics: self._calculate_cache_hit_rate(metrics) < 0.3,
                'severity': 'info',
                'message': 'Cache hit rate below 30%'
            }
        }
    
    async def record_request(self, metrics: RAGMetrics):
        """ìš”ì²­ ë©”íŠ¸ë¦­ ê¸°ë¡"""
        # Prometheus ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
        rag_requests_total.labels(
            endpoint='rag_query',
            status='success' if not metrics.error else 'error'
        ).inc()
        
        rag_latency_seconds.labels(operation='total').observe(metrics.latency_total)
        rag_latency_seconds.labels(operation='retrieval').observe(metrics.latency_retrieval)
        rag_latency_seconds.labels(operation='generation').observe(metrics.latency_generation)
        
        rag_document_retrieval.observe(metrics.documents_retrieved)
        rag_relevance_score.observe(metrics.relevance_score)
        
        # í™œì„± ì‚¬ìš©ì ì¶”ì 
        if metrics.user_id:
            self.active_users.add(metrics.user_id)
        
        # ë²„í¼ì— ì¶”ê°€
        self.metrics_buffer.append(metrics)
        
        # ì´ìƒ íƒì§€
        await self._check_alerts(metrics)
        
        # ì£¼ê¸°ì  ì •ë¦¬ (5ë¶„ ì´ìƒ ëœ ë°ì´í„°)
        cutoff = datetime.now() - timedelta(minutes=5)
        self.metrics_buffer = [m for m in self.metrics_buffer if m.timestamp > cutoff]
    
    async def _check_alerts(self, metrics: RAGMetrics):
        """ì•Œë¦¼ ê·œì¹™ ì²´í¬"""
        for rule_name, rule in self.alert_rules.items():
            try:
                if rule_name in ['error_rate', 'cache_miss']:
                    # ì§‘ê³„ ê¸°ë°˜ ê·œì¹™
                    if rule['condition'](self.metrics_buffer):
                        await self._send_alert(rule_name, rule)
                else:
                    # ê°œë³„ ë©”íŠ¸ë¦­ ê¸°ë°˜ ê·œì¹™
                    if rule['condition'](metrics):
                        await self._send_alert(rule_name, rule, metrics)
            except Exception as e:
                print(f"Alert check error: {e}")
    
    async def _send_alert(self, rule_name: str, rule: Dict[str, Any], 
                         metrics: Optional[RAGMetrics] = None):
        """ì•Œë¦¼ ë°œì†¡"""
        message = rule['message']
        if metrics:
            message = message.format(**metrics.__dict__)
        
        alert = {
            'rule': rule_name,
            'severity': rule['severity'],
            'message': message,
            'timestamp': datetime.now()
        }
        
        # ì‹¤ì œë¡œëŠ” Slack, PagerDuty ë“±ìœ¼ë¡œ ë°œì†¡
        print(f"ğŸš¨ ALERT [{rule['severity'].upper()}]: {message}")
        
        # Webhook í˜¸ì¶œ (ì˜ˆì œ)
        if rule['severity'] == 'critical':
            await self._send_webhook(alert)
    
    async def _send_webhook(self, alert: Dict[str, Any]):
        """Webhook ë°œì†¡"""
        webhook_url = "https://hooks.slack.com/services/YOUR/WEBHOOK/URL"
        
        payload = {
            'text': f"RAG System Alert",
            'attachments': [{
                'color': 'danger',
                'fields': [
                    {'title': 'Rule', 'value': alert['rule'], 'short': True},
                    {'title': 'Severity', 'value': alert['severity'], 'short': True},
                    {'title': 'Message', 'value': alert['message']},
                    {'title': 'Time', 'value': alert['timestamp'].isoformat()}
                ]
            }]
        }
        
        # async with aiohttp.ClientSession() as session:
        #     await session.post(webhook_url, json=payload)
    
    def _calculate_error_rate(self, metrics: List[RAGMetrics]) -> float:
        """ì—ëŸ¬ìœ¨ ê³„ì‚°"""
        if not metrics:
            return 0.0
        
        error_count = sum(1 for m in metrics if m.error is not None)
        return error_count / len(metrics)
    
    def _calculate_cache_hit_rate(self, metrics: List[RAGMetrics]) -> float:
        """ìºì‹œ íˆíŠ¸ìœ¨ ê³„ì‚°"""
        if not metrics:
            return 0.0
        
        cache_hits = sum(1 for m in metrics if m.cache_hit)
        return cache_hits / len(metrics)
    
    def get_dashboard_data(self) -> Dict[str, Any]:
        """ëŒ€ì‹œë³´ë“œìš© ë°ì´í„° ì¡°íšŒ"""
        if not self.metrics_buffer:
            return self._empty_dashboard_data()
        
        # ë°ì´í„°í”„ë ˆì„ ë³€í™˜
        df = pd.DataFrame([m.__dict__ for m in self.metrics_buffer])
        
        # í˜„ì¬ ë©”íŠ¸ë¦­
        current_metrics = {
            'qps': len(df[df['timestamp'] > datetime.now() - timedelta(seconds=60)]),
            'avg_latency': df['latency_total'].mean(),
            'p95_latency': df['latency_total'].quantile(0.95),
            'avg_relevance': df['relevance_score'].mean(),
            'error_rate': self._calculate_error_rate(self.metrics_buffer),
            'cache_hit_rate': self._calculate_cache_hit_rate(self.metrics_buffer),
            'active_users': len(self.active_users),
            'total_requests': len(df)
        }
        
        # ì‹œê³„ì—´ ë°ì´í„° (1ë¶„ ë‹¨ìœ„)
        df['timestamp_minute'] = df['timestamp'].dt.floor('T')
        time_series = {
            'latency': df.groupby('timestamp_minute')['latency_total'].agg(['mean', 'p95']),
            'throughput': df.groupby('timestamp_minute').size(),
            'relevance': df.groupby('timestamp_minute')['relevance_score'].mean()
        }
        
        # Top ëŠë¦° ì¿¼ë¦¬
        slow_queries = df.nlargest(10, 'latency_total')[
            ['query', 'latency_total', 'documents_retrieved']
        ].to_dict('records')
        
        # ëª¨ë¸ë³„ ë ˆì´í„´ì‹œ
        model_latency = {
            'retrieval': {
                'mean': df['latency_retrieval'].mean(),
                'p95': df['latency_retrieval'].quantile(0.95)
            },
            'generation': {
                'mean': df['latency_generation'].mean(),
                'p95': df['latency_generation'].quantile(0.95)
            }
        }
        
        return {
            'current_metrics': current_metrics,
            'time_series': time_series,
            'slow_queries': slow_queries,
            'model_latency': model_latency,
            'last_updated': datetime.now()
        }
    
    def _empty_dashboard_data(self) -> Dict[str, Any]:
        """ë¹ˆ ëŒ€ì‹œë³´ë“œ ë°ì´í„°"""
        return {
            'current_metrics': {
                'qps': 0,
                'avg_latency': 0,
                'p95_latency': 0,
                'avg_relevance': 0,
                'error_rate': 0,
                'cache_hit_rate': 0,
                'active_users': 0,
                'total_requests': 0
            },
            'time_series': {},
            'slow_queries': [],
            'model_latency': {},
            'last_updated': datetime.now()
        }
    
    async def health_check(self) -> Dict[str, Any]:
        """ì‹œìŠ¤í…œ í—¬ìŠ¤ ì²´í¬"""
        health_status = {
            'status': 'healthy',
            'checks': {},
            'timestamp': datetime.now()
        }
        
        # ê°œë³„ ì»´í¬ë„ŒíŠ¸ ì²´í¬
        checks = {
            'metrics_collection': self._check_metrics_collection(),
            'alert_system': self._check_alert_system(),
            'data_freshness': self._check_data_freshness()
        }
        
        health_status['checks'] = checks
        
        # ì „ì²´ ìƒíƒœ ê²°ì •
        if any(check['status'] == 'unhealthy' for check in checks.values()):
            health_status['status'] = 'unhealthy'
        elif any(check['status'] == 'degraded' for check in checks.values()):
            health_status['status'] = 'degraded'
        
        return health_status
    
    def _check_metrics_collection(self) -> Dict[str, str]:
        """ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ìƒíƒœ ì²´í¬"""
        if not self.metrics_buffer:
            return {'status': 'unhealthy', 'message': 'No metrics collected'}
        
        latest_metric = max(self.metrics_buffer, key=lambda m: m.timestamp)
        age = (datetime.now() - latest_metric.timestamp).total_seconds()
        
        if age > 300:  # 5ë¶„ ì´ìƒ ëœ ë°ì´í„°
            return {'status': 'unhealthy', 'message': f'Stale data: {age:.0f}s old'}
        elif age > 60:  # 1ë¶„ ì´ìƒ
            return {'status': 'degraded', 'message': f'Delayed data: {age:.0f}s old'}
        
        return {'status': 'healthy', 'message': 'Collecting metrics normally'}
    
    def _check_alert_system(self) -> Dict[str, str]:
        """ì•Œë¦¼ ì‹œìŠ¤í…œ ìƒíƒœ ì²´í¬"""
        # ì‹¤ì œë¡œëŠ” ë§ˆì§€ë§‰ ì•Œë¦¼ ë°œì†¡ ì‹œê°„ ë“±ì„ ì²´í¬
        return {'status': 'healthy', 'message': 'Alert system operational'}
    
    def _check_data_freshness(self) -> Dict[str, str]:
        """ë°ì´í„° ì‹ ì„ ë„ ì²´í¬"""
        if not self.metrics_buffer:
            return {'status': 'unhealthy', 'message': 'No data available'}
        
        # ìµœê·¼ 1ë¶„ê°„ ë°ì´í„° ê°œìˆ˜
        recent_count = sum(1 for m in self.metrics_buffer 
                          if m.timestamp > datetime.now() - timedelta(minutes=1))
        
        if recent_count < 10:
            return {'status': 'degraded', 'message': f'Low traffic: {recent_count} requests/min'}
        
        return {'status': 'healthy', 'message': f'Normal traffic: {recent_count} requests/min'}

# Grafana ëŒ€ì‹œë³´ë“œ ì„¤ì •
GRAFANA_DASHBOARD_CONFIG = {
    "dashboard": {
        "title": "RAG System Monitoring",
        "panels": [
            {
                "title": "Request Rate",
                "targets": [{
                    "expr": "rate(rag_requests_total[5m])",
                    "legendFormat": "{{endpoint}} - {{status}}"
                }]
            },
            {
                "title": "Latency Percentiles",
                "targets": [{
                    "expr": "histogram_quantile(0.95, rate(rag_latency_seconds_bucket[5m]))",
                    "legendFormat": "P95 {{operation}}"
                }]
            },
            {
                "title": "Relevance Score Distribution",
                "targets": [{
                    "expr": "rag_relevance_score",
                    "legendFormat": "Relevance Score"
                }]
            },
            {
                "title": "Cache Hit Rate",
                "targets": [{
                    "expr": "rag_cache_hit_rate",
                    "legendFormat": "Cache Hit %"
                }]
            },
            {
                "title": "Active Users",
                "targets": [{
                    "expr": "rag_active_users",
                    "legendFormat": "Active Users"
                }]
            },
            {
                "title": "Error Rate",
                "targets": [{
                    "expr": "rate(rag_requests_total{status='error'}[5m]) / rate(rag_requests_total[5m])",
                    "legendFormat": "Error Rate"
                }]
            }
        ]
    }
}

# ì‚¬ìš© ì˜ˆì œ
async def simulate_rag_traffic():
    """RAG íŠ¸ë˜í”½ ì‹œë®¬ë ˆì´ì…˜"""
    monitoring = RAGMonitoringService()
    
    # 100ê°œì˜ ìš”ì²­ ì‹œë®¬ë ˆì´ì…˜
    for i in range(100):
        # ë©”íŠ¸ë¦­ ìƒì„±
        latency_retrieval = np.random.exponential(0.2)  # í‰ê·  200ms
        latency_generation = np.random.exponential(0.5)  # í‰ê·  500ms
        
        metrics = RAGMetrics(
            request_id=f"req_{i}",
            timestamp=datetime.now(),
            query=f"Query {i % 20}",
            latency_total=latency_retrieval + latency_generation + np.random.exponential(0.1),
            latency_retrieval=latency_retrieval,
            latency_generation=latency_generation,
            documents_retrieved=np.random.randint(5, 20),
            relevance_score=min(0.95, max(0.5, np.random.normal(0.85, 0.1))),
            cache_hit=np.random.random() > 0.6,
            error="timeout" if np.random.random() < 0.02 else None,
            user_id=f"user_{i % 30}"
        )
        
        await monitoring.record_request(metrics)
        await asyncio.sleep(0.1)  # 100ms ê°„ê²©
    
    # ëŒ€ì‹œë³´ë“œ ë°ì´í„° ì¡°íšŒ
    dashboard_data = monitoring.get_dashboard_data()
    
    print("=== RAG Monitoring Dashboard ===")
    print(f"Current Metrics:")
    for metric, value in dashboard_data['current_metrics'].items():
        if isinstance(value, float):
            print(f"  {metric}: {value:.3f}")
        else:
            print(f"  {metric}: {value}")
    
    print(f"\nTop Slow Queries:")
    for query in dashboard_data['slow_queries'][:5]:
        print(f"  - {query['query']}: {query['latency_total']:.2f}s")
    
    # í—¬ìŠ¤ ì²´í¬
    health = await monitoring.health_check()
    print(f"\nHealth Status: {health['status']}")
    for component, status in health['checks'].items():
        print(f"  {component}: {status['status']} - {status['message']}")

# ì‹¤í–‰
print("=== RAG Monitoring System Demo ===\n")
asyncio.run(simulate_rag_traffic())`}
                </pre>
              </div>
            </div>

            <div className="bg-green-50 dark:bg-green-900/20 p-6 rounded-xl border border-green-200 dark:border-green-700">
              <h3 className="font-bold text-green-800 dark:text-green-200 mb-4">ì‹¤ì œ Grafana ëŒ€ì‹œë³´ë“œ êµ¬ì„±</h3>
              
              <div className="grid md:grid-cols-2 gap-4">
                <div className="bg-white dark:bg-gray-800 p-4 rounded-lg border">
                  <h4 className="font-medium text-gray-900 dark:text-white mb-2">ğŸ¯ í•µì‹¬ ë©”íŠ¸ë¦­</h4>
                  <ul className="text-sm text-gray-700 dark:text-gray-300 space-y-1">
                    <li>â€¢ Request Rate (QPS)</li>
                    <li>â€¢ Latency (P50, P95, P99)</li>
                    <li>â€¢ Error Rate & Success Rate</li>
                    <li>â€¢ Active Users</li>
                    <li>â€¢ Resource Utilization</li>
                  </ul>
                </div>
                
                <div className="bg-white dark:bg-gray-800 p-4 rounded-lg border">
                  <h4 className="font-medium text-gray-900 dark:text-white mb-2">ğŸ“Š ìƒì„¸ ë¶„ì„</h4>
                  <ul className="text-sm text-gray-700 dark:text-gray-300 space-y-1">
                    <li>â€¢ Query Distribution</li>
                    <li>â€¢ Document Retrieval Stats</li>
                    <li>â€¢ Model Performance</li>
                    <li>â€¢ Cache Performance</li>
                    <li>â€¢ Cost Analysis</li>
                  </ul>
                </div>
              </div>

              <div className="mt-4 bg-emerald-100 dark:bg-emerald-900/40 p-4 rounded-lg">
                <p className="text-sm text-emerald-800 dark:text-emerald-200">
                  <strong>ğŸ’¡ Pro Tip:</strong> Golden Signals (Latency, Traffic, Errors, Saturation)ë¥¼
                  ê¸°ë°˜ìœ¼ë¡œ ëŒ€ì‹œë³´ë“œë¥¼ êµ¬ì„±í•˜ë©´ ì‹œìŠ¤í…œ ìƒíƒœë¥¼ íš¨ê³¼ì ìœ¼ë¡œ íŒŒì•…í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
                  íŠ¹íˆ RAG ì‹œìŠ¤í…œì—ì„œëŠ” Relevance Scoreë¥¼ ì¶”ê°€ ì§€í‘œë¡œ í™œìš©í•˜ì„¸ìš”.
                </p>
              </div>
            </div>
          </div>
        </section>

        {/* Practical Exercise */}
        <section className="bg-gradient-to-r from-emerald-500 to-teal-600 rounded-2xl p-8 text-white">
          <h2 className="text-2xl font-bold mb-6">ì‹¤ìŠµ ê³¼ì œ</h2>
          
          <div className="bg-white/10 rounded-xl p-6 backdrop-blur">
            <h3 className="font-bold mb-4">ì¢…í•© RAG í‰ê°€ ë° ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ êµ¬ì¶•</h3>
            
            <div className="space-y-4">
              <div className="bg-white/10 p-4 rounded-lg">
                <h4 className="font-medium mb-2">ğŸ“‹ ìš”êµ¬ì‚¬í•­</h4>
                <ol className="space-y-2 text-sm">
                  <li>1. RAGAS í”„ë ˆì„ì›Œí¬ë¥¼ í™œìš©í•œ ì˜¤í”„ë¼ì¸ í‰ê°€ ì‹œìŠ¤í…œ êµ¬ì¶•</li>
                  <li>2. A/B í…ŒìŠ¤íŠ¸ í”Œë«í¼ êµ¬í˜„ (ìµœì†Œ 3ê°€ì§€ ë³€í˜• ì§€ì›)</li>
                  <li>3. Prometheus + Grafana ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ êµ¬ì¶•</li>
                  <li>4. ìë™ ì•Œë¦¼ ì‹œìŠ¤í…œ (Slack/Email ì—°ë™)</li>
                  <li>5. ì£¼ê°„ ì„±ëŠ¥ ë¦¬í¬íŠ¸ ìë™ ìƒì„±</li>
                </ol>
              </div>
              
              <div className="bg-white/10 p-4 rounded-lg">
                <h4 className="font-medium mb-2">ğŸ¯ í‰ê°€ ê¸°ì¤€</h4>
                <ul className="space-y-1 text-sm">
                  <li>â€¢ í‰ê°€ ë©”íŠ¸ë¦­ì˜ í¬ê´„ì„± (Retrieval + Generation + System)</li>
                  <li>â€¢ A/B í…ŒìŠ¤íŠ¸ì˜ í†µê³„ì  ì—„ë°€ì„±</li>
                  <li>â€¢ ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œì˜ ì‹¤ìš©ì„±</li>
                  <li>â€¢ ì•Œë¦¼ ì‹œìŠ¤í…œì˜ ì •í™•ì„± (False positive rate &lt; 5%)</li>
                  <li>â€¢ ì‹œìŠ¤í…œ í™•ì¥ì„± (1000+ QPS ì§€ì›)</li>
                </ul>
              </div>
              
              <div className="bg-white/10 p-4 rounded-lg">
                <h4 className="font-medium mb-2">ğŸ’¡ ë„ì „ ê³¼ì œ</h4>
                <p className="text-sm">
                  ML ê¸°ë°˜ ì´ìƒ íƒì§€ ì‹œìŠ¤í…œì„ êµ¬ì¶•í•˜ì—¬ ë‹¨ìˆœ threshold ê¸°ë°˜ ì•Œë¦¼ì„
                  ë„˜ì–´ì„œëŠ” ì§€ëŠ¥í˜• ëª¨ë‹ˆí„°ë§ì„ êµ¬í˜„í•´ë³´ì„¸ìš”. íŠ¹íˆ ê³„ì ˆì„±ê³¼ íŠ¸ë Œë“œë¥¼
                  ê³ ë ¤í•œ ë™ì  ì„ê³„ê°’ ì„¤ì •ì„ êµ¬í˜„í•˜ë©´ ë”ìš± íš¨ê³¼ì ì…ë‹ˆë‹¤.
                </p>
              </div>
            </div>
          </div>
        </section>
      </div>

      {/* Navigation */}
      <div className="mt-12 bg-white dark:bg-gray-800 rounded-2xl p-6 shadow-sm border border-gray-200 dark:border-gray-700">
        <div className="flex justify-between items-center">
          <Link
            href="/modules/rag/advanced/chapter4"
            className="inline-flex items-center gap-2 text-gray-600 dark:text-gray-400 hover:text-gray-900 dark:hover:text-white transition-colors"
          >
            <ArrowLeft size={16} />
            ì´ì „: ê³ ê¸‰ Reranking ì „ëµ
          </Link>
          
          <Link
            href="/modules/rag/advanced/chapter6"
            className="inline-flex items-center gap-2 bg-emerald-500 text-white px-6 py-3 rounded-lg font-medium hover:bg-emerald-600 transition-colors"
          >
            ë‹¤ìŒ: ìµœì‹  ì—°êµ¬ ë™í–¥
            <ArrowRight size={16} />
          </Link>
        </div>
      </div>
    </div>
  )
}