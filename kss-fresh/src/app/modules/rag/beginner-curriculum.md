# RAG ì´ˆê¸‰ ì»¤ë¦¬í˜ëŸ¼ (Step 1: Naive RAG ì™„ë²½ ë§ˆìŠ¤í„°)

## ğŸ¯ í•™ìŠµ ëª©í‘œ
RAGì˜ íƒ„ìƒ ë°°ê²½ê³¼ í•µì‹¬ ì›ë¦¬ë¥¼ ê¹Šì´ ìˆê²Œ ì´í•´í•˜ê³ , Naive RAG ì‹œìŠ¤í…œì„ ì²˜ìŒë¶€í„° ëê¹Œì§€ ì§ì ‘ êµ¬í˜„í•  ìˆ˜ ìˆëŠ” ëŠ¥ë ¥ì„ ê°–ì¶¥ë‹ˆë‹¤. ì‹¤ì œ í”„ë¡œë•ì…˜ì—ì„œ ë°œìƒí•˜ëŠ” ë¬¸ì œë“¤ì„ ê²½í—˜í•˜ê³  í•´ê²° ë°©ë²•ì„ ì²´ë“í•©ë‹ˆë‹¤.

## ğŸ“š ì´ í•™ìŠµ ì‹œê°„: 10ì‹œê°„

## ğŸ† ì°¨ë³„í™”ëœ êµìœ¡ ì² í•™
- **"ì´ë¡ ë³´ë‹¤ ì‹¤ì „"**: ëª¨ë“  ê°œë…ì„ ì½”ë“œë¡œ ì§ì ‘ êµ¬í˜„
- **"ì‹¤íŒ¨ì—ì„œ ë°°ìš°ê¸°"**: ì¼ë¶€ëŸ¬ ë¬¸ì œ ìƒí™©ì„ ë§Œë“¤ê³  í•´ê²°
- **"í˜„ì—… ê·¸ëŒ€ë¡œ"**: ì‹¤ì œ ê¸°ì—…ì—ì„œ ì‚¬ìš©í•˜ëŠ” ë„êµ¬ì™€ ë°©ë²•ë¡ 
- **"ì¸¡ì • ê°€ëŠ¥í•œ ì„±ê³¼"**: ëª…í™•í•œ í‰ê°€ ì§€í‘œì™€ ê°œì„  ë°©ë²•

## ğŸ“‹ ì»¤ë¦¬í˜ëŸ¼ êµ¬ì„±

### Module 1: RAGì˜ íƒ„ìƒ - LLMì˜ ë³¸ì§ˆì  í•œê³„ ê·¹ë³µí•˜ê¸° (2.5ì‹œê°„)

#### 1.1 LLMì˜ ì¹˜ëª…ì  ì•½ì  ì§ì ‘ ì²´í—˜í•˜ê¸° (1ì‹œê°„)
**ì‹¤ìŠµ ì¤‘ì‹¬ í•™ìŠµ**
```python
# ì‹¤ìŠµ 1: Hallucination ì‹¤ì œë¡œ ê²½í—˜í•˜ê¸°
class HallucinationDemo:
    def __init__(self):
        self.llm = OpenAI()
    
    def demonstrate_hallucination(self):
        # ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ì •ë³´ ì§ˆë¬¸
        questions = [
            "2024ë…„ ë…¸ë²¨ë¬¼ë¦¬í•™ìƒ ìˆ˜ìƒìì˜ ì—°êµ¬ ë‚´ìš©ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”",
            "ê¹€ì² ìˆ˜ êµìˆ˜ì˜ 'Quantum RAG Theory' ë…¼ë¬¸ì„ ìš”ì•½í•´ì£¼ì„¸ìš”",
            "ì„œìš¸ëŒ€í•™êµ AIì—°êµ¬ì†Œì˜ 2025ë…„ ì—°êµ¬ ê³„íšì€?",
        ]
        
        for q in questions:
            response = self.llm.complete(q)
            print(f"ì§ˆë¬¸: {q}")
            print(f"LLM ë‹µë³€: {response}")
            print(f"ì‚¬ì‹¤ ì—¬ë¶€: âŒ (ëª¨ë‘ ê°€ìƒì˜ ì •ë³´)")
            print("-" * 50)
```

**í•µì‹¬ ì¸ì‚¬ì´íŠ¸**
- LLMì€ 'ê·¸ëŸ´ë“¯í•œ ê±°ì§“ë§'ì„ ë§¤ìš° ì˜í•¨
- í•™ìŠµ ë°ì´í„°ì— ì—†ëŠ” ì •ë³´ëŠ” ì°½ì‘í•¨
- í™•ë¥ ì  ìƒì„±ì´ë¼ëŠ” ë³¸ì§ˆì  í•œê³„

#### 1.2 ì‹¤ì‹œê°„ ì •ë³´ì˜ ë¶€ì¬ (30ë¶„)
```python
# ì‹¤ìŠµ 2: ìµœì‹  ì •ë³´ í•œê³„ í…ŒìŠ¤íŠ¸
def test_knowledge_cutoff():
    current_events = [
        "ì˜¤ëŠ˜ì˜ ì½”ìŠ¤í”¼ ì§€ìˆ˜",
        "í˜„ì¬ ë¹„íŠ¸ì½”ì¸ ê°€ê²©",
        "ì–´ì œ ë°œí‘œëœ ì• í”Œ ì‹ ì œí’ˆ",
        "ì´ë²ˆì£¼ ë‚ ì”¨ ì˜ˆë³´"
    ]
    
    for event in current_events:
        print(f"âŒ LLMì€ '{event}'ë¥¼ ëª¨ë¦…ë‹ˆë‹¤")
        print(f"âœ… RAGëŠ” ì‹¤ì‹œê°„ ë°ì´í„°ë¥¼ ê²€ìƒ‰í•´ì„œ ë‹µí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤")
```

#### 1.3 ê¸°ì—… ë‚´ë¶€ ì§€ì‹ì˜ í™œìš© ë¶ˆê°€ (1ì‹œê°„)
**ì‹¤ì œ ë¹„ì¦ˆë‹ˆìŠ¤ ì‹œë‚˜ë¦¬ì˜¤**
```python
# ê¸°ì—…ì´ ì§ë©´í•˜ëŠ” ì‹¤ì œ ë¬¸ì œ
enterprise_challenges = {
    "ë²•ë¬´íŒ€": "ìš°ë¦¬ íšŒì‚¬ ê³„ì•½ì„œ í…œí”Œë¦¿ì—ì„œ Force Majeure ì¡°í•­ì€?",
    "ì¸ì‚¬íŒ€": "ì—°ì°¨ ì‚¬ìš© ê·œì • ì¤‘ ì´ì›” ê°€ëŠ¥ ì¼ìˆ˜ëŠ”?",
    "ê°œë°œíŒ€": "ìš°ë¦¬ APIì˜ rate limiting ì •ì±…ì€?",
    "ì˜ì—…íŒ€": "ì‘ë…„ 3ë¶„ê¸° ë§¤ì¶œ ë°ì´í„°ì™€ ì „ë…„ ëŒ€ë¹„ ì„±ì¥ë¥ ì€?"
}

# LLMë§Œìœ¼ë¡œëŠ” ì ˆëŒ€ ë‹µí•  ìˆ˜ ì—†ëŠ” ì§ˆë¬¸ë“¤
# RAGë¡œ í•´ê²° ê°€ëŠ¥í•œ ì‹¤ì œ use case
```

### Module 2: Naive RAG ì•„í‚¤í…ì²˜ ì™„ë²½ ì´í•´ (3ì‹œê°„)

#### 2.1 Naive RAGì˜ 3ë‹¨ê³„ í”„ë¡œì„¸ìŠ¤ (1ì‹œê°„)
```python
class NaiveRAGArchitecture:
    """
    Indexing â†’ Retrieval â†’ Generation
    ê° ë‹¨ê³„ë¥¼ ëª…í™•íˆ ë¶„ë¦¬í•˜ì—¬ ì´í•´
    """
    
    def __init__(self):
        self.vector_store = ChromaDB()
        self.embedder = OpenAIEmbeddings()
        self.llm = ChatOpenAI()
    
    def indexing_phase(self, documents):
        """1ë‹¨ê³„: ì¸ë±ì‹± - ë¬¸ì„œë¥¼ ê²€ìƒ‰ ê°€ëŠ¥í•œ í˜•íƒœë¡œ ë³€í™˜"""
        chunks = []
        for doc in documents:
            # ì²­í‚¹: ë¬¸ì„œë¥¼ ì ì ˆí•œ í¬ê¸°ë¡œ ë¶„í• 
            doc_chunks = self.chunk_document(doc)
            
            # ì„ë² ë”©: ê° ì²­í¬ë¥¼ ë²¡í„°ë¡œ ë³€í™˜
            for chunk in doc_chunks:
                embedding = self.embedder.embed(chunk.text)
                chunks.append({
                    'id': chunk.id,
                    'text': chunk.text,
                    'embedding': embedding,
                    'metadata': chunk.metadata
                })
        
        # ë²¡í„° ì €ì¥ì†Œì— ì €ì¥
        self.vector_store.add(chunks)
        return len(chunks)
    
    def retrieval_phase(self, query, top_k=5):
        """2ë‹¨ê³„: ê²€ìƒ‰ - ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ì •ë³´ ì°¾ê¸°"""
        # ì§ˆë¬¸ì„ ë²¡í„°ë¡œ ë³€í™˜
        query_embedding = self.embedder.embed(query)
        
        # ìœ ì‚¬ë„ ê²€ìƒ‰
        results = self.vector_store.search(
            query_embedding, 
            top_k=top_k
        )
        
        # ê²€ìƒ‰ ê²°ê³¼ ì •ë¦¬
        retrieved_chunks = [
            {
                'text': r.text,
                'score': r.score,
                'metadata': r.metadata
            }
            for r in results
        ]
        
        return retrieved_chunks
    
    def generation_phase(self, query, retrieved_chunks):
        """3ë‹¨ê³„: ìƒì„± - ê²€ìƒ‰ëœ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€ ìƒì„±"""
        # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        context = "\n\n".join([chunk['text'] for chunk in retrieved_chunks])
        
        prompt = f"""ë‹¤ìŒ ì •ë³´ë¥¼ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ë‹µí•˜ì„¸ìš”.
        
ì°¸ê³  ì •ë³´:
{context}

ì§ˆë¬¸: {query}

ë‹µë³€:"""
        
        # LLMìœ¼ë¡œ ë‹µë³€ ìƒì„±
        response = self.llm.generate(prompt)
        
        return response
```

#### 2.2 ê° ë‹¨ê³„ë³„ ì‹¬í™” ì´í•´ (1ì‹œê°„ 30ë¶„)
**Indexing ë‹¨ê³„ ìƒì„¸ ë¶„ì„**
```python
def deep_dive_indexing():
    """ì¸ë±ì‹±ì˜ ëª¨ë“  ê²ƒ"""
    
    # 1. ë¬¸ì„œ ë¡œë”ì˜ ë‹¤ì–‘ì„±
    loaders = {
        'pdf': PyPDFLoader,
        'docx': UnstructuredWordDocumentLoader,
        'html': BSHTMLLoader,
        'csv': CSVLoader,
        'json': JSONLoader
    }
    
    # 2. ì²­í‚¹ ì „ëµì˜ ì¤‘ìš”ì„±
    chunking_strategies = {
        'fixed_size': {
            'chunk_size': 1000,
            'chunk_overlap': 200,
            'pros': 'êµ¬í˜„ ê°„ë‹¨, ì¼ì •í•œ í¬ê¸°',
            'cons': 'ë¬¸ë§¥ ë‹¨ì ˆ ê°€ëŠ¥ì„±'
        },
        'sentence_based': {
            'sentences_per_chunk': 5,
            'pros': 'ë¬¸ì¥ ë‹¨ìœ„ ë³´ì¡´',
            'cons': 'í¬ê¸° ë¶ˆê· ì¼'
        },
        'semantic': {
            'method': 'embedding_similarity',
            'pros': 'ì˜ë¯¸ì  ì¼ê´€ì„±',
            'cons': 'ê³„ì‚° ë¹„ìš© ë†’ìŒ'
        }
    }
    
    # 3. ë©”íƒ€ë°ì´í„°ì˜ í™œìš©
    metadata_examples = {
        'source': 'document.pdf',
        'page': 42,
        'section': 'Chapter 3.2',
        'date_created': '2024-01-15',
        'author': 'John Doe',
        'department': 'Engineering'
    }
```

**Retrieval ë‹¨ê³„ ìƒì„¸ ë¶„ì„**
```python
def deep_dive_retrieval():
    """ê²€ìƒ‰ì˜ í•µì‹¬ ì´í•´"""
    
    # 1. ë²¡í„° ìœ ì‚¬ë„ ë©”íŠ¸ë¦­
    similarity_metrics = {
        'cosine': {
            'formula': 'dot(A, B) / (norm(A) * norm(B))',
            'range': '[-1, 1]',
            'use_case': 'ê°€ì¥ ì¼ë°˜ì , ë°©í–¥ì„± ì¤‘ìš”'
        },
        'euclidean': {
            'formula': 'sqrt(sum((A - B)^2))',
            'range': '[0, âˆ)',
            'use_case': 'ì‹¤ì œ ê±°ë¦¬ê°€ ì¤‘ìš”í•œ ê²½ìš°'
        },
        'dot_product': {
            'formula': 'sum(A * B)',
            'range': '(-âˆ, âˆ)',
            'use_case': 'ì •ê·œí™”ëœ ë²¡í„°ì—ì„œ ë¹ ë¥¸ ê³„ì‚°'
        }
    }
    
    # 2. Top-K ì„ íƒì˜ trade-off
    topk_analysis = {
        'k=1': 'ì •í™•ë„ ë†’ìŒ, recall ë‚®ìŒ',
        'k=5': 'ê· í˜•ì¡íŒ ì„ íƒ',
        'k=10': 'recall ë†’ìŒ, noise ì¦ê°€',
        'k=20': 'ë„ˆë¬´ ë§ì€ ì •ë³´, ì„±ëŠ¥ ì €í•˜'
    }
```

#### 2.3 ì²« ë²ˆì§¸ Naive RAG êµ¬í˜„ (30ë¶„)
```python
# ì‹¤ì œ ë™ì‘í•˜ëŠ” Naive RAG ì‹œìŠ¤í…œ
class MyFirstRAG:
    def __init__(self):
        self.documents = []
        self.embeddings = []
        self.model = SentenceTransformer('all-MiniLM-L6-v2')
    
    def add_document(self, text):
        """ë¬¸ì„œ ì¶”ê°€ ë° ì„ë² ë”©"""
        # ê°„ë‹¨í•œ ì²­í‚¹ (ë¬¸ì¥ ë‹¨ìœ„)
        sentences = text.split('. ')
        
        for sentence in sentences:
            if len(sentence) > 10:  # ë„ˆë¬´ ì§§ì€ ë¬¸ì¥ ì œì™¸
                embedding = self.model.encode(sentence)
                self.documents.append(sentence)
                self.embeddings.append(embedding)
    
    def search(self, query, top_k=3):
        """ê²€ìƒ‰"""
        query_embedding = self.model.encode(query)
        
        # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
        similarities = []
        for i, doc_embedding in enumerate(self.embeddings):
            sim = cosine_similarity(
                query_embedding.reshape(1, -1),
                doc_embedding.reshape(1, -1)
            )[0][0]
            similarities.append((i, sim))
        
        # Top-K ì„ íƒ
        similarities.sort(key=lambda x: x[1], reverse=True)
        top_results = similarities[:top_k]
        
        return [self.documents[i] for i, _ in top_results]
    
    def answer(self, query):
        """RAG ë‹µë³€ ìƒì„±"""
        # ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰
        relevant_docs = self.search(query)
        
        # ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
        context = "\n".join(relevant_docs)
        
        # í”„ë¡¬í”„íŠ¸ ìƒì„±
        prompt = f"""
Based on the following information, answer the question.

Information:
{context}

Question: {query}
Answer:"""
        
        # ì—¬ê¸°ì„œëŠ” ê°„ë‹¨íˆ ì»¨í…ìŠ¤íŠ¸ ë°˜í™˜ (ì‹¤ì œë¡œëŠ” LLM í˜¸ì¶œ)
        return f"Based on the documents: {context[:200]}..."

# ì‚¬ìš© ì˜ˆì‹œ
rag = MyFirstRAG()
rag.add_document("Pythonì€ 1991ë…„ì— ê·€ë„ ë°˜ ë¡œì„¬ì´ ê°œë°œí•œ í”„ë¡œê·¸ë˜ë° ì–¸ì–´ì…ë‹ˆë‹¤.")
rag.add_document("Pythonì€ ê°„ê²°í•˜ê³  ì½ê¸° ì‰¬ìš´ ë¬¸ë²•ìœ¼ë¡œ ìœ ëª…í•©ë‹ˆë‹¤.")
rag.add_document("ê¸°ê³„í•™ìŠµê³¼ ë°ì´í„° ë¶„ì„ì— Pythonì´ ë„ë¦¬ ì‚¬ìš©ë©ë‹ˆë‹¤.")

answer = rag.answer("Pythonì€ ì–¸ì œ ë§Œë“¤ì–´ì¡Œë‚˜ìš”?")
print(answer)
```

### Module 3: Naive RAGì˜ ì‹¤ì „ êµ¬í˜„ê³¼ í•œê³„ ì²´í—˜ (3ì‹œê°„)

#### 3.1 ì‹¤ì „ í”„ë¡œì íŠ¸ 1: íšŒì‚¬ ê·œì • Q&A ë´‡ (1ì‹œê°„)
```python
class CompanyPolicyRAG:
    """ì‹¤ì œ íšŒì‚¬ì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ê·œì • ê²€ìƒ‰ ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.vector_store = Chroma(
            collection_name="company_policies",
            embedding_function=OpenAIEmbeddings()
        )
        self.llm = ChatOpenAI(temperature=0)  # ì •í™•ì„± ì¤‘ìš”
    
    def load_policies(self, policy_dir):
        """íšŒì‚¬ ê·œì • ë¬¸ì„œ ë¡œë“œ"""
        policies = {
            "íœ´ê°€ê·œì •.pdf": "ì—°ì°¨, ë³‘ê°€, ê²½ì¡°ì‚¬ íœ´ê°€ ê·œì •",
            "ë³´ì•ˆì •ì±….docx": "ì •ë³´ë³´ì•ˆ, ì¶œì…í†µì œ ê·œì •",
            "ì¸ì‚¬ê·œì •.pdf": "ì±„ìš©, í‰ê°€, ìŠ¹ì§„ ê·œì •",
            "ë³µë¦¬í›„ìƒ.docx": "ë³µì§€ì œë„, ì§€ì›ê¸ˆ ê·œì •"
        }
        
        for filename, description in policies.items():
            # ë¬¸ì„œ ë¡œë“œ ë° ì²­í‚¹
            loader = self.get_loader(filename)
            documents = loader.load()
            
            # ë©”íƒ€ë°ì´í„° ì¶”ê°€
            for doc in documents:
                doc.metadata.update({
                    'source': filename,
                    'category': description,
                    'last_updated': '2024-01-01'
                })
            
            # ë²¡í„° ìŠ¤í† ì–´ì— ì €ì¥
            self.vector_store.add_documents(documents)
    
    def ask_policy(self, question):
        """ê·œì • ê´€ë ¨ ì§ˆë¬¸ì— ë‹µë³€"""
        # 1. ê´€ë ¨ ê·œì • ê²€ìƒ‰
        relevant_docs = self.vector_store.similarity_search(
            question, 
            k=5,
            filter=None  # íŠ¹ì • ì¹´í…Œê³ ë¦¬ë¡œ í•„í„°ë§ ê°€ëŠ¥
        )
        
        # 2. ì¶œì²˜ ì •ë³´ í¬í•¨í•œ ë‹µë³€ ìƒì„±
        context = self.format_context_with_sources(relevant_docs)
        
        prompt = f"""ë‹¹ì‹ ì€ íšŒì‚¬ ê·œì • ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ë‹¤ìŒ ê·œì • ë¬¸ì„œë¥¼ ì°¸ê³ í•˜ì—¬ ì§ì›ì˜ ì§ˆë¬¸ì— ì •í™•í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.

ê·œì • ë‚´ìš©:
{context}

ì§ˆë¬¸: {question}

ë‹µë³€ ì‹œ ì£¼ì˜ì‚¬í•­:
1. ê·œì •ì— ëª…ì‹œëœ ë‚´ìš©ë§Œ ë‹µë³€
2. ì¶”ì¸¡ì´ë‚˜ ì¼ë°˜ë¡  ê¸ˆì§€
3. ì¶œì²˜ ë¬¸ì„œëª… ëª…ì‹œ
4. ê·œì •ì— ì—†ëŠ” ê²½ìš° "ê·œì •ì— ëª…ì‹œë˜ì§€ ì•ŠìŒ" í‘œì‹œ

ë‹µë³€:"""
        
        response = self.llm.invoke(prompt)
        return response.content
```

#### 3.2 ì‹¤ì „ í”„ë¡œì íŠ¸ 2: ê¸°ìˆ  ë¬¸ì„œ ê²€ìƒ‰ ì‹œìŠ¤í…œ (1ì‹œê°„)
```python
class TechnicalDocRAG:
    """ê°œë°œíŒ€ì„ ìœ„í•œ ê¸°ìˆ  ë¬¸ì„œ RAG"""
    
    def __init__(self):
        self.setup_specialized_components()
    
    def setup_specialized_components(self):
        # ì½”ë“œ íŠ¹í™” ì²­í‚¹
        self.code_splitter = RecursiveCharacterTextSplitter(
            separators=["\nclass ", "\ndef ", "\n\n", "\n", " ", ""],
            chunk_size=1500,
            chunk_overlap=200,
            length_function=len,
            is_separator_regex=False,
        )
        
        # ê¸°ìˆ  ë¬¸ì„œ íŠ¹í™” ì„ë² ë”©
        self.embeddings = HuggingFaceEmbeddings(
            model_name="sentence-transformers/all-mpnet-base-v2",
            model_kwargs={'device': 'cuda'},
            encode_kwargs={'normalize_embeddings': True}
        )
    
    def process_technical_docs(self):
        """ë‹¤ì–‘í•œ ê¸°ìˆ  ë¬¸ì„œ ì²˜ë¦¬"""
        doc_types = {
            'api_docs': self.process_api_documentation,
            'code_comments': self.process_code_files,
            'readme': self.process_markdown,
            'architecture': self.process_diagrams
        }
        
        for doc_type, processor in doc_types.items():
            documents = processor()
            self.index_documents(documents, doc_type)
    
    def intelligent_search(self, query):
        """ìŠ¤ë§ˆíŠ¸ ê²€ìƒ‰"""
        # ì¿¼ë¦¬ íƒ€ì… ë¶„ì„
        query_type = self.analyze_query_type(query)
        
        if query_type == 'code_search':
            # ì½”ë“œ ê²€ìƒ‰ì— íŠ¹í™”ëœ ì²˜ë¦¬
            return self.search_code_snippets(query)
        elif query_type == 'api_search':
            # API ê²€ìƒ‰ì— íŠ¹í™”ëœ ì²˜ë¦¬
            return self.search_api_endpoints(query)
        else:
            # ì¼ë°˜ ë¬¸ì„œ ê²€ìƒ‰
            return self.search_general_docs(query)
```

#### 3.3 Naive RAGì˜ í•œê³„ ì§ì ‘ ê²½í—˜í•˜ê¸° (1ì‹œê°„)
```python
class NaiveRAGLimitations:
    """Naive RAGì˜ í•œê³„ë¥¼ ì§ì ‘ ì²´í—˜í•˜ëŠ” ì‹¤ìŠµ"""
    
    def demonstrate_limitation_1_retrieval_quality(self):
        """í•œê³„ 1: ê²€ìƒ‰ í’ˆì§ˆ ë¬¸ì œ"""
        # ë¬¸ì œ ìƒí™©: ë¹„ìŠ·í•œ ë‹¨ì–´ëŠ” ë§ì§€ë§Œ ì‹¤ì œ ë‹µì€ ì—†ëŠ” ê²½ìš°
        documents = [
            "Pythonì€ í”„ë¡œê·¸ë˜ë° ì–¸ì–´ì…ë‹ˆë‹¤.",
            "Javaë„ í”„ë¡œê·¸ë˜ë° ì–¸ì–´ì…ë‹ˆë‹¤.",
            "í”„ë¡œê·¸ë˜ë° ì–¸ì–´ëŠ” ì»´í“¨í„°ì™€ ëŒ€í™”í•˜ëŠ” ë„êµ¬ì…ë‹ˆë‹¤.",
            "ë§ì€ í”„ë¡œê·¸ë˜ë° ì–¸ì–´ê°€ ì¡´ì¬í•©ë‹ˆë‹¤."
        ]
        
        query = "Pythonì˜ ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ì€ ì–´ë–»ê²Œ ë™ì‘í•˜ë‚˜ìš”?"
        
        # ê²°ê³¼: ê´€ë ¨ ì—†ëŠ” ë¬¸ì„œë“¤ì´ ê²€ìƒ‰ë¨
        print("âŒ ë¬¸ì œ: í‚¤ì›Œë“œëŠ” ë§¤ì¹­ë˜ì§€ë§Œ ì‹¤ì œ ë‹µì€ ì—†ìŒ")
    
    def demonstrate_limitation_2_context_window(self):
        """í•œê³„ 2: ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš° ì œí•œ"""
        # ë„ˆë¬´ ë§ì€ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•˜ë©´?
        large_context = "ë¬¸ì„œ ë‚´ìš©" * 1000  # ë§¤ìš° ê¸´ ì»¨í…ìŠ¤íŠ¸
        
        print("âŒ ë¬¸ì œ: LLMì˜ í† í° ì œí•œ ì´ˆê³¼")
        print("âŒ ë¬¸ì œ: ì¤‘ìš”í•œ ì •ë³´ê°€ ë’¤ìª½ì— ìˆìœ¼ë©´ ë¬´ì‹œë¨")
    
    def demonstrate_limitation_3_redundancy(self):
        """í•œê³„ 3: ì¤‘ë³µ ì •ë³´ ì²˜ë¦¬"""
        # ê°™ì€ ë‚´ìš©ì´ ì—¬ëŸ¬ ë¬¸ì„œì— ë°˜ë³µ
        redundant_docs = [
            "íšŒì‚¬ ì°½ë¦½ì¼ì€ 2010ë…„ 1ì›” 1ì¼ì…ë‹ˆë‹¤.",
            "ìš°ë¦¬ íšŒì‚¬ëŠ” 2010ë…„ 1ì›” 1ì¼ì— ì°½ë¦½í–ˆìŠµë‹ˆë‹¤.",
            "2010ë…„ 1ì›” 1ì¼, íšŒì‚¬ê°€ ì„¤ë¦½ë˜ì—ˆìŠµë‹ˆë‹¤."
        ]
        
        print("âŒ ë¬¸ì œ: ë™ì¼í•œ ì •ë³´ê°€ ë°˜ë³µë˜ì–´ í† í° ë‚­ë¹„")
    
    def demonstrate_limitation_4_conflicting_info(self):
        """í•œê³„ 4: ìƒì¶©í•˜ëŠ” ì •ë³´ ì²˜ë¦¬"""
        conflicting_docs = [
            "ì œí’ˆ ê°€ê²©ì€ 10ë§Œì›ì…ë‹ˆë‹¤. (2023ë…„ ìë£Œ)",
            "ì œí’ˆ ê°€ê²©ì€ 12ë§Œì›ì…ë‹ˆë‹¤. (2024ë…„ ìë£Œ)",
            "í• ì¸ ì¤‘! ì œí’ˆ ê°€ê²© 8ë§Œì› (í”„ë¡œëª¨ì…˜)"
        ]
        
        print("âŒ ë¬¸ì œ: ì–´ë–¤ ì •ë³´ê°€ ì •í™•í•œì§€ íŒë‹¨ ë¶ˆê°€")
        print("âŒ ë¬¸ì œ: ì‹œê°„ì  ë§¥ë½ ê³ ë ¤ ë¶ˆê°€")
```

### Module 4: ì‹¤ì „ í‰ê°€ì™€ ê°œì„  (1.5ì‹œê°„)

#### 4.1 ì„±ëŠ¥ ì¸¡ì •í•˜ê¸° (45ë¶„)
```python
class RAGEvaluator:
    """RAG ì‹œìŠ¤í…œ ì„±ëŠ¥ í‰ê°€"""
    
    def __init__(self, rag_system):
        self.rag = rag_system
        self.metrics = {}
    
    def evaluate_retrieval_quality(self, test_set):
        """ê²€ìƒ‰ í’ˆì§ˆ í‰ê°€"""
        metrics = {
            'precision_at_k': [],
            'recall_at_k': [],
            'mrr': []  # Mean Reciprocal Rank
        }
        
        for query, relevant_docs in test_set:
            retrieved = self.rag.search(query, top_k=5)
            
            # Precision@K ê³„ì‚°
            relevant_retrieved = len(set(retrieved) & set(relevant_docs))
            precision = relevant_retrieved / len(retrieved)
            metrics['precision_at_k'].append(precision)
            
            # Recall@K ê³„ì‚°
            recall = relevant_retrieved / len(relevant_docs)
            metrics['recall_at_k'].append(recall)
            
            # MRR ê³„ì‚°
            for i, doc in enumerate(retrieved):
                if doc in relevant_docs:
                    metrics['mrr'].append(1 / (i + 1))
                    break
        
        return {
            'avg_precision': np.mean(metrics['precision_at_k']),
            'avg_recall': np.mean(metrics['recall_at_k']),
            'mrr': np.mean(metrics['mrr'])
        }
    
    def evaluate_answer_quality(self, test_qa_pairs):
        """ë‹µë³€ í’ˆì§ˆ í‰ê°€"""
        results = []
        
        for question, expected_answer in test_qa_pairs:
            generated_answer = self.rag.answer(question)
            
            # ë‹¤ì–‘í•œ í‰ê°€ ê¸°ì¤€
            evaluation = {
                'relevance': self.check_relevance(generated_answer, question),
                'accuracy': self.check_accuracy(generated_answer, expected_answer),
                'completeness': self.check_completeness(generated_answer, expected_answer),
                'hallucination': self.detect_hallucination(generated_answer)
            }
            
            results.append(evaluation)
        
        return self.aggregate_results(results)
```

#### 4.2 ê°œì„  ì•„ì´ë””ì–´ ë„ì¶œ (45ë¶„)
```python
class ImprovementIdeas:
    """Naive RAG ê°œì„  ë°©í–¥ íƒìƒ‰"""
    
    def brainstorm_improvements(self):
        improvements = {
            'pre_retrieval': [
                'ë” ë‚˜ì€ ì²­í‚¹ ì „ëµ',
                'ë©”íƒ€ë°ì´í„° í™œìš©',
                'ì¿¼ë¦¬ í™•ì¥',
                'ë¬¸ì„œ í’ˆì§ˆ í•„í„°ë§'
            ],
            'retrieval': [
                'í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (ë²¡í„° + í‚¤ì›Œë“œ)',
                'ì˜ë¯¸ì  ìœ ì‚¬ë„ ê°œì„ ',
                'ë‹¤ë‹¨ê³„ ê²€ìƒ‰',
                'MMR (ìµœëŒ€ í•œê³„ ê´€ë ¨ì„±) ì•Œê³ ë¦¬ì¦˜'
            ],
            'post_retrieval': [
                'ì¬ìˆœìœ„í™” (Reranking)',
                'ì¤‘ë³µ ì œê±°',
                'ìš”ì•½ ë° ì••ì¶•',
                'ì‹ ë¢°ë„ ì ìˆ˜ ì¶”ê°€'
            ],
            'generation': [
                'ë” ë‚˜ì€ í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§',
                'ì¶œì²˜ ëª…ì‹œ',
                'ë‹µë³€ ê²€ì¦',
                'ë°˜ë³µì  ê°œì„ '
            ]
        }
        
        return improvements
```

## ğŸ› ï¸ ì‹¤ìŠµ í”„ë¡œì íŠ¸

### ë©”ì¸ í”„ë¡œì íŠ¸: "ë‚˜ë§Œì˜ ì²« RAG ì‹œìŠ¤í…œ"
- ì£¼ì œ: ê´€ì‹¬ ë¶„ì•¼ì˜ ë¬¸ì„œ 10ê°œë¡œ Q&A ì‹œìŠ¤í…œ êµ¬ì¶•
- ìš”êµ¬ì‚¬í•­:
  - ìµœì†Œ 3ê°€ì§€ íŒŒì¼ í˜•ì‹ ì§€ì› (PDF, TXT, DOCX)
  - 50ê°œ ì´ìƒì˜ ì²­í¬ ìƒì„±
  - ê²€ìƒ‰ ì •í™•ë„ 70% ì´ìƒ ë‹¬ì„±
  - ì†ŒìŠ¤ ì¶”ì  ê¸°ëŠ¥ êµ¬í˜„
  - ì„±ëŠ¥ ë³´ê³ ì„œ ì‘ì„±

### ë„ì „ ê³¼ì œ
1. **ë‹¤êµ­ì–´ RAG**: í•œêµ­ì–´/ì˜ì–´ ë™ì‹œ ì§€ì›
2. **ì‹¤ì‹œê°„ ì—…ë°ì´íŠ¸**: ìƒˆ ë¬¸ì„œ ì¶”ê°€ ì‹œ ì¦‰ì‹œ ë°˜ì˜
3. **ì‹œê°í™”**: ê²€ìƒ‰ ê²°ê³¼ì™€ ì„ë² ë”© ê³µê°„ ì‹œê°í™”

## ğŸ“Š í‰ê°€ ê¸°ì¤€

### í•„ìˆ˜ ë‹¬ì„± ëª©í‘œ
- [ ] LLMì˜ 3ê°€ì§€ í•µì‹¬ í•œê³„ë¥¼ ì½”ë“œë¡œ ì¦ëª…
- [ ] Naive RAG ì „ì²´ íŒŒì´í”„ë¼ì¸ êµ¬í˜„
- [ ] ìµœì†Œ 2ê°œì˜ ì‹¤ì „ í”„ë¡œì íŠ¸ ì™„ì„±
- [ ] 4ê°€ì§€ Naive RAG í•œê³„ ê²½í—˜ ë° ë¬¸ì„œí™”
- [ ] ê²€ìƒ‰ ì •í™•ë„ 70% ì´ìƒ ë‹¬ì„±

### ì¶”ê°€ ì ìˆ˜ í•­ëª©
- [ ] ë…ì°½ì ì¸ RAG í™œìš© ì‚¬ë¡€ ì œì•ˆ
- [ ] ì„±ëŠ¥ ê°œì„  ì•„ì´ë””ì–´ 3ê°œ ì´ìƒ êµ¬í˜„
- [ ] ì˜¤í”ˆì†ŒìŠ¤ í”„ë¡œì íŠ¸ë¡œ ê³µê°œ

## ğŸ¯ í•™ìŠµ ì„±ê³¼
ì´ ê³¼ì •ì„ ì™„ë£Œí•˜ë©´:
- Naive RAGì˜ ëª¨ë“  êµ¬ì„± ìš”ì†Œë¥¼ ì§ì ‘ êµ¬í˜„ ê°€ëŠ¥
- ì‹¤ì œ ë¹„ì¦ˆë‹ˆìŠ¤ ë¬¸ì œì— RAG ì ìš© ëŠ¥ë ¥
- RAG ì‹œìŠ¤í…œì˜ ì„±ëŠ¥ ì¸¡ì • ë° í‰ê°€ ëŠ¥ë ¥
- Advanced RAGë¡œ ë‚˜ì•„ê°ˆ ì¤€ë¹„ ì™„ë£Œ

## ğŸ“š í•„ìˆ˜ ì°¸ê³  ìë£Œ
- [RAG ì›ë…¼ë¬¸ (Lewis et al., 2020)](https://arxiv.org/abs/2005.11401)
- [LangChain RAG íŠœí† ë¦¬ì–¼](https://python.langchain.com/docs/use_cases/question_answering)
- [Chroma DB ê³µì‹ ë¬¸ì„œ](https://docs.trychroma.com/)
- [OpenAI Embeddings ê°€ì´ë“œ](https://platform.openai.com/docs/guides/embeddings)

## â­ï¸ ë‹¤ìŒ ë‹¨ê³„: Advanced RAG
- Pre-retrieval ìµœì í™” ê¸°ë²•
- Post-retrieval ì²˜ë¦¬ ì „ëµ
- í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì‹œìŠ¤í…œ
- ì„±ëŠ¥ ìµœì í™” ê³ ê¸‰ ê¸°ë²•

## ğŸ’¡ í•µì‹¬ ë©”ì‹œì§€
"Naive RAGëŠ” ì‹œì‘ì¼ ë¿ì…ë‹ˆë‹¤. í•˜ì§€ë§Œ ì´ ê¸°ì´ˆê°€ íƒ„íƒ„í•´ì•¼ ê³ ê¸‰ ê¸°ë²•ë„ ì œëŒ€ë¡œ êµ¬í˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ëª¨ë“  í•œê³„ë¥¼ ì§ì ‘ ê²½í—˜í•˜ê³ , ì™œ Advanced RAGê°€ í•„ìš”í•œì§€ ì²´ê°í•˜ì„¸ìš”."