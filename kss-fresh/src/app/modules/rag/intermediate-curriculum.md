# RAG ì¤‘ê¸‰ ì»¤ë¦¬í˜ëŸ¼ (Step 2: Advanced RAG ë§ˆìŠ¤í„°)

## ğŸ¯ í•™ìŠµ ëª©í‘œ
Naive RAGì˜ í•œê³„ë¥¼ ê·¹ë³µí•˜ëŠ” Advanced RAG ê¸°ë²•ë“¤ì„ ì™„ë²½íˆ ë§ˆìŠ¤í„°í•©ë‹ˆë‹¤. Pre-retrievalê³¼ Post-retrieval ìµœì í™”ë¥¼ í†µí•´ ê²€ìƒ‰ í’ˆì§ˆì„ íšê¸°ì ìœ¼ë¡œ í–¥ìƒì‹œí‚¤ê³ , ì‹¤ì œ í”„ë¡œë•ì…˜ í™˜ê²½ì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ ê³ ì„±ëŠ¥ RAG ì‹œìŠ¤í…œì„ êµ¬ì¶•í•©ë‹ˆë‹¤.

## ğŸ“š ì´ í•™ìŠµ ì‹œê°„: 15ì‹œê°„

## ğŸ† ì°¨ë³„í™”ëœ êµìœ¡ ì² í•™
- **"ì‹¤íŒ¨ë¥¼ í†µí•œ í˜ì‹ "**: Naive RAGì˜ ê° í•œê³„ì ì— ëŒ€í•œ êµ¬ì²´ì  í•´ê²°ì±… êµ¬í˜„
- **"ì¸¡ì •í•˜ì§€ ì•Šìœ¼ë©´ ê°œì„ í•  ìˆ˜ ì—†ë‹¤"**: ëª¨ë“  ìµœì í™” ê¸°ë²•ì˜ ì„±ëŠ¥ í–¥ìƒ ìˆ˜ì¹˜í™”
- **"ì‹¤ë¬´ ì¤‘ì‹¬ ì„¤ê³„"**: ì‹¤ì œ ê¸°ì—…ì—ì„œ ë‹¹ì¥ ì‚¬ìš© ê°€ëŠ¥í•œ ì‹œìŠ¤í…œ êµ¬ì¶•
- **"ì ì§„ì  ê°œì„ "**: ì‘ì€ ê°œì„ ë“¤ì´ ëª¨ì—¬ í° ì„±ëŠ¥ í–¥ìƒ ë‹¬ì„±

## ğŸ“‹ ì»¤ë¦¬í˜ëŸ¼ êµ¬ì„±

### Module 1: Advanced RAG ì•„í‚¤í…ì²˜ ì´í•´ (3ì‹œê°„)

#### 1.1 Naive RAGì—ì„œ Advanced RAGë¡œì˜ ì§„í™” (1ì‹œê°„)
```python
class RAGEvolution:
    """RAG ì§„í™” ê³¼ì • ì‹¤ìŠµ"""
    
    def compare_architectures(self):
        naive_rag = {
            'pipeline': 'Indexing â†’ Retrieval â†’ Generation',
            'limitations': [
                'ë‚®ì€ ê²€ìƒ‰ ì •í™•ë„',
                'ì¤‘ë³µ ì •ë³´ ì²˜ë¦¬ ë¯¸í¡',
                'ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ ë¹„íš¨ìœ¨',
                'ì‹œê°„ì /ê³µê°„ì  ë§¥ë½ ë¶€ì¬'
            ],
            'performance': {
                'precision': 0.65,
                'recall': 0.58,
                'latency': 250  # ms
            }
        }
        
        advanced_rag = {
            'pipeline': '''
                Pre-Retrieval ìµœì í™”
                    â†“
                Indexing â†’ Enhanced Retrieval â†’ Post-Retrieval ì²˜ë¦¬
                    â†“
                Optimized Generation
            ''',
            'improvements': [
                'ì¿¼ë¦¬ ê°œì„  ë° í™•ì¥',
                'í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰',
                'ì¬ìˆœìœ„í™” (Reranking)',
                'ì¤‘ë³µ ì œê±° ë° ì••ì¶•'
            ],
            'performance': {
                'precision': 0.88,  # +35% í–¥ìƒ
                'recall': 0.82,     # +41% í–¥ìƒ
                'latency': 180      # -28% ê°ì†Œ
            }
        }
        
        return naive_rag, advanced_rag
```

#### 1.2 Pre-Retrieval ìµœì í™” ì „ëµ (1ì‹œê°„)
```python
class PreRetrievalOptimization:
    """ê²€ìƒ‰ ì „ ìµœì í™” ê¸°ë²•ë“¤"""
    
    def __init__(self):
        self.query_enhancer = QueryEnhancer()
        self.data_optimizer = DataOptimizer()
    
    def optimize_query(self, original_query):
        """ì¿¼ë¦¬ ìµœì í™” íŒŒì´í”„ë¼ì¸"""
        
        # 1. ì¿¼ë¦¬ ì •ì œ (Query Refinement)
        refined_query = self.clean_and_normalize(original_query)
        
        # 2. ì¿¼ë¦¬ í™•ì¥ (Query Expansion)
        expanded_query = self.expand_with_synonyms(refined_query)
        
        # 3. ì¿¼ë¦¬ ë¶„í•´ (Query Decomposition)
        sub_queries = self.decompose_complex_query(expanded_query)
        
        # 4. ì˜ë„ ë¶„ì„ (Intent Analysis)
        query_intent = self.analyze_intent(original_query)
        
        return {
            'original': original_query,
            'refined': refined_query,
            'expanded': expanded_query,
            'sub_queries': sub_queries,
            'intent': query_intent
        }
    
    def optimize_data_indexing(self, documents):
        """ë°ì´í„° ì¸ë±ì‹± ìµœì í™”"""
        
        optimized_docs = []
        
        for doc in documents:
            # 1. ë¬¸ì„œ í’ˆì§ˆ í‰ê°€
            quality_score = self.assess_document_quality(doc)
            
            if quality_score < 0.5:
                # í’ˆì§ˆì´ ë‚®ì€ ë¬¸ì„œ ì œì™¸ ë˜ëŠ” ê°œì„ 
                doc = self.improve_document_quality(doc)
            
            # 2. ë©”íƒ€ë°ì´í„° ê°•í™”
            enriched_doc = self.enrich_metadata(doc)
            
            # 3. êµ¬ì¡°í™”ëœ ì²­í‚¹
            chunks = self.smart_chunking(enriched_doc)
            
            # 4. ê³„ì¸µì  ì¸ë±ì‹±
            hierarchical_chunks = self.create_hierarchical_index(chunks)
            
            optimized_docs.extend(hierarchical_chunks)
        
        return optimized_docs
```

#### 1.3 Post-Retrieval ì²˜ë¦¬ ê¸°ë²• (1ì‹œê°„)
```python
class PostRetrievalProcessing:
    """ê²€ìƒ‰ í›„ ì²˜ë¦¬ ìµœì í™”"""
    
    def __init__(self):
        self.reranker = CrossEncoderReranker()
        self.compressor = ContextCompressor()
        self.deduplicator = SemanticDeduplicator()
    
    def process_retrieved_documents(self, query, documents):
        """ê²€ìƒ‰ëœ ë¬¸ì„œ í›„ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸"""
        
        # 1. ì¬ìˆœìœ„í™” (Reranking)
        reranked_docs = self.rerank_documents(query, documents)
        
        # 2. ì¤‘ë³µ ì œê±° (Deduplication)
        unique_docs = self.remove_duplicates(reranked_docs)
        
        # 3. ì»¨í…ìŠ¤íŠ¸ ì••ì¶• (Context Compression)
        compressed_docs = self.compress_context(unique_docs, query)
        
        # 4. ê´€ë ¨ì„± í•„í„°ë§ (Relevance Filtering)
        filtered_docs = self.filter_by_relevance(compressed_docs, threshold=0.7)
        
        # 5. ë¬¸ì„œ ìˆœì„œ ìµœì í™” (Order Optimization)
        optimized_order = self.optimize_document_order(filtered_docs)
        
        return optimized_order
    
    def rerank_documents(self, query, documents):
        """Cross-encoderë¥¼ ì‚¬ìš©í•œ ì¬ìˆœìœ„í™”"""
        
        # ì¿¼ë¦¬-ë¬¸ì„œ ìŒ ìƒì„±
        pairs = [(query, doc.content) for doc in documents]
        
        # Cross-encoder ì ìˆ˜ ê³„ì‚°
        scores = self.reranker.predict(pairs)
        
        # ì ìˆ˜ì— ë”°ë¼ ì¬ì •ë ¬
        reranked = sorted(
            zip(documents, scores),
            key=lambda x: x[1],
            reverse=True
        )
        
        return [doc for doc, score in reranked]
```

### Module 2: ê³ ê¸‰ ê²€ìƒ‰ ê¸°ë²• êµ¬í˜„ (4ì‹œê°„)

#### 2.1 í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì‹œìŠ¤í…œ (1.5ì‹œê°„)
```python
class HybridSearchSystem:
    """ë²¡í„° ê²€ìƒ‰ + í‚¤ì›Œë“œ ê²€ìƒ‰ ìœµí•©"""
    
    def __init__(self):
        self.vector_searcher = VectorSearch()
        self.keyword_searcher = BM25Search()
        self.elastic_searcher = ElasticSearch()
    
    def hybrid_search(self, query, alpha=0.5):
        """ë‹¤ì¤‘ ê²€ìƒ‰ ë°©ë²• ìœµí•©"""
        
        # 1. ë²¡í„° ê¸°ë°˜ ì˜ë¯¸ ê²€ìƒ‰
        vector_results = self.vector_searcher.search(
            query,
            top_k=20,
            similarity_threshold=0.7
        )
        
        # 2. BM25 í‚¤ì›Œë“œ ê²€ìƒ‰
        keyword_results = self.keyword_searcher.search(
            query,
            top_k=20,
            boost_exact_match=True
        )
        
        # 3. Elasticsearch í’€í…ìŠ¤íŠ¸ ê²€ìƒ‰
        elastic_results = self.elastic_searcher.search(
            query,
            top_k=20,
            fuzzy_matching=True
        )
        
        # 4. ê²°ê³¼ ìœµí•© (Reciprocal Rank Fusion)
        fused_results = self.reciprocal_rank_fusion([
            vector_results,
            keyword_results,
            elastic_results
        ], weights=[0.5, 0.3, 0.2])
        
        return fused_results
    
    def reciprocal_rank_fusion(self, result_lists, weights=None, k=60):
        """RRF ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ ê²°ê³¼ ìœµí•©"""
        
        if weights is None:
            weights = [1.0] * len(result_lists)
        
        # ë¬¸ì„œë³„ ì ìˆ˜ ê³„ì‚°
        doc_scores = {}
        
        for weight, results in zip(weights, result_lists):
            for rank, doc in enumerate(results):
                doc_id = doc.id
                
                # RRF ì ìˆ˜: weight / (k + rank)
                score = weight / (k + rank + 1)
                
                if doc_id in doc_scores:
                    doc_scores[doc_id] += score
                else:
                    doc_scores[doc_id] = score
        
        # ì ìˆ˜ ê¸°ì¤€ ì •ë ¬
        sorted_docs = sorted(
            doc_scores.items(),
            key=lambda x: x[1],
            reverse=True
        )
        
        return sorted_docs
```

#### 2.2 Query Understanding & Expansion (1.5ì‹œê°„)
```python
class QueryUnderstandingPipeline:
    """ì¿¼ë¦¬ ì´í•´ ë° í™•ì¥ íŒŒì´í”„ë¼ì¸"""
    
    def __init__(self):
        self.nlp = spacy.load('ko_core_news_lg')
        self.word2vec = Word2VecModel()
        self.llm = ChatGPT()
    
    def understand_query(self, query):
        """ì¿¼ë¦¬ ì‹¬ì¸µ ë¶„ì„"""
        
        # 1. ì˜ë„ ë¶„ë¥˜ (Intent Classification)
        intent = self.classify_intent(query)
        
        # 2. ê°œì²´ëª… ì¸ì‹ (NER)
        entities = self.extract_entities(query)
        
        # 3. ì‹œê°„ì  ë§¥ë½ íŒŒì•…
        temporal_context = self.extract_temporal_context(query)
        
        # 4. ë„ë©”ì¸ íŠ¹í™” ìš©ì–´ ì‹ë³„
        domain_terms = self.identify_domain_terms(query)
        
        return {
            'intent': intent,
            'entities': entities,
            'temporal': temporal_context,
            'domain_terms': domain_terms
        }
    
    def expand_query_intelligently(self, query, understanding):
        """ì§€ëŠ¥ì  ì¿¼ë¦¬ í™•ì¥"""
        
        expanded_queries = []
        
        # 1. ë™ì˜ì–´ í™•ì¥
        synonyms = self.get_contextual_synonyms(
            query,
            understanding['domain_terms']
        )
        
        # 2. í•˜ì´í¼ë‹˜/í•˜ì´í¬ë‹˜ í™•ì¥
        hypernyms = self.get_hypernyms(understanding['entities'])
        hyponyms = self.get_hyponyms(understanding['entities'])
        
        # 3. ê´€ë ¨ ê°œë… í™•ì¥
        related_concepts = self.get_related_concepts(query)
        
        # 4. LLM ê¸°ë°˜ ì¿¼ë¦¬ ë³€í˜•
        llm_variations = self.generate_query_variations(query)
        
        # 5. ì‹œê°„ì  ë³€í˜• (ê³¼ê±°/í˜„ì¬/ë¯¸ë˜)
        temporal_variations = self.create_temporal_variations(
            query,
            understanding['temporal']
        )
        
        # ëª¨ë“  í™•ì¥ í†µí•©
        all_expansions = {
            'synonyms': synonyms,
            'hypernyms': hypernyms,
            'hyponyms': hyponyms,
            'related': related_concepts,
            'llm_variations': llm_variations,
            'temporal': temporal_variations
        }
        
        return self.rank_expansions(all_expansions, query)
    
    def generate_query_variations(self, query):
        """LLMì„ í™œìš©í•œ ì¿¼ë¦¬ ë³€í˜• ìƒì„±"""
        
        prompt = f"""
        ë‹¤ìŒ ê²€ìƒ‰ ì¿¼ë¦¬ì˜ ë‹¤ì–‘í•œ ë³€í˜•ì„ ìƒì„±í•˜ì„¸ìš”.
        ì›ë³¸ ì¿¼ë¦¬ì˜ ì˜ë„ëŠ” ìœ ì§€í•˜ë©´ì„œ ë‹¤ë¥¸ í‘œí˜„ìœ¼ë¡œ ë°”ê¿”ì£¼ì„¸ìš”.
        
        ì›ë³¸ ì¿¼ë¦¬: {query}
        
        ë³€í˜• ê·œì¹™:
        1. ë™ì¼í•œ ì˜ë¯¸ë¥¼ ë‹¤ë¥¸ ë°©ì‹ìœ¼ë¡œ í‘œí˜„
        2. ë” êµ¬ì²´ì ì¸ ë²„ì „
        3. ë” ì¼ë°˜ì ì¸ ë²„ì „
        4. ê´€ë ¨ í•˜ìœ„ ì§ˆë¬¸ë“¤
        
        ë³€í˜•ëœ ì¿¼ë¦¬ë“¤:
        """
        
        variations = self.llm.generate(prompt)
        return self.parse_variations(variations)
```

#### 2.3 Smart Chunking Strategies (1ì‹œê°„)
```python
class SmartChunkingSystem:
    """ì§€ëŠ¥ì  ì²­í‚¹ ì „ëµ"""
    
    def __init__(self):
        self.sentence_splitter = SentenceSplitter()
        self.semantic_analyzer = SemanticAnalyzer()
        self.layout_parser = LayoutParser()
    
    def adaptive_chunking(self, document):
        """ë¬¸ì„œ ìœ í˜•ë³„ ì ì‘ì  ì²­í‚¹"""
        
        # 1. ë¬¸ì„œ ìœ í˜• ë¶„ì„
        doc_type = self.analyze_document_type(document)
        
        # 2. ë ˆì´ì•„ì›ƒ ë¶„ì„ (ì„¹ì…˜, ë‹¨ë½, í‘œ ë“±)
        layout = self.layout_parser.parse(document)
        
        # 3. ìœ í˜•ë³„ ì²­í‚¹ ì „ëµ ì„ íƒ
        if doc_type == 'technical':
            chunks = self.chunk_technical_document(document, layout)
        elif doc_type == 'narrative':
            chunks = self.chunk_narrative_document(document, layout)
        elif doc_type == 'structured':
            chunks = self.chunk_structured_document(document, layout)
        else:
            chunks = self.chunk_generic_document(document)
        
        # 4. ì²­í¬ í’ˆì§ˆ ê²€ì¦ ë° ì¡°ì •
        validated_chunks = self.validate_and_adjust_chunks(chunks)
        
        return validated_chunks
    
    def semantic_chunking(self, text, max_chunk_size=512):
        """ì˜ë¯¸ì  ì¼ê´€ì„± ê¸°ë°˜ ì²­í‚¹"""
        
        # ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„í• 
        sentences = self.sentence_splitter.split(text)
        
        # ê° ë¬¸ì¥ì˜ ì„ë² ë”© ìƒì„±
        embeddings = [self.get_embedding(s) for s in sentences]
        
        chunks = []
        current_chunk = []
        current_size = 0
        
        for i, sentence in enumerate(sentences):
            # í˜„ì¬ ì²­í¬ì— ì¶”ê°€í• ì§€ ê²°ì •
            if current_chunk:
                # ì˜ë¯¸ì  ìœ ì‚¬ë„ ê³„ì‚°
                similarity = self.calculate_similarity(
                    embeddings[i],
                    np.mean([embeddings[j] for j in range(
                        i - len(current_chunk), i
                    )], axis=0)
                )
                
                # ìœ ì‚¬ë„ê°€ ì„ê³„ê°’ ì´ìƒì´ê³  í¬ê¸° ì œí•œ ë‚´ë¼ë©´ ì¶”ê°€
                if similarity > 0.7 and current_size + len(sentence) <= max_chunk_size:
                    current_chunk.append(sentence)
                    current_size += len(sentence)
                else:
                    # ìƒˆë¡œìš´ ì²­í¬ ì‹œì‘
                    chunks.append(' '.join(current_chunk))
                    current_chunk = [sentence]
                    current_size = len(sentence)
            else:
                current_chunk = [sentence]
                current_size = len(sentence)
        
        # ë§ˆì§€ë§‰ ì²­í¬ ì¶”ê°€
        if current_chunk:
            chunks.append(' '.join(current_chunk))
        
        return chunks
    
    def hierarchical_chunking(self, document):
        """ê³„ì¸µì  ì²­í‚¹ - ë¬¸ì„œ, ì„¹ì…˜, ë‹¨ë½ ë ˆë²¨"""
        
        hierarchy = {
            'document': {
                'content': document.content,
                'metadata': document.metadata,
                'sections': []
            }
        }
        
        # ì„¹ì…˜ ë ˆë²¨ ì²­í‚¹
        sections = self.extract_sections(document)
        
        for section in sections:
            section_data = {
                'title': section.title,
                'content': section.content,
                'paragraphs': []
            }
            
            # ë‹¨ë½ ë ˆë²¨ ì²­í‚¹
            paragraphs = self.extract_paragraphs(section)
            
            for para in paragraphs:
                para_data = {
                    'content': para.content,
                    'sentences': self.extract_sentences(para)
                }
                section_data['paragraphs'].append(para_data)
            
            hierarchy['document']['sections'].append(section_data)
        
        # ê³„ì¸µì  ì¸ë±ìŠ¤ ìƒì„±
        return self.create_hierarchical_index(hierarchy)
```

### Module 3: ì¬ìˆœìœ„í™”ì™€ í•„í„°ë§ (4ì‹œê°„)

#### 3.1 Cross-Encoder Reranking (1.5ì‹œê°„)
```python
class CrossEncoderReranking:
    """Cross-Encoderë¥¼ í™œìš©í•œ ì •êµí•œ ì¬ìˆœìœ„í™”"""
    
    def __init__(self):
        # ì—¬ëŸ¬ Cross-Encoder ëª¨ë¸ ì•™ìƒë¸”
        self.models = {
            'general': CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2'),
            'multilingual': CrossEncoder('cross-encoder/mmarco-mMiniLMv2-L12-H384-v1'),
            'domain_specific': self.load_domain_model()
        }
    
    def rerank_with_ensemble(self, query, documents):
        """ì•™ìƒë¸” ì¬ìˆœìœ„í™”"""
        
        all_scores = {}
        
        # ê° ëª¨ë¸ë¡œ ì ìˆ˜ ê³„ì‚°
        for model_name, model in self.models.items():
            pairs = [[query, doc.content] for doc in documents]
            scores = model.predict(pairs)
            
            # ì ìˆ˜ ì •ê·œí™”
            normalized_scores = self.normalize_scores(scores)
            all_scores[model_name] = normalized_scores
        
        # ê°€ì¤‘ í‰ê· ìœ¼ë¡œ ìµœì¢… ì ìˆ˜ ê³„ì‚°
        final_scores = self.weighted_average_scores(
            all_scores,
            weights={'general': 0.5, 'multilingual': 0.3, 'domain_specific': 0.2}
        )
        
        # ì¬ìˆœìœ„í™”
        reranked_docs = [
            doc for _, doc in sorted(
                zip(final_scores, documents),
                key=lambda x: x[0],
                reverse=True
            )
        ]
        
        return reranked_docs
    
    def adaptive_reranking(self, query, documents, context=None):
        """ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ì ì‘ì  ì¬ìˆœìœ„í™”"""
        
        # ì¿¼ë¦¬ íŠ¹ì„± ë¶„ì„
        query_features = self.analyze_query_features(query)
        
        # ì ì ˆí•œ ì¬ìˆœìœ„í™” ì „ëµ ì„ íƒ
        if query_features['is_factual']:
            return self.factual_reranking(query, documents)
        elif query_features['is_comparative']:
            return self.comparative_reranking(query, documents)
        elif query_features['requires_reasoning']:
            return self.reasoning_based_reranking(query, documents)
        else:
            return self.default_reranking(query, documents)
    
    def diversity_aware_reranking(self, query, documents, lambda_param=0.5):
        """ë‹¤ì–‘ì„±ì„ ê³ ë ¤í•œ ì¬ìˆœìœ„í™” (MMR)"""
        
        selected = []
        remaining = documents.copy()
        
        # ì²« ë²ˆì§¸ ë¬¸ì„œëŠ” ê°€ì¥ ê´€ë ¨ì„± ë†’ì€ ê²ƒ ì„ íƒ
        scores = self.calculate_relevance_scores(query, remaining)
        best_idx = np.argmax(scores)
        selected.append(remaining.pop(best_idx))
        
        # MMR ê¸°ë°˜ ì„ íƒ
        while remaining and len(selected) < len(documents):
            mmr_scores = []
            
            for doc in remaining:
                # ì¿¼ë¦¬ì™€ì˜ ê´€ë ¨ì„±
                relevance = self.calculate_relevance(query, doc)
                
                # ê¸°ì„ íƒ ë¬¸ì„œì™€ì˜ ìµœëŒ€ ìœ ì‚¬ë„
                max_similarity = max([
                    self.calculate_similarity(doc, selected_doc)
                    for selected_doc in selected
                ])
                
                # MMR ì ìˆ˜ ê³„ì‚°
                mmr = lambda_param * relevance - (1 - lambda_param) * max_similarity
                mmr_scores.append(mmr)
            
            # ìµœê³  MMR ì ìˆ˜ ë¬¸ì„œ ì„ íƒ
            best_idx = np.argmax(mmr_scores)
            selected.append(remaining.pop(best_idx))
        
        return selected
```

#### 3.2 Context Compression (1.5ì‹œê°„)
```python
class ContextCompressionSystem:
    """ì»¨í…ìŠ¤íŠ¸ ì••ì¶• ë° ìµœì í™”"""
    
    def __init__(self):
        self.summarizer = BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn')
        self.extractor = KeyInfoExtractor()
        self.compressor = LLMCompressor()
    
    def compress_context(self, documents, query, target_length=2048):
        """ì¿¼ë¦¬ ê´€ë ¨ ì»¨í…ìŠ¤íŠ¸ ì••ì¶•"""
        
        compressed_docs = []
        
        for doc in documents:
            # 1. ê´€ë ¨ì„± ê¸°ë°˜ ë¬¸ì¥ ì¶”ì¶œ
            relevant_sentences = self.extract_query_relevant_sentences(
                doc.content,
                query,
                min_relevance=0.6
            )
            
            # 2. ì¤‘ìš” ì •ë³´ ì¶”ì¶œ
            key_info = self.extractor.extract(
                relevant_sentences,
                focus_on=query
            )
            
            # 3. ì••ì¶• ìˆ˜í–‰
            if len(key_info) > target_length // len(documents):
                compressed = self.intelligent_compression(
                    key_info,
                    query,
                    max_length=target_length // len(documents)
                )
            else:
                compressed = key_info
            
            compressed_docs.append({
                'content': compressed,
                'source': doc.metadata.get('source', 'unknown'),
                'relevance_score': doc.score
            })
        
        return compressed_docs
    
    def intelligent_compression(self, text, query, max_length):
        """ì§€ëŠ¥ì  ì••ì¶• - í•µì‹¬ ì •ë³´ ë³´ì¡´"""
        
        # LLMì„ ì‚¬ìš©í•œ ì••ì¶•
        prompt = f"""
        ë‹¤ìŒ í…ìŠ¤íŠ¸ë¥¼ {max_length}ì ì´ë‚´ë¡œ ì••ì¶•í•˜ì„¸ìš”.
        ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ í•µì‹¬ ì •ë³´ëŠ” ë°˜ë“œì‹œ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤.
        
        ì§ˆë¬¸: {query}
        
        ì›ë³¸ í…ìŠ¤íŠ¸:
        {text}
        
        ì••ì¶• ê·œì¹™:
        1. ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì— í•„ìš”í•œ ì •ë³´ ìš°ì„ 
        2. êµ¬ì²´ì  ì‚¬ì‹¤, ìˆ«ì, ë‚ ì§œ ë³´ì¡´
        3. ì¤‘ë³µ ì •ë³´ ì œê±°
        4. ë¬¸ì¥ì˜ ìì—°ìŠ¤ëŸ¬ì›€ ìœ ì§€
        
        ì••ì¶•ëœ í…ìŠ¤íŠ¸:
        """
        
        compressed = self.compressor.compress(prompt)
        
        # ê¸¸ì´ í™•ì¸ ë° ì¡°ì •
        if len(compressed) > max_length:
            compressed = self.hard_truncate(compressed, max_length)
        
        return compressed
    
    def adaptive_compression_by_position(self, documents, query):
        """ìœ„ì¹˜ë³„ ì ì‘ì  ì••ì¶•"""
        
        compressed = []
        
        for i, doc in enumerate(documents):
            if i < 3:
                # ìƒìœ„ ë¬¸ì„œëŠ” ëœ ì••ì¶•
                compression_ratio = 0.8
            elif i < 6:
                # ì¤‘ê°„ ë¬¸ì„œëŠ” ë³´í†µ ì••ì¶•
                compression_ratio = 0.5
            else:
                # í•˜ìœ„ ë¬¸ì„œëŠ” ê°•í•˜ê²Œ ì••ì¶•
                compression_ratio = 0.3
            
            target_length = int(len(doc.content) * compression_ratio)
            compressed_doc = self.compress_context(
                [doc],
                query,
                target_length
            )[0]
            
            compressed.append(compressed_doc)
        
        return compressed
```

#### 3.3 Deduplication & Filtering (1ì‹œê°„)
```python
class DeduplicationSystem:
    """ì¤‘ë³µ ì œê±° ë° í•„í„°ë§ ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.similarity_threshold = 0.85
        self.hasher = MinHashLSH(threshold=0.8, num_perm=128)
    
    def remove_duplicates(self, documents):
        """ë‹¤ì¸µì  ì¤‘ë³µ ì œê±°"""
        
        # 1. í•´ì‹œ ê¸°ë°˜ ë¹ ë¥¸ ì¤‘ë³µ ê²€ì‚¬
        unique_docs = self.hash_based_dedup(documents)
        
        # 2. ì˜ë¯¸ì  ì¤‘ë³µ ì œê±°
        unique_docs = self.semantic_dedup(unique_docs)
        
        # 3. ë¶€ë¶„ ì¤‘ë³µ ì²˜ë¦¬
        unique_docs = self.partial_overlap_dedup(unique_docs)
        
        return unique_docs
    
    def semantic_dedup(self, documents):
        """ì˜ë¯¸ì  ìœ ì‚¬ë„ ê¸°ë°˜ ì¤‘ë³µ ì œê±°"""
        
        # ëª¨ë“  ë¬¸ì„œì˜ ì„ë² ë”© ê³„ì‚°
        embeddings = [self.get_embedding(doc.content) for doc in documents]
        
        # ìœ ì‚¬ë„ ë§¤íŠ¸ë¦­ìŠ¤ ê³„ì‚°
        similarity_matrix = cosine_similarity(embeddings)
        
        # ì¤‘ë³µ í´ëŸ¬ìŠ¤í„° ì°¾ê¸°
        clusters = []
        visited = set()
        
        for i in range(len(documents)):
            if i in visited:
                continue
            
            cluster = [i]
            visited.add(i)
            
            for j in range(i + 1, len(documents)):
                if j not in visited and similarity_matrix[i][j] > self.similarity_threshold:
                    cluster.append(j)
                    visited.add(j)
            
            clusters.append(cluster)
        
        # ê° í´ëŸ¬ìŠ¤í„°ì—ì„œ ëŒ€í‘œ ë¬¸ì„œ ì„ íƒ
        unique_docs = []
        for cluster in clusters:
            # ê°€ì¥ ê¸´ ë¬¸ì„œë¥¼ ëŒ€í‘œë¡œ ì„ íƒ (ë” ë§ì€ ì •ë³´ í¬í•¨ ê°€ëŠ¥ì„±)
            representative_idx = max(cluster, key=lambda x: len(documents[x].content))
            
            # ë‹¤ë¥¸ ë¬¸ì„œë“¤ì˜ ìœ ìš©í•œ ì •ë³´ ë³‘í•©
            merged_doc = self.merge_cluster_information(
                documents,
                cluster,
                representative_idx
            )
            
            unique_docs.append(merged_doc)
        
        return unique_docs
    
    def intelligent_filtering(self, documents, query, context=None):
        """ì§€ëŠ¥ì  ë¬¸ì„œ í•„í„°ë§"""
        
        filtered = []
        
        for doc in documents:
            # 1. ê´€ë ¨ì„± ì ìˆ˜ í™•ì¸
            if doc.relevance_score < 0.5:
                continue
            
            # 2. ì •ë³´ í’ˆì§ˆ í™•ì¸
            quality_score = self.assess_information_quality(doc)
            if quality_score < 0.6:
                continue
            
            # 3. ì‹œê°„ì  ê´€ë ¨ì„± í™•ì¸
            if not self.check_temporal_relevance(doc, query):
                continue
            
            # 4. ì‹ ë¢°ë„ í™•ì¸
            if not self.verify_credibility(doc):
                continue
            
            filtered.append(doc)
        
        return filtered
```

### Module 4: ì‹¤ì „ Advanced RAG êµ¬í˜„ (4ì‹œê°„)

#### 4.1 Enterprise-Grade RAG System (2ì‹œê°„)
```python
class EnterpriseRAG:
    """ê¸°ì—…ìš© Advanced RAG ì‹œìŠ¤í…œ"""
    
    def __init__(self, config):
        self.config = config
        self.setup_components()
    
    def setup_components(self):
        """ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”"""
        
        # Pre-retrieval ì»´í¬ë„ŒíŠ¸
        self.query_processor = QueryProcessor(
            expand=True,
            correct_spelling=True,
            detect_language=True
        )
        
        # Retrieval ì»´í¬ë„ŒíŠ¸
        self.hybrid_searcher = HybridSearch(
            vector_weight=0.6,
            keyword_weight=0.4
        )
        
        # Post-retrieval ì»´í¬ë„ŒíŠ¸
        self.reranker = CrossEncoderReranker(
            model='cross-encoder/ms-marco-MiniLM-L-12-v2'
        )
        self.compressor = ContextCompressor(
            method='extractive',
            preserve_key_info=True
        )
        
        # Generation ì»´í¬ë„ŒíŠ¸
        self.generator = RAGGenerator(
            model='gpt-4',
            temperature=0.3,
            include_citations=True
        )
    
    def process_query(self, query, user_context=None):
        """ì „ì²´ RAG íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
        
        try:
            # 1. Pre-retrieval ì²˜ë¦¬
            processed_query = self.pre_retrieval_pipeline(query, user_context)
            
            # 2. í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰
            search_results = self.retrieval_pipeline(processed_query)
            
            # 3. Post-retrieval ì²˜ë¦¬
            optimized_context = self.post_retrieval_pipeline(
                processed_query,
                search_results
            )
            
            # 4. ë‹µë³€ ìƒì„±
            response = self.generation_pipeline(
                processed_query,
                optimized_context
            )
            
            # 5. í’ˆì§ˆ ê²€ì¦
            validated_response = self.validate_response(
                query,
                response,
                optimized_context
            )
            
            return validated_response
            
        except Exception as e:
            return self.handle_error(e, query)
    
    def pre_retrieval_pipeline(self, query, user_context):
        """Pre-retrieval ìµœì í™” íŒŒì´í”„ë¼ì¸"""
        
        # ì¿¼ë¦¬ ë¶„ì„
        analysis = self.query_processor.analyze(query)
        
        # ë§ì¶¤ë²• êµì •
        corrected_query = self.query_processor.correct_spelling(query)
        
        # ì¿¼ë¦¬ í™•ì¥
        expanded_queries = self.query_processor.expand_query(
            corrected_query,
            method='all',  # synonym, hypernym, llm
            user_context=user_context
        )
        
        # ì˜ë„ ê¸°ë°˜ ìµœì í™”
        optimized_queries = self.optimize_by_intent(
            expanded_queries,
            analysis['intent']
        )
        
        return {
            'original': query,
            'corrected': corrected_query,
            'expanded': expanded_queries,
            'optimized': optimized_queries,
            'analysis': analysis
        }
    
    def post_retrieval_pipeline(self, query_data, search_results):
        """Post-retrieval ìµœì í™” íŒŒì´í”„ë¼ì¸"""
        
        # 1. ì¬ìˆœìœ„í™”
        reranked = self.reranker.rerank(
            query_data['original'],
            search_results,
            diversity_weight=0.3
        )
        
        # 2. ì¤‘ë³µ ì œê±°
        deduplicated = self.deduplicator.remove_duplicates(
            reranked,
            similarity_threshold=0.85
        )
        
        # 3. ì»¨í…ìŠ¤íŠ¸ ì••ì¶•
        compressed = self.compressor.compress(
            deduplicated,
            query_data['original'],
            target_tokens=2000
        )
        
        # 4. ë©”íƒ€ë°ì´í„° ê°•í™”
        enriched = self.enrich_with_metadata(compressed)
        
        # 5. ìµœì¢… í•„í„°ë§
        final_context = self.final_filtering(
            enriched,
            query_data['analysis']
        )
        
        return final_context
```

#### 4.2 Performance Optimization (2ì‹œê°„)
```python
class PerformanceOptimizer:
    """Advanced RAG ì„±ëŠ¥ ìµœì í™”"""
    
    def __init__(self):
        self.cache = RedisCache()
        self.monitor = PerformanceMonitor()
    
    def optimize_retrieval_speed(self):
        """ê²€ìƒ‰ ì†ë„ ìµœì í™” ê¸°ë²•"""
        
        optimizations = {
            # 1. ìºì‹± ì „ëµ
            'query_cache': self.implement_smart_caching(),
            
            # 2. ì¸ë±ìŠ¤ ìµœì í™”
            'index_optimization': self.optimize_vector_indices(),
            
            # 3. ë³‘ë ¬ ì²˜ë¦¬
            'parallel_search': self.setup_parallel_search(),
            
            # 4. ì‚¬ì „ ê³„ì‚°
            'precomputation': self.precompute_embeddings()
        }
        
        return optimizations
    
    def implement_smart_caching(self):
        """ìŠ¤ë§ˆíŠ¸ ìºì‹± êµ¬í˜„"""
        
        class SmartCache:
            def __init__(self):
                self.cache = {}
                self.access_counts = {}
                self.ttl = 3600  # 1ì‹œê°„
            
            def get(self, key):
                if key in self.cache:
                    self.access_counts[key] += 1
                    return self.cache[key]
                return None
            
            def set(self, key, value):
                # LRU with frequency consideration
                if len(self.cache) >= 10000:
                    # ê°€ì¥ ì ê²Œ ì‚¬ìš©ëœ í•­ëª© ì œê±°
                    least_used = min(
                        self.access_counts.items(),
                        key=lambda x: x[1]
                    )[0]
                    del self.cache[least_used]
                    del self.access_counts[least_used]
                
                self.cache[key] = value
                self.access_counts[key] = 1
        
        return SmartCache()
    
    def measure_and_optimize(self, rag_system):
        """ì„±ëŠ¥ ì¸¡ì • ë° ìë™ ìµœì í™”"""
        
        # 1. í˜„ì¬ ì„±ëŠ¥ ì¸¡ì •
        baseline_metrics = self.monitor.measure_performance(rag_system)
        
        # 2. ë³‘ëª© ì§€ì  ì‹ë³„
        bottlenecks = self.identify_bottlenecks(baseline_metrics)
        
        # 3. ìë™ ìµœì í™” ì ìš©
        for bottleneck in bottlenecks:
            if bottleneck['component'] == 'retrieval':
                self.optimize_retrieval(rag_system)
            elif bottleneck['component'] == 'reranking':
                self.optimize_reranking(rag_system)
            elif bottleneck['component'] == 'generation':
                self.optimize_generation(rag_system)
        
        # 4. ê°œì„  í›„ ì„±ëŠ¥ ì¸¡ì •
        optimized_metrics = self.monitor.measure_performance(rag_system)
        
        # 5. ê°œì„  ë³´ê³ ì„œ ìƒì„±
        return self.generate_optimization_report(
            baseline_metrics,
            optimized_metrics
        )
```

## ğŸ› ï¸ ì‹¤ì „ í”„ë¡œì íŠ¸

### ë©”ì¸ í”„ë¡œì íŠ¸: "ê³ ì„±ëŠ¥ ê¸°ì—… ì§€ì‹ ê´€ë¦¬ ì‹œìŠ¤í…œ"
**ìš”êµ¬ì‚¬í•­**:
- 10,000ê°œ ì´ìƒì˜ ë¬¸ì„œ ì²˜ë¦¬
- í‰ê·  ì‘ë‹µ ì‹œê°„ < 2ì´ˆ
- ê²€ìƒ‰ ì •í™•ë„ 85% ì´ìƒ
- ë‹¤êµ­ì–´ ì§€ì› (í•œêµ­ì–´, ì˜ì–´, ì¼ë³¸ì–´)
- ì‹¤ì‹œê°„ ë¬¸ì„œ ì—…ë°ì´íŠ¸

**êµ¬í˜„ ë‚´ìš©**:
```python
class EnterpriseKnowledgeRAG:
    def __init__(self):
        # Advanced RAG ì»´í¬ë„ŒíŠ¸
        self.query_optimizer = AdvancedQueryOptimizer()
        self.hybrid_retriever = HybridRetriever()
        self.reranker = EnsembleReranker()
        self.compressor = AdaptiveCompressor()
        
    def build_system(self):
        # ì‹œìŠ¤í…œ êµ¬ì¶• ê³¼ì •
        pass
```

### ë„ì „ ê³¼ì œ
1. **ì‹¤ì‹œê°„ ë‰´ìŠ¤ RAG**: ìŠ¤íŠ¸ë¦¬ë° ë°ì´í„° ì²˜ë¦¬
2. **Multi-Modal RAG**: í…ìŠ¤íŠ¸ + ì´ë¯¸ì§€ í†µí•© ê²€ìƒ‰
3. **Federated RAG**: ë¶„ì‚° ë°ì´í„°ì†ŒìŠ¤ í†µí•©

## ğŸ“Š í‰ê°€ ê¸°ì¤€

### í•„ìˆ˜ ë‹¬ì„± ëª©í‘œ
- [ ] Pre-retrieval ìµœì í™” 3ê°€ì§€ ì´ìƒ êµ¬í˜„
- [ ] í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì‹œìŠ¤í…œ êµ¬ì¶•
- [ ] Cross-encoder ì¬ìˆœìœ„í™” êµ¬í˜„
- [ ] ì»¨í…ìŠ¤íŠ¸ ì••ì¶• ì•Œê³ ë¦¬ì¦˜ ì ìš©
- [ ] ì„±ëŠ¥ í–¥ìƒ 30% ì´ìƒ ë‹¬ì„±

### ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬
| ì§€í‘œ | Naive RAG | Advanced RAG | ëª©í‘œ |
|------|-----------|--------------|------|
| Precision@5 | 65% | 85% | +20% |
| Recall@10 | 58% | 82% | +24% |
| F1 Score | 61% | 83% | +22% |
| Latency | 250ms | 180ms | -28% |

## ğŸ¯ í•™ìŠµ ì„±ê³¼
- Advanced RAGì˜ ëª¨ë“  êµ¬ì„±ìš”ì†Œ ì™„ë²½ ì´í•´
- ì‹¤ì œ í”„ë¡œë•ì…˜ í™˜ê²½ ì ìš© ëŠ¥ë ¥
- ì„±ëŠ¥ ìµœì í™” ë° ì¸¡ì • ëŠ¥ë ¥
- Modular RAGë¡œ í™•ì¥ ê°€ëŠ¥í•œ ê¸°ë°˜ êµ¬ì¶•

## ğŸ“š í•„ìˆ˜ ì°¸ê³  ìë£Œ
- [Query Expansion Techniques](https://arxiv.org/abs/2305.03653)
- [Reranking for RAG](https://arxiv.org/abs/2304.09542)
- [Context Compression Methods](https://arxiv.org/abs/2310.06201)
- [Hybrid Search in RAG](https://www.elastic.co/blog/hybrid-retrieval)

## â­ï¸ ë‹¤ìŒ ë‹¨ê³„: Modular RAG
- ìœ ì—°í•œ ëª¨ë“ˆ êµ¬ì¡° ì„¤ê³„
- í”ŒëŸ¬ê·¸ì¸ ë°©ì‹ ì»´í¬ë„ŒíŠ¸
- ë„ë©”ì¸ íŠ¹í™” ëª¨ë“ˆ
- ìë™í™”ëœ íŒŒì´í”„ë¼ì¸ ìµœì í™”

## ğŸ’¡ í•µì‹¬ ë©”ì‹œì§€
"Advanced RAGëŠ” ë‹¨ìˆœíˆ ê¸°ëŠ¥ì„ ì¶”ê°€í•˜ëŠ” ê²ƒì´ ì•„ë‹™ë‹ˆë‹¤. ê° ì»´í¬ë„ŒíŠ¸ê°€ ìœ ê¸°ì ìœ¼ë¡œ ì—°ê²°ë˜ì–´ ì‹œë„ˆì§€ë¥¼ ë§Œë“¤ì–´ë‚´ëŠ” ê²ƒì´ í•µì‹¬ì…ë‹ˆë‹¤. ì¸¡ì •í•˜ê³ , ê°œì„ í•˜ê³ , ë‹¤ì‹œ ì¸¡ì •í•˜ëŠ” ì‚¬ì´í´ì„ ë°˜ë³µí•˜ì„¸ìš”."