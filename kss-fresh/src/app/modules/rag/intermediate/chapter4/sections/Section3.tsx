'use client'

import { Cpu } from 'lucide-react'

export default function Section3() {
  return (
    <section className="bg-white dark:bg-gray-800 rounded-2xl p-8 shadow-sm border border-gray-200 dark:border-gray-700">
      <div className="flex items-center gap-3 mb-6">
        <div className="w-12 h-12 rounded-xl bg-indigo-100 dark:bg-indigo-900/20 flex items-center justify-center">
          <Cpu className="text-indigo-600" size={24} />
        </div>
        <div>
          <h2 className="text-2xl font-bold text-gray-900 dark:text-white">4.3 ëª¨ë¸ ì–‘ìí™” ë° ìµœì í™”</h2>
          <p className="text-gray-600 dark:text-gray-400">ë©”ëª¨ë¦¬ì™€ ì—°ì‚° íš¨ìœ¨ì„± ê·¹ëŒ€í™”</p>
        </div>
      </div>

      <div className="space-y-6">
        <div className="bg-indigo-50 dark:bg-indigo-900/20 p-6 rounded-xl border border-indigo-200 dark:border-indigo-700">
          <h3 className="font-bold text-indigo-800 dark:text-indigo-200 mb-4">ëª¨ë¸ ì••ì¶• ê¸°ë²•</h3>

          <div className="prose prose-sm dark:prose-invert mb-4">
            <p className="text-gray-700 dark:text-gray-300">
              <strong>ëª¨ë¸ ì–‘ìí™”ëŠ” ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ì¤„ì´ê³  ì¶”ë¡  ì†ë„ë¥¼ í–¥ìƒì‹œí‚¤ëŠ” ê°•ë ¥í•œ ê¸°ìˆ ì…ë‹ˆë‹¤.</strong>
              32ë¹„íŠ¸ ë¶€ë™ì†Œìˆ˜ì ì„ 8ë¹„íŠ¸ ì •ìˆ˜ë¡œ ë³€í™˜í•˜ì—¬ ëª¨ë¸ í¬ê¸°ë¥¼ 4ë¶„ì˜ 1ë¡œ ì¤„ì´ë©´ì„œë„ ì„±ëŠ¥ ì €í•˜ëŠ” ìµœì†Œí™”í•©ë‹ˆë‹¤.
            </p>
            <p className="text-gray-700 dark:text-gray-300">
              <strong>ì£¼ìš” ì••ì¶• ê¸°ë²•:</strong>
            </p>
            <ul className="list-disc list-inside text-gray-700 dark:text-gray-300 space-y-1">
              <li><strong>ë™ì  ì–‘ìí™”</strong>: ì‹¤í–‰ ì‹œ ê°€ì¤‘ì¹˜ë¥¼ int8ë¡œ ë³€í™˜ (ê°€ì¥ ì‰¬ìš´ ë°©ë²•)</li>
              <li><strong>ì •ì  ì–‘ìí™”</strong>: ì‚¬ì „ì— ìº˜ë¦¬ë¸Œë ˆì´ì…˜í•˜ì—¬ ë” ë†’ì€ ì••ì¶•ë¥  ë‹¬ì„±</li>
              <li><strong>ONNX ë³€í™˜</strong>: í”„ë ˆì„ì›Œí¬ ë…ë¦½ì ì´ê³  ìµœì í™”ëœ í¬ë§·</li>
              <li><strong>í”„ë£¨ë‹</strong>: ì¤‘ìš”í•˜ì§€ ì•Šì€ ì—°ê²°ì„ ì œê±°í•˜ì—¬ í¬ì†Œ ëª¨ë¸ ìƒì„±</li>
            </ul>
            <div className="bg-yellow-100 dark:bg-yellow-900/20 p-3 rounded-lg mt-3">
              <p className="text-sm text-yellow-800 dark:text-yellow-200">
                <strong>ğŸ’¡ ì‹¤ë¬´ íŒ:</strong> ì–‘ìí™” ì‹œ ì •í™•ë„ë¥¼ ë°˜ë“œì‹œ ê²€ì¦í•˜ì„¸ìš”. ì¼ë°˜ì ìœ¼ë¡œ 2-5% ì •ë„ì˜ ì„±ëŠ¥ í•˜ë½ì€
                4ë°°ì˜ ì†ë„ í–¥ìƒì„ ìœ„í•œ í•©ë¦¬ì ì¸ íŠ¸ë ˆì´ë“œì˜¤í”„ì…ë‹ˆë‹¤.
              </p>
            </div>
          </div>

          <div className="bg-slate-50 dark:bg-slate-800 p-4 rounded-lg overflow-hidden border border-slate-200 dark:border-slate-700">
            <pre className="text-sm text-slate-800 dark:text-slate-200 overflow-x-auto max-h-96 overflow-y-auto font-mono">
{`import torch
import torch.quantization as quant
from transformers import AutoModel, AutoTokenizer
import onnx
import onnxruntime as ort

class ModelOptimizer:
    def __init__(self, model_name: str):
        self.model_name = model_name
        self.model = AutoModel.from_pretrained(model_name)
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)

    def quantize_dynamic(self, output_path: str):
        """ë™ì  ì–‘ìí™” (int8)"""
        print("ë™ì  ì–‘ìí™” ì‹œì‘...")

        # PyTorch ë™ì  ì–‘ìí™”
        quantized_model = torch.quantization.quantize_dynamic(
            self.model,
            {torch.nn.Linear},  # ì„ í˜• ë ˆì´ì–´ë§Œ ì–‘ìí™”
            dtype=torch.qint8   # int8ë¡œ ì••ì¶•
        )

        # ëª¨ë¸ ì €ì¥
        torch.save(quantized_model.state_dict(), f"{output_path}/quantized_model.pt")

        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë¹„êµ
        original_size = sum(p.numel() * p.element_size() for p in self.model.parameters())
        quantized_size = sum(p.numel() * p.element_size() for p in quantized_model.parameters())

        compression_ratio = original_size / quantized_size

        return {
            "original_size_mb": original_size / (1024**2),
            "quantized_size_mb": quantized_size / (1024**2),
            "compression_ratio": compression_ratio,
            "model": quantized_model
        }

    def convert_to_onnx(self, output_path: str, optimize: bool = True):
        """ONNX ë³€í™˜ ë° ìµœì í™”"""
        print("ONNX ë³€í™˜ ì‹œì‘...")

        # ë”ë¯¸ ì…ë ¥ ìƒì„±
        dummy_input = torch.randint(0, 1000, (1, 512))  # [batch_size, seq_len]

        # ONNX ë³€í™˜
        torch.onnx.export(
            self.model,
            dummy_input,
            f"{output_path}/model.onnx",
            input_names=['input_ids'],
            output_names=['last_hidden_state'],
            dynamic_axes={
                'input_ids': {0: 'batch_size', 1: 'sequence_length'},
                'last_hidden_state': {0: 'batch_size', 1: 'sequence_length'}
            },
            opset_version=14
        )

        if optimize:
            # ONNX ê·¸ë˜í”„ ìµœì í™”
            from onnxruntime.tools import optimizer
            optimized_model = optimizer.optimize_model(
                f"{output_path}/model.onnx",
                model_type='bert',
                use_gpu=False,
                opt_level=99  # ìµœëŒ€ ìµœì í™”
            )
            optimized_model.save_model_to_file(f"{output_path}/optimized_model.onnx")

        return f"{output_path}/optimized_model.onnx" if optimize else f"{output_path}/model.onnx"

    def benchmark_models(self, test_queries: List[str]):
        """ëª¨ë¸ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬"""
        results = {}

        # ì›ë³¸ ëª¨ë¸ ë²¤ì¹˜ë§ˆí¬
        print("ì›ë³¸ ëª¨ë¸ ë²¤ì¹˜ë§ˆí¬...")
        original_times = []
        for query in test_queries:
            start_time = time.time()
            inputs = self.tokenizer(query, return_tensors='pt', truncation=True)
            with torch.no_grad():
                outputs = self.model(**inputs)
            original_times.append(time.time() - start_time)

        results['original'] = {
            'avg_time': np.mean(original_times),
            'std_time': np.std(original_times)
        }

        # ì–‘ìí™”ëœ ëª¨ë¸ ë²¤ì¹˜ë§ˆí¬
        quantized_info = self.quantize_dynamic("./temp")
        quantized_model = quantized_info['model']

        print("ì–‘ìí™”ëœ ëª¨ë¸ ë²¤ì¹˜ë§ˆí¬...")
        quantized_times = []
        for query in test_queries:
            start_time = time.time()
            inputs = self.tokenizer(query, return_tensors='pt', truncation=True)
            with torch.no_grad():
                outputs = quantized_model(**inputs)
            quantized_times.append(time.time() - start_time)

        results['quantized'] = {
            'avg_time': np.mean(quantized_times),
            'std_time': np.std(quantized_times),
            'compression_ratio': quantized_info['compression_ratio']
        }

        # ONNX ëª¨ë¸ ë²¤ì¹˜ë§ˆí¬
        onnx_path = self.convert_to_onnx("./temp")
        session = ort.InferenceSession(onnx_path)

        print("ONNX ëª¨ë¸ ë²¤ì¹˜ë§ˆí¬...")
        onnx_times = []
        for query in test_queries:
            start_time = time.time()
            inputs = self.tokenizer(query, return_tensors='np', truncation=True)
            outputs = session.run(None, {'input_ids': inputs['input_ids']})
            onnx_times.append(time.time() - start_time)

        results['onnx'] = {
            'avg_time': np.mean(onnx_times),
            'std_time': np.std(onnx_times)
        }

        return results

# ì‚¬ìš© ì˜ˆì‹œ
optimizer = ModelOptimizer("sentence-transformers/all-MiniLM-L6-v2")

# í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬
test_queries = [
    "ì¸ê³µì§€ëŠ¥ì˜ ê¸°ë³¸ ì›ë¦¬ëŠ” ë¬´ì—‡ì¸ê°€ìš”?",
    "ë¨¸ì‹ ëŸ¬ë‹ê³¼ ë”¥ëŸ¬ë‹ì˜ ì°¨ì´ì ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”.",
    "ìì—°ì–´ ì²˜ë¦¬ì˜ ì£¼ìš” ê¸°ìˆ ë“¤ì€?",
    "ì»´í“¨í„° ë¹„ì „ì˜ ì‘ìš© ë¶„ì•¼ëŠ”?",
    "ê°•í™”í•™ìŠµì˜ í•µì‹¬ ê°œë…ì€?"
] * 20  # 100ê°œ ì¿¼ë¦¬

# ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
benchmark_results = optimizer.benchmark_models(test_queries)

print("\\n=== ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ ===")
for model_type, metrics in benchmark_results.items():
    print(f"{model_type.upper()}:")
    print(f"  í‰ê·  ì²˜ë¦¬ ì‹œê°„: {metrics['avg_time']:.4f}ì´ˆ")
    print(f"  í‘œì¤€ í¸ì°¨: {metrics['std_time']:.4f}ì´ˆ")
    if 'compression_ratio' in metrics:
        print(f"  ì••ì¶• ë¹„ìœ¨: {metrics['compression_ratio']:.2f}x")
    print()`}
            </pre>
          </div>
        </div>

        <div className="bg-cyan-50 dark:bg-cyan-900/20 p-6 rounded-xl border border-cyan-200 dark:border-cyan-700">
          <h3 className="font-bold text-cyan-800 dark:text-cyan-200 mb-4">ëª¨ë¸ ì–‘ìí™” ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼</h3>

          <div className="space-y-4">
            <div className="bg-white dark:bg-gray-800 p-4 rounded-lg border">
              <h4 className="font-medium text-gray-900 dark:text-white mb-3">ğŸ¯ ì‹¤ì œ ëª¨ë¸ë³„ ì••ì¶• íš¨ê³¼</h4>
              <div className="grid md:grid-cols-2 gap-4">
                <div>
                  <p className="text-sm font-medium text-gray-700 dark:text-gray-300 mb-2">BERT-base (110M íŒŒë¼ë¯¸í„°)</p>
                  <div className="space-y-1 text-xs">
                    <div className="flex justify-between">
                      <span>ì›ë³¸ í¬ê¸°:</span>
                      <span className="font-mono">438MB</span>
                    </div>
                    <div className="flex justify-between">
                      <span>INT8 ì–‘ìí™”:</span>
                      <span className="font-mono text-green-600">112MB (4x ì••ì¶•)</span>
                    </div>
                    <div className="flex justify-between">
                      <span>ì†ë„ í–¥ìƒ:</span>
                      <span className="font-mono text-blue-600">2.3x ë¹ ë¦„</span>
                    </div>
                    <div className="flex justify-between">
                      <span>ì •í™•ë„ ì†ì‹¤:</span>
                      <span className="font-mono text-orange-600">-1.2%</span>
                    </div>
                  </div>
                </div>
                <div>
                  <p className="text-sm font-medium text-gray-700 dark:text-gray-300 mb-2">T5-base (220M íŒŒë¼ë¯¸í„°)</p>
                  <div className="space-y-1 text-xs">
                    <div className="flex justify-between">
                      <span>ì›ë³¸ í¬ê¸°:</span>
                      <span className="font-mono">892MB</span>
                    </div>
                    <div className="flex justify-between">
                      <span>INT8 ì–‘ìí™”:</span>
                      <span className="font-mono text-green-600">230MB (3.9x ì••ì¶•)</span>
                    </div>
                    <div className="flex justify-between">
                      <span>ì†ë„ í–¥ìƒ:</span>
                      <span className="font-mono text-blue-600">2.8x ë¹ ë¦„</span>
                    </div>
                    <div className="flex justify-between">
                      <span>ì •í™•ë„ ì†ì‹¤:</span>
                      <span className="font-mono text-orange-600">-2.5%</span>
                    </div>
                  </div>
                </div>
              </div>
            </div>

            <div className="bg-white dark:bg-gray-800 p-4 rounded-lg border">
              <h4 className="font-medium text-gray-900 dark:text-white mb-2">ğŸ’¾ ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± ê°œì„ </h4>
              <div className="relative h-4 bg-gray-200 dark:bg-gray-700 rounded-full overflow-hidden">
                <div className="absolute left-0 top-0 h-full bg-red-500 flex items-center justify-center text-xs text-white font-bold" style={{width: '100%'}}>
                  ì›ë³¸: 4GB RAM í•„ìš”
                </div>
              </div>
              <div className="relative h-4 bg-gray-200 dark:bg-gray-700 rounded-full overflow-hidden mt-2">
                <div className="absolute left-0 top-0 h-full bg-green-500 flex items-center justify-center text-xs text-white font-bold" style={{width: '25%'}}>
                  ì–‘ìí™”: 1GB RAM
                </div>
              </div>
              <p className="text-xs text-gray-600 dark:text-gray-400 mt-2">
                ëª¨ë°”ì¼ ë””ë°”ì´ìŠ¤ì—ì„œë„ ì‹¤í–‰ ê°€ëŠ¥í•œ ìˆ˜ì¤€ìœ¼ë¡œ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê°ì†Œ
              </p>
            </div>
          </div>
        </div>
      </div>
    </section>
  )
}
