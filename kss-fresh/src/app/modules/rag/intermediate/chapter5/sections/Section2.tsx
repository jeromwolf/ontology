'use client'

import { Video } from 'lucide-react'

export default function Section2() {
  return (
    <section className="bg-white dark:bg-gray-800 rounded-2xl p-8 shadow-sm border border-gray-200 dark:border-gray-700">
      <div className="flex items-center gap-3 mb-6">
        <div className="w-12 h-12 rounded-xl bg-red-100 dark:bg-red-900/20 flex items-center justify-center">
          <Video className="text-red-600" size={24} />
        </div>
        <div>
          <h2 className="text-2xl font-bold text-gray-900 dark:text-white">5.2 ë¹„ë””ì˜¤ ê²€ìƒ‰ ë° ìš”ì•½</h2>
          <p className="text-gray-600 dark:text-gray-400">ì‹œê°„ ê¸°ë°˜ ë©€í‹°ë¯¸ë””ì–´ ì»¨í…ì¸  ì²˜ë¦¬</p>
        </div>
      </div>

      <div className="space-y-6">
        <div className="bg-red-50 dark:bg-red-900/20 p-6 rounded-xl border border-red-200 dark:border-red-700">
          <h3 className="font-bold text-red-800 dark:text-red-200 mb-4">ë¹„ë””ì˜¤ RAG ì‹œìŠ¤í…œ</h3>

          <div className="prose prose-sm dark:prose-invert mb-4">
            <p className="text-gray-700 dark:text-gray-300">
              <strong>ë¹„ë””ì˜¤ RAGëŠ” ì‹œê°„ ìˆœì„œê°€ ìˆëŠ” ë©€í‹°ë¯¸ë””ì–´ ì½˜í…ì¸ ë¥¼ ì²˜ë¦¬í•˜ëŠ” ê³ ê¸‰ ê¸°ìˆ ì…ë‹ˆë‹¤.</strong>
              ê¸°ì¡´ í…ìŠ¤íŠ¸ RAGì™€ ë‹¬ë¦¬ í”„ë ˆì„ ì‹œí€€ìŠ¤, ì˜¤ë””ì˜¤ íŠ¸ë™, ìë§‰ ë“± ë‹¤ì¸µì  ì •ë³´ë¥¼ í†µí•© ì²˜ë¦¬í•´ì•¼ í•©ë‹ˆë‹¤.
            </p>
            <p className="text-gray-700 dark:text-gray-300">
              <strong>ë¹„ë””ì˜¤ RAG í•µì‹¬ ê³¼ì •:</strong>
            </p>
            <ul className="list-disc list-inside text-gray-700 dark:text-gray-300 space-y-1">
              <li><strong>í‚¤í”„ë ˆì„ ì¶”ì¶œ</strong>: 10ì´ˆ ê°„ê²©ìœ¼ë¡œ ëŒ€í‘œ í”„ë ˆì„ ìƒ˜í”Œë§ (1ì‹œê°„â†’360í”„ë ˆì„)</li>
              <li><strong>ì´ë¯¸ì§€ ìº¡ì…”ë‹</strong>: BLIP ëª¨ë¸ë¡œ ê° í”„ë ˆì„ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜</li>
              <li><strong>ì‹œë§¨í‹± ì„¸ê·¸ë¨¼í…Œì´ì…˜</strong>: ì˜ë¯¸ ë‹¨ìœ„ë¡œ ë¹„ë””ì˜¤ë¥¼ 60ì´ˆ êµ¬ê°„ ë¶„í• </li>
              <li><strong>ì‹œê°„ ê¸°ë°˜ ê²€ìƒ‰</strong>: ì¿¼ë¦¬ì™€ ê´€ë ¨ëœ ì •í™•í•œ íƒ€ì„ìŠ¤íƒ¬í”„ ë°˜í™˜</li>
            </ul>

            <div className="bg-amber-50 dark:bg-amber-900/20 p-4 rounded-lg border border-amber-200 dark:border-amber-700 mt-4">
              <h4 className="font-bold text-amber-800 dark:text-amber-200 mb-2">ğŸ’¡ ì‹¤ë¬´ í™œìš© ì‚¬ë¡€</h4>
              <div className="grid md:grid-cols-2 gap-3 text-sm">
                <div>
                  <strong>ì˜¨ë¼ì¸ êµìœ¡</strong>
                  <ul className="list-disc list-inside ml-2 text-amber-700 dark:text-amber-300">
                    <li>ê°•ì˜ ë‚´ìš© ìë™ ì¸ë±ì‹±</li>
                    <li>"ê²½ì‚¬í•˜ê°•ë²• ì„¤ëª…" â†’ ì •í™•í•œ êµ¬ê°„ ì í”„</li>
                    <li>ìë™ í€´ì¦ˆ ë° ìš”ì•½ ìƒì„±</li>
                  </ul>
                </div>
                <div>
                  <strong>ë¯¸ë””ì–´ ê²€ìƒ‰</strong>
                  <ul className="list-disc list-inside ml-2 text-amber-700 dark:text-amber-300">
                    <li>ë‰´ìŠ¤ ì•„ì¹´ì´ë¸Œ ê²€ìƒ‰</li>
                    <li>"ì½”ë¡œë‚˜ ê´€ë ¨ ë³´ë„" â†’ ê´€ë ¨ ì˜ìƒ í´ë¦½</li>
                    <li>ê´‘ê³  ì½˜í…ì¸  ìë™ íƒœê¹…</li>
                  </ul>
                </div>
              </div>
            </div>
          </div>

          <div className="bg-slate-50 dark:bg-slate-800 p-4 rounded-lg overflow-hidden border border-slate-200 dark:border-slate-700">
            <pre className="text-sm text-slate-800 dark:text-slate-200 overflow-x-auto max-h-96 overflow-y-auto font-mono">
{`import cv2
import torch
from transformers import BlipProcessor, BlipForConditionalGeneration
from typing import List, Dict, Tuple
import numpy as np
from datetime import timedelta

class VideoRAGSystem:
    def __init__(self):
        # ì´ë¯¸ì§€ ìº¡ì…”ë‹ ëª¨ë¸ (ë¹„ë””ì˜¤ í”„ë ˆì„ ì„¤ëª…ìš©)
        self.processor = BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-base")
        self.captioning_model = BlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-base")

        # ë¹„ë””ì˜¤ ì„¸ê·¸ë¨¼íŠ¸ ì €ì¥ì†Œ
        self.video_segments = []
        self.segment_embeddings = []

    def extract_keyframes(self, video_path: str, interval_seconds: int = 10) -> List[Dict]:
        """ë¹„ë””ì˜¤ì—ì„œ í‚¤í”„ë ˆì„ ì¶”ì¶œ"""
        cap = cv2.VideoCapture(video_path)
        fps = cap.get(cv2.CAP_PROP_FPS)
        frame_interval = int(fps * interval_seconds)

        keyframes = []
        frame_count = 0

        while True:
            ret, frame = cap.read()
            if not ret:
                break

            if frame_count % frame_interval == 0:
                # í”„ë ˆì„ì„ PIL ì´ë¯¸ì§€ë¡œ ë³€í™˜
                frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                pil_image = Image.fromarray(frame_rgb)

                # íƒ€ì„ìŠ¤íƒ¬í”„ ê³„ì‚°
                timestamp_seconds = frame_count / fps
                timestamp = str(timedelta(seconds=int(timestamp_seconds)))

                keyframes.append({
                    'frame_number': frame_count,
                    'timestamp': timestamp,
                    'timestamp_seconds': timestamp_seconds,
                    'image': pil_image
                })

            frame_count += 1

        cap.release()
        return keyframes

    def generate_frame_captions(self, keyframes: List[Dict]) -> List[Dict]:
        """í‚¤í”„ë ˆì„ì— ëŒ€í•œ ìº¡ì…˜ ìƒì„±"""
        captioned_frames = []

        for frame_data in keyframes:
            try:
                # BLIPìœ¼ë¡œ ì´ë¯¸ì§€ ìº¡ì…˜ ìƒì„±
                inputs = self.processor(frame_data['image'], return_tensors="pt")

                with torch.no_grad():
                    out = self.captioning_model.generate(**inputs, max_length=50)

                caption = self.processor.decode(out[0], skip_special_tokens=True)

                captioned_frames.append({
                    'timestamp': frame_data['timestamp'],
                    'timestamp_seconds': frame_data['timestamp_seconds'],
                    'caption': caption,
                    'frame_number': frame_data['frame_number']
                })

            except Exception as e:
                print(f"ìº¡ì…˜ ìƒì„± ì‹¤íŒ¨ (í”„ë ˆì„ {frame_data['frame_number']}): {e}")
                captioned_frames.append({
                    'timestamp': frame_data['timestamp'],
                    'timestamp_seconds': frame_data['timestamp_seconds'],
                    'caption': 'ìº¡ì…˜ ìƒì„± ì‹¤íŒ¨',
                    'frame_number': frame_data['frame_number']
                })

        return captioned_frames

    def create_video_segments(self, video_path: str,
                            captioned_frames: List[Dict],
                            segment_duration: int = 60) -> List[Dict]:
        """ë¹„ë””ì˜¤ë¥¼ ì˜ë¯¸ìˆëŠ” ì„¸ê·¸ë¨¼íŠ¸ë¡œ ë¶„í• """
        segments = []
        current_segment = {
            'start_time': 0,
            'end_time': segment_duration,
            'captions': [],
            'key_topics': []
        }

        for frame in captioned_frames:
            timestamp = frame['timestamp_seconds']

            # í˜„ì¬ ì„¸ê·¸ë¨¼íŠ¸ì— ì†í•˜ëŠ”ì§€ í™•ì¸
            if current_segment['start_time'] <= timestamp < current_segment['end_time']:
                current_segment['captions'].append(frame['caption'])
            else:
                # í˜„ì¬ ì„¸ê·¸ë¨¼íŠ¸ ì™„ë£Œ ë° ì €ì¥
                if current_segment['captions']:
                    current_segment['summary'] = self.summarize_segment_captions(
                        current_segment['captions']
                    )
                    segments.append(current_segment.copy())

                # ìƒˆ ì„¸ê·¸ë¨¼íŠ¸ ì‹œì‘
                current_segment = {
                    'start_time': int(timestamp // segment_duration) * segment_duration,
                    'end_time': int(timestamp // segment_duration + 1) * segment_duration,
                    'captions': [frame['caption']],
                    'key_topics': []
                }

        # ë§ˆì§€ë§‰ ì„¸ê·¸ë¨¼íŠ¸ ì²˜ë¦¬
        if current_segment['captions']:
            current_segment['summary'] = self.summarize_segment_captions(
                current_segment['captions']
            )
            segments.append(current_segment)

        return segments

    def summarize_segment_captions(self, captions: List[str]) -> str:
        """ì„¸ê·¸ë¨¼íŠ¸ ìº¡ì…˜ë“¤ì„ ìš”ì•½"""
        if not captions:
            return "ë‚´ìš© ì—†ìŒ"

        # ê°„ë‹¨í•œ ìš”ì•½ ë¡œì§ (ì‹¤ì œë¡œëŠ” ë” ê³ ê¸‰ ìš”ì•½ ëª¨ë¸ ì‚¬ìš©)
        caption_text = ' '.join(captions)

        # í‚¤ì›Œë“œ ì¶”ì¶œ (ë¹ˆë„ ê¸°ë°˜)
        words = caption_text.lower().split()
        word_freq = {}

        for word in words:
            if len(word) > 3:  # 3ê¸€ì ì´ìƒ ë‹¨ì–´ë§Œ
                word_freq[word] = word_freq.get(word, 0) + 1

        # ìƒìœ„ í‚¤ì›Œë“œë“¤ë¡œ ìš”ì•½ êµ¬ì„±
        top_keywords = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)[:5]
        keywords = [word for word, freq in top_keywords]

        summary = f"ì£¼ìš” ë‚´ìš©: {', '.join(keywords)}"

        # ì²« ë²ˆì§¸ì™€ ë§ˆì§€ë§‰ ìº¡ì…˜ë„ í¬í•¨
        if len(captions) > 1:
            summary += f". ì‹œì‘: {captions[0][:50]}... ë: {captions[-1][:50]}..."

        return summary

    def search_video_content(self, query: str, video_segments: List[Dict]) -> List[Dict]:
        """ë¹„ë””ì˜¤ ì»¨í…ì¸ ì—ì„œ ì¿¼ë¦¬ ê´€ë ¨ ë¶€ë¶„ ê²€ìƒ‰"""
        results = []

        for segment in video_segments:
            # ì„¸ê·¸ë¨¼íŠ¸ ìš”ì•½ê³¼ ì¿¼ë¦¬ì˜ ìœ ì‚¬ë„ ê³„ì‚° (ê°„ë‹¨í•œ í‚¤ì›Œë“œ ë§¤ì¹­)
            segment_text = segment['summary'] + ' ' + ' '.join(segment['captions'])

            # í‚¤ì›Œë“œ ê¸°ë°˜ ì ìˆ˜ ê³„ì‚°
            query_words = query.lower().split()
            segment_words = segment_text.lower().split()

            matches = sum(1 for word in query_words if word in segment_words)
            score = matches / len(query_words) if query_words else 0

            if score > 0.3:  # 30% ì´ìƒ ë§¤ì¹­ì‹œ ê²°ê³¼ì— í¬í•¨
                results.append({
                    'segment': segment,
                    'score': score,
                    'start_time': segment['start_time'],
                    'end_time': segment['end_time'],
                    'relevant_captions': [
                        cap for cap in segment['captions']
                        if any(word in cap.lower() for word in query_words)
                    ]
                })

        # ì ìˆ˜ìˆœ ì •ë ¬
        results.sort(key=lambda x: x['score'], reverse=True)
        return results

    def process_video(self, video_path: str) -> Dict:
        """ë¹„ë””ì˜¤ ì „ì²´ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸"""
        print("1. í‚¤í”„ë ˆì„ ì¶”ì¶œ ì¤‘...")
        keyframes = self.extract_keyframes(video_path, interval_seconds=15)

        print("2. í”„ë ˆì„ ìº¡ì…˜ ìƒì„± ì¤‘...")
        captioned_frames = self.generate_frame_captions(keyframes)

        print("3. ë¹„ë””ì˜¤ ì„¸ê·¸ë¨¼íŠ¸ ìƒì„± ì¤‘...")
        video_segments = self.create_video_segments(video_path, captioned_frames)

        return {
            'video_path': video_path,
            'total_keyframes': len(keyframes),
            'total_segments': len(video_segments),
            'segments': video_segments,
            'processing_complete': True
        }

# ì‚¬ìš© ì˜ˆì‹œ
video_rag = VideoRAGSystem()

# ë¹„ë””ì˜¤ ì²˜ë¦¬
# video_data = video_rag.process_video('/path/to/lecture_video.mp4')

# print(f"ì²˜ë¦¬ ì™„ë£Œ: {video_data['total_segments']}ê°œ ì„¸ê·¸ë¨¼íŠ¸")

# ë¹„ë””ì˜¤ì—ì„œ ê²€ìƒ‰
# search_results = video_rag.search_video_content(
#     "machine learning",
#     video_data['segments']
# )

# print("\\nê²€ìƒ‰ ê²°ê³¼:")
# for result in search_results[:3]:
#     segment = result['segment']
#     print(f"ì‹œê°„: {segment['start_time']}s - {segment['end_time']}s")
#     print(f"ì ìˆ˜: {result['score']:.3f}")
#     print(f"ìš”ì•½: {segment['summary'][:100]}...")
#     print()`}
            </pre>
          </div>
        </div>
      </div>
    </section>
  )
}
