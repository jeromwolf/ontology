# RAG ê³ ê¸‰ ì»¤ë¦¬í˜ëŸ¼ (Step 3: Advanced)

## ğŸ¯ í•™ìŠµ ëª©í‘œ
í”„ë¡œë•ì…˜ ë ˆë²¨ì˜ ëŒ€ê·œëª¨ RAG ì‹œìŠ¤í…œì„ ì„¤ê³„í•˜ê³  êµ¬í˜„í•©ë‹ˆë‹¤. GraphRAG, Multi-hop reasoning, êµì°¨ ì–¸ì–´ ê²€ìƒ‰ ë“± ìµœì‹  ê¸°ìˆ ì„ ë§ˆìŠ¤í„°í•˜ê³ , ì—”í„°í”„ë¼ì´ì¦ˆ í™˜ê²½ì—ì„œì˜ ì‹¤ì œ ë°°í¬ì™€ ìš´ì˜ ëŠ¥ë ¥ì„ ê°–ì¶¥ë‹ˆë‹¤.

## ğŸ“š ì´ í•™ìŠµ ì‹œê°„: 20ì‹œê°„

## ğŸ“‹ ì»¤ë¦¬í˜ëŸ¼ êµ¬ì„±

### 1. GraphRAG ì•„í‚¤í…ì²˜ (5ì‹œê°„)

#### 1.1 ì§€ì‹ ê·¸ë˜í”„ ê¸°ë°˜ RAGì˜ ì´í•´ (1ì‹œê°„ 30ë¶„)
- **ì „í†µì  RAGì˜ í•œê³„**
  - ë‹¨ìˆœ ìœ ì‚¬ë„ ê¸°ë°˜ ê²€ìƒ‰ì˜ ë¬¸ì œì 
  - ë³µì¡í•œ ê´€ê³„ì„± íŒŒì•… ë¶ˆê°€
  - ë‹¤ì¤‘ í™‰ ì¶”ë¡ ì˜ ì–´ë ¤ì›€
  
- **GraphRAGì˜ í˜ì‹ **
  ```python
  # ê¸°ì¡´ RAG: í‰ë©´ì  ê²€ìƒ‰
  traditional_rag = "Query â†’ Vector Search â†’ Top-K Documents â†’ Generate"
  
  # GraphRAG: ê·¸ë˜í”„ ê¸°ë°˜ ì¶”ë¡ 
  graph_rag = "Query â†’ Entity Extraction â†’ Graph Traversal â†’ Context Construction â†’ Generate"
  ```

- **ì§€ì‹ ê·¸ë˜í”„ êµ¬ì„± ìš”ì†Œ**
  - **ë…¸ë“œ(Entity)**: ì‚¬ëŒ, ì¡°ì§, ê°œë…, ì´ë²¤íŠ¸
  - **ì—£ì§€(Relation)**: ê´€ê³„ì˜ ìœ í˜•ê³¼ ë°©í–¥ì„±
  - **ì†ì„±(Properties)**: ì¶”ê°€ ë©”íƒ€ë°ì´í„°

#### 1.2 GraphRAG êµ¬í˜„ ì•„í‚¤í…ì²˜ (2ì‹œê°„)
```python
class GraphRAGSystem:
    def __init__(self):
        self.neo4j_client = Neo4jClient()
        self.entity_extractor = EntityExtractor()
        self.relation_extractor = RelationExtractor()
        self.graph_embedder = GraphEmbedder()
    
    def build_knowledge_graph(self, documents):
        # 1. ì—”í‹°í‹° ì¶”ì¶œ
        entities = []
        for doc in documents:
            doc_entities = self.entity_extractor.extract(doc)
            entities.extend(doc_entities)
        
        # 2. ê´€ê³„ ì¶”ì¶œ
        relations = []
        for doc in documents:
            doc_relations = self.relation_extractor.extract(doc, entities)
            relations.extend(doc_relations)
        
        # 3. ê·¸ë˜í”„ êµ¬ì¶•
        self.create_graph_structure(entities, relations)
        
        # 4. ê·¸ë˜í”„ ì„ë² ë”©
        self.embed_graph_elements()
    
    def query(self, question):
        # 1. ì¿¼ë¦¬ì—ì„œ ì—”í‹°í‹° ì¶”ì¶œ
        query_entities = self.entity_extractor.extract(question)
        
        # 2. ì„œë¸Œê·¸ë˜í”„ ê²€ìƒ‰
        subgraph = self.find_relevant_subgraph(query_entities)
        
        # 3. ê²½ë¡œ íƒìƒ‰
        paths = self.traverse_paths(subgraph, max_hops=3)
        
        # 4. ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
        context = self.construct_context_from_paths(paths)
        
        return context
```

#### 1.3 Neo4jì™€ GraphRAG í†µí•© (1ì‹œê°„ 30ë¶„)
```python
# Neo4j ìŠ¤í‚¤ë§ˆ ì„¤ê³„
CREATE_SCHEMA = """
// ì—”í‹°í‹° ë…¸ë“œ
CREATE CONSTRAINT entity_id IF NOT EXISTS 
FOR (e:Entity) REQUIRE e.id IS UNIQUE;

// ë¬¸ì„œ ë…¸ë“œ
CREATE CONSTRAINT doc_id IF NOT EXISTS 
FOR (d:Document) REQUIRE d.id IS UNIQUE;

// ê´€ê³„ ì¸ë±ìŠ¤
CREATE INDEX rel_type IF NOT EXISTS 
FOR ()-[r:RELATED_TO]->() ON (r.type);
"""

# ë³µì¡í•œ ì¿¼ë¦¬ ì˜ˆì‹œ
COMPLEX_QUERY = """
MATCH path = (start:Entity {name: $entity_name})-[*1..3]-(related:Entity)
WHERE ALL(r IN relationships(path) WHERE r.confidence > 0.7)
WITH path, related, 
     reduce(score = 1.0, r IN relationships(path) | score * r.confidence) AS path_score
ORDER BY path_score DESC
LIMIT 10
RETURN path, path_score, 
       [n IN nodes(path) | {name: n.name, type: n.type}] AS entities,
       [r IN relationships(path) | {type: type(r), properties: properties(r)}] AS relations
"""

class Neo4jGraphRAG:
    def __init__(self, uri, user, password):
        self.driver = GraphDatabase.driver(uri, auth=(user, password))
    
    def multi_hop_query(self, start_entity, max_hops=3):
        with self.driver.session() as session:
            result = session.run("""
                MATCH path = (start:Entity {name: $name})-[*1..$max_hops]-(end:Entity)
                WHERE start <> end
                AND ALL(r IN relationships(path) WHERE r.relevance > 0.5)
                WITH path, 
                     length(path) as hop_count,
                     reduce(s = 1.0, r IN relationships(path) | s * r.relevance) as path_relevance
                ORDER BY path_relevance DESC
                LIMIT 20
                RETURN path, hop_count, path_relevance
            """, name=start_entity, max_hops=max_hops)
            
            return self.process_paths(result)
```

#### 1.4 ì§€ì‹ ê·¸ë˜í”„ ìë™ êµ¬ì¶• (1ì‹œê°„)
```python
class AutoGraphBuilder:
    def __init__(self):
        self.llm = LLMClient()
        self.ner_model = load_ner_model()
        self.relation_model = load_relation_model()
    
    def extract_triplets(self, text):
        # LLM ê¸°ë°˜ íŠ¸ë¦¬í”Œ ì¶”ì¶œ
        prompt = f"""
        ë‹¤ìŒ í…ìŠ¤íŠ¸ì—ì„œ (ì£¼ì–´, ìˆ ì–´, ëª©ì ì–´) í˜•íƒœì˜ ì§€ì‹ íŠ¸ë¦¬í”Œì„ ì¶”ì¶œí•˜ì„¸ìš”:
        
        í…ìŠ¤íŠ¸: {text}
        
        ì¶œë ¥ í˜•ì‹:
        - (ì—”í‹°í‹°1, ê´€ê³„, ì—”í‹°í‹°2)
        - ê° íŠ¸ë¦¬í”Œì€ ëª…í™•í•˜ê³  ì›ìì ì´ì–´ì•¼ í•¨
        - ê´€ê³„ëŠ” ë™ì‚¬í˜•ìœ¼ë¡œ í‘œí˜„
        
        íŠ¸ë¦¬í”Œ:
        """
        
        triplets = self.llm.extract(prompt)
        return self.validate_triplets(triplets)
    
    def incremental_update(self, new_documents):
        # ì¦ë¶„ ì—…ë°ì´íŠ¸
        for doc in new_documents:
            # 1. ìƒˆë¡œìš´ ì—”í‹°í‹°/ê´€ê³„ ì¶”ì¶œ
            new_triplets = self.extract_triplets(doc)
            
            # 2. ê¸°ì¡´ ê·¸ë˜í”„ì™€ ì¶©ëŒ í•´ê²°
            resolved_triplets = self.resolve_conflicts(new_triplets)
            
            # 3. ê·¸ë˜í”„ ì—…ë°ì´íŠ¸
            self.update_graph(resolved_triplets)
            
            # 4. ì„ë² ë”© ì¬ê³„ì‚° (ì˜í–¥ë°›ì€ ë…¸ë“œë§Œ)
            self.update_embeddings(resolved_triplets)
```

### 2. ë‹¤ì¤‘ í™‰ ì¶”ë¡ ê³¼ ë³µì¡í•œ ì¿¼ë¦¬ (5ì‹œê°„)

#### 2.1 Multi-hop Reasoning êµ¬í˜„ (2ì‹œê°„)
```python
class MultiHopReasoner:
    def __init__(self, graph_db, llm):
        self.graph = graph_db
        self.llm = llm
        self.reasoning_cache = {}
    
    def reason(self, question, max_hops=3):
        # 1. ì§ˆë¬¸ ë¶„í•´
        sub_questions = self.decompose_question(question)
        
        # 2. ê° ì„œë¸Œ ì§ˆë¬¸ì— ëŒ€í•œ ì¦ê±° ìˆ˜ì§‘
        evidence_chains = []
        for sq in sub_questions:
            chain = self.collect_evidence_chain(sq, max_hops)
            evidence_chains.append(chain)
        
        # 3. ì¦ê±° í†µí•© ë° ì¶”ë¡ 
        final_answer = self.integrate_evidence(evidence_chains, question)
        
        return {
            'answer': final_answer,
            'reasoning_path': evidence_chains,
            'confidence': self.calculate_confidence(evidence_chains)
        }
    
    def decompose_question(self, question):
        prompt = f"""
        ë³µì¡í•œ ì§ˆë¬¸ì„ ë‹¨ê³„ë³„ ì„œë¸Œ ì§ˆë¬¸ìœ¼ë¡œ ë¶„í•´í•˜ì„¸ìš”:
        
        ì§ˆë¬¸: {question}
        
        ë¶„í•´ ê·œì¹™:
        1. ê° ì„œë¸Œ ì§ˆë¬¸ì€ ë…ë¦½ì ìœ¼ë¡œ ë‹µë³€ ê°€ëŠ¥í•´ì•¼ í•¨
        2. ë…¼ë¦¬ì  ìˆœì„œë¥¼ ìœ ì§€
        3. ìµœëŒ€ 4ê°œì˜ ì„œë¸Œ ì§ˆë¬¸ìœ¼ë¡œ ì œí•œ
        
        ì„œë¸Œ ì§ˆë¬¸ë“¤:
        """
        
        return self.llm.generate(prompt).split('\n')
    
    def collect_evidence_chain(self, question, max_hops):
        # ì‹œì‘ ì—”í‹°í‹° ì‹ë³„
        start_entities = self.identify_entities(question)
        
        evidence = []
        current_nodes = start_entities
        
        for hop in range(max_hops):
            # í˜„ì¬ ë…¸ë“œì—ì„œ ê´€ë ¨ ì •ë³´ ìˆ˜ì§‘
            hop_evidence = []
            
            for node in current_nodes:
                # ì´ì›ƒ ë…¸ë“œ íƒìƒ‰
                neighbors = self.graph.get_neighbors(node, hop_depth=1)
                
                # ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚°
                scored_neighbors = [
                    (n, self.calculate_relevance(n, question))
                    for n in neighbors
                ]
                
                # ìƒìœ„ Kê°œ ì„ íƒ
                top_neighbors = sorted(
                    scored_neighbors, 
                    key=lambda x: x[1], 
                    reverse=True
                )[:5]
                
                hop_evidence.extend(top_neighbors)
            
            evidence.append(hop_evidence)
            
            # ë‹¤ìŒ í™‰ì„ ìœ„í•œ ë…¸ë“œ ì—…ë°ì´íŠ¸
            current_nodes = [n[0] for n in hop_evidence]
            
            # ì¡°ê¸° ì¢…ë£Œ ì¡°ê±´
            if self.has_sufficient_evidence(evidence, question):
                break
        
        return evidence
```

#### 2.2 Chain-of-Thought RAG (1ì‹œê°„ 30ë¶„)
```python
class ChainOfThoughtRAG:
    def __init__(self):
        self.llm = LLMClient()
        self.retriever = HybridRetriever()
    
    def generate_with_reasoning(self, question):
        # 1. ì´ˆê¸° ê²€ìƒ‰
        initial_docs = self.retriever.search(question, top_k=10)
        
        # 2. ì¶”ë¡  ì²´ì¸ ìƒì„±
        reasoning_prompt = f"""
        ì§ˆë¬¸: {question}
        
        ê´€ë ¨ ë¬¸ì„œë“¤:
        {self.format_documents(initial_docs)}
        
        ì´ ì§ˆë¬¸ì— ë‹µí•˜ê¸° ìœ„í•œ ë‹¨ê³„ë³„ ì¶”ë¡  ê³¼ì •ì„ ì‘ì„±í•˜ì„¸ìš”:
        1. ë¨¼ì € ë¬´ì—‡ì„ í™•ì¸í•´ì•¼ í•˜ë‚˜ìš”?
        2. ì–´ë–¤ ì¶”ê°€ ì •ë³´ê°€ í•„ìš”í•œê°€ìš”?
        3. ì–´ë–¤ ë…¼ë¦¬ì  ì—°ê²°ì´ í•„ìš”í•œê°€ìš”?
        
        ì¶”ë¡  ê³¼ì •:
        """
        
        reasoning_steps = self.llm.generate(reasoning_prompt)
        
        # 3. ê° ì¶”ë¡  ë‹¨ê³„ì— ëŒ€í•œ ì¶”ê°€ ê²€ìƒ‰
        enriched_context = []
        for step in reasoning_steps.split('\n'):
            if self.needs_additional_info(step):
                additional_docs = self.retriever.search(step, top_k=3)
                enriched_context.extend(additional_docs)
        
        # 4. ìµœì¢… ë‹µë³€ ìƒì„±
        final_prompt = f"""
        ì§ˆë¬¸: {question}
        
        ì¶”ë¡  ê³¼ì •:
        {reasoning_steps}
        
        ìˆ˜ì§‘ëœ ì •ë³´:
        ì´ˆê¸° ë¬¸ì„œ: {self.format_documents(initial_docs)}
        ì¶”ê°€ ë¬¸ì„œ: {self.format_documents(enriched_context)}
        
        ìœ„ì˜ ì¶”ë¡  ê³¼ì •ê³¼ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìƒì„¸í•œ ë‹µë³€ì„ ì‘ì„±í•˜ì„¸ìš”:
        """
        
        return self.llm.generate(final_prompt)
```

#### 2.3 Self-RAG: ìê¸° ì„±ì°°ì  ê²€ìƒ‰ (1ì‹œê°„ 30ë¶„)
```python
class SelfRAG:
    def __init__(self):
        self.retriever = AdaptiveRetriever()
        self.critic = CriticModel()
        self.generator = GeneratorModel()
    
    def generate(self, question, max_iterations=3):
        # ì´ˆê¸° ìƒì„±
        context = self.retriever.search(question)
        answer = self.generator.generate(question, context)
        
        for iteration in range(max_iterations):
            # 1. ë‹µë³€ í’ˆì§ˆ í‰ê°€
            critique = self.critic.evaluate(
                question=question,
                answer=answer,
                context=context
            )
            
            if critique['score'] > 0.9:
                break
            
            # 2. ê°œì„ ì´ í•„ìš”í•œ ë¶€ë¶„ ì‹ë³„
            weak_points = self.identify_weaknesses(critique)
            
            # 3. ì¶”ê°€ ê²€ìƒ‰ ì „ëµ ê²°ì •
            search_strategy = self.determine_search_strategy(weak_points)
            
            # 4. íƒ€ê²Ÿ ê²€ìƒ‰ ìˆ˜í–‰
            if search_strategy['type'] == 'specific':
                # íŠ¹ì • ì •ë³´ ê²€ìƒ‰
                additional_context = self.retriever.search_specific(
                    search_strategy['query'],
                    filters=search_strategy['filters']
                )
            elif search_strategy['type'] == 'broader':
                # ë” ë„“ì€ ë²”ìœ„ ê²€ìƒ‰
                additional_context = self.retriever.search_broad(
                    question,
                    excluded_docs=context
                )
            
            # 5. ì»¨í…ìŠ¤íŠ¸ ì—…ë°ì´íŠ¸
            context = self.merge_contexts(context, additional_context)
            
            # 6. ë‹µë³€ ì¬ìƒì„±
            answer = self.generator.generate(
                question, 
                context,
                previous_answer=answer,
                critique=critique
            )
        
        return {
            'answer': answer,
            'iterations': iteration + 1,
            'final_score': critique['score'],
            'context_used': context
        }
```

### 3. êµì°¨ ì–¸ì–´ ë° ë‹¤ì¤‘ ëª¨ë“œ RAG (5ì‹œê°„)

#### 3.1 Cross-lingual RAG êµ¬í˜„ (2ì‹œê°„)
```python
class CrossLingualRAG:
    def __init__(self):
        self.multilingual_encoder = M3Encoder()  # BGE-M3
        self.language_detector = LanguageDetector()
        self.translator = TranslationModel()
    
    def build_multilingual_index(self, documents):
        # ì–¸ì–´ë³„ ì¸ë±ìŠ¤ êµ¬ì¶•
        self.indices = {}
        
        for doc in documents:
            lang = self.language_detector.detect(doc.text)
            
            if lang not in self.indices:
                self.indices[lang] = VectorIndex()
            
            # ì–¸ì–´ íŠ¹í™” ì „ì²˜ë¦¬
            processed_text = self.preprocess_by_language(doc.text, lang)
            
            # ë‹¤êµ­ì–´ ì„ë² ë”©
            embedding = self.multilingual_encoder.encode(
                processed_text,
                language=lang
            )
            
            # ì¸ë±ì‹±
            self.indices[lang].add(
                id=doc.id,
                embedding=embedding,
                metadata={
                    'text': doc.text,
                    'language': lang,
                    'processed': processed_text
                }
            )
    
    def cross_lingual_search(self, query, target_languages=None):
        query_lang = self.language_detector.detect(query)
        
        if target_languages is None:
            target_languages = list(self.indices.keys())
        
        all_results = []
        
        # 1. ì›ì–´ ê²€ìƒ‰
        if query_lang in self.indices:
            native_results = self.search_in_language(query, query_lang)
            all_results.extend(native_results)
        
        # 2. êµì°¨ ì–¸ì–´ ê²€ìƒ‰
        for target_lang in target_languages:
            if target_lang == query_lang:
                continue
            
            # ì¿¼ë¦¬ ë²ˆì—­ (í•„ìš”ì‹œ)
            if self.should_translate(query_lang, target_lang):
                translated_query = self.translator.translate(
                    query, 
                    source=query_lang,
                    target=target_lang
                )
            else:
                translated_query = query
            
            # íƒ€ê²Ÿ ì–¸ì–´ë¡œ ê²€ìƒ‰
            cross_results = self.search_in_language(
                translated_query, 
                target_lang,
                cross_lingual=True
            )
            
            all_results.extend(cross_results)
        
        # 3. ê²°ê³¼ í†µí•© ë° ì¬ìˆœìœ„
        return self.rerank_multilingual_results(all_results, query)
```

#### 3.2 Multimodal RAG (ì´ë¯¸ì§€, í…ìŠ¤íŠ¸, í‘œ) (1ì‹œê°„ 30ë¶„)
```python
class MultimodalRAG:
    def __init__(self):
        self.text_encoder = TextEncoder()
        self.image_encoder = CLIPModel()
        self.table_processor = TableProcessor()
        self.layout_analyzer = LayoutAnalyzer()
    
    def process_multimodal_document(self, document):
        # 1. ë ˆì´ì•„ì›ƒ ë¶„ì„
        layout = self.layout_analyzer.analyze(document)
        
        elements = []
        
        for region in layout.regions:
            if region.type == 'text':
                # í…ìŠ¤íŠ¸ ì²˜ë¦¬
                text_embedding = self.text_encoder.encode(region.content)
                elements.append({
                    'type': 'text',
                    'content': region.content,
                    'embedding': text_embedding,
                    'bbox': region.bbox
                })
                
            elif region.type == 'image':
                # ì´ë¯¸ì§€ ì²˜ë¦¬
                image = self.extract_image(document, region.bbox)
                image_embedding = self.image_encoder.encode_image(image)
                image_caption = self.generate_caption(image)
                
                elements.append({
                    'type': 'image',
                    'embedding': image_embedding,
                    'caption': image_caption,
                    'bbox': region.bbox
                })
                
            elif region.type == 'table':
                # í‘œ ì²˜ë¦¬
                table_data = self.table_processor.extract_table(
                    document, 
                    region.bbox
                )
                table_text = self.table_to_text(table_data)
                table_embedding = self.text_encoder.encode(table_text)
                
                elements.append({
                    'type': 'table',
                    'data': table_data,
                    'text': table_text,
                    'embedding': table_embedding,
                    'bbox': region.bbox
                })
        
        return elements
    
    def multimodal_search(self, query, modalities=['text', 'image', 'table']):
        results = {}
        
        if isinstance(query, str):
            # í…ìŠ¤íŠ¸ ì¿¼ë¦¬
            query_embedding = self.text_encoder.encode(query)
            
            if 'text' in modalities:
                results['text'] = self.search_text_elements(query_embedding)
            
            if 'image' in modalities:
                # í…ìŠ¤íŠ¸â†’ì´ë¯¸ì§€ ê²€ìƒ‰ (CLIP)
                image_query_embedding = self.image_encoder.encode_text(query)
                results['image'] = self.search_image_elements(image_query_embedding)
            
            if 'table' in modalities:
                results['table'] = self.search_table_elements(query_embedding)
                
        elif isinstance(query, Image):
            # ì´ë¯¸ì§€ ì¿¼ë¦¬
            query_embedding = self.image_encoder.encode_image(query)
            results['image'] = self.search_image_elements(query_embedding)
        
        # ê²°ê³¼ í†µí•©
        return self.fuse_multimodal_results(results)
```

#### 3.3 ì½”ë“œ ê²€ìƒ‰ RAG (1ì‹œê°„ 30ë¶„)
```python
class CodeRAG:
    def __init__(self):
        self.code_encoder = CodeBERTModel()
        self.ast_parser = ASTParser()
        self.doc_parser = DocstringParser()
    
    def index_codebase(self, repo_path):
        # 1. ì½”ë“œ íŒŒì¼ ìˆ˜ì§‘
        code_files = self.collect_code_files(repo_path)
        
        for file_path in code_files:
            with open(file_path, 'r') as f:
                code = f.read()
            
            # 2. AST ë¶„ì„
            ast_tree = self.ast_parser.parse(code)
            
            # 3. í•¨ìˆ˜/í´ë˜ìŠ¤ ë‹¨ìœ„ë¡œ ë¶„í• 
            for node in ast_tree.walk():
                if isinstance(node, (FunctionDef, ClassDef)):
                    # ì½”ë“œ ìŠ¤ë‹ˆí« ì¶”ì¶œ
                    snippet = self.extract_snippet(code, node)
                    
                    # ë¬¸ì„œí™” ì¶”ì¶œ
                    docstring = self.doc_parser.extract(node)
                    
                    # ì‹œê·¸ë‹ˆì²˜ ë¶„ì„
                    signature = self.analyze_signature(node)
                    
                    # ì„ë² ë”© ìƒì„±
                    embedding = self.create_code_embedding(
                        code=snippet,
                        docstring=docstring,
                        signature=signature
                    )
                    
                    # ì¸ë±ì‹±
                    self.index.add({
                        'id': f"{file_path}:{node.name}",
                        'type': node.__class__.__name__,
                        'name': node.name,
                        'code': snippet,
                        'docstring': docstring,
                        'signature': signature,
                        'embedding': embedding,
                        'file_path': file_path,
                        'line_number': node.lineno
                    })
    
    def search_code(self, query, search_type='natural'):
        if search_type == 'natural':
            # ìì—°ì–´ ì¿¼ë¦¬
            query_embedding = self.code_encoder.encode_text(query)
        elif search_type == 'code':
            # ì½”ë“œ ì¿¼ë¦¬
            query_embedding = self.code_encoder.encode_code(query)
        elif search_type == 'signature':
            # ì‹œê·¸ë‹ˆì²˜ ê²€ìƒ‰
            return self.signature_search(query)
        
        # ìœ ì‚¬ë„ ê²€ìƒ‰
        results = self.index.search(query_embedding, top_k=10)
        
        # ì½”ë“œ íŠ¹í™” ì¬ìˆœìœ„
        return self.rerank_code_results(results, query)
```

### 4. ëŒ€ê·œëª¨ ì‹œìŠ¤í…œ ì„¤ê³„ì™€ ìš´ì˜ (5ì‹œê°„)

#### 4.1 ë¶„ì‚° RAG ì•„í‚¤í…ì²˜ (2ì‹œê°„)
```python
# Kubernetes ë°°í¬ ì„¤ì •
apiVersion: apps/v1
kind: Deployment
metadata:
  name: rag-system
spec:
  replicas: 3
  selector:
    matchLabels:
      app: rag
  template:
    metadata:
      labels:
        app: rag
    spec:
      containers:
      - name: embedding-service
        image: rag/embedding:latest
        resources:
          requests:
            memory: "4Gi"
            cpu: "2"
          limits:
            memory: "8Gi"
            cpu: "4"
            nvidia.com/gpu: "1"  # GPU í• ë‹¹
      
      - name: retrieval-service
        image: rag/retrieval:latest
        env:
        - name: VECTOR_DB_URL
          value: "vector-db-service:8080"
        
      - name: generation-service
        image: rag/generation:latest
        env:
        - name: LLM_ENDPOINT
          value: "https://api.openai.com/v1"

---
# ë¡œë“œ ë°¸ëŸ°ì„œ
apiVersion: v1
kind: Service
metadata:
  name: rag-load-balancer
spec:
  selector:
    app: rag
  ports:
  - protocol: TCP
    port: 80
    targetPort: 8000
  type: LoadBalancer

# ë¶„ì‚° ì²˜ë¦¬ ì½”ë“œ
class DistributedRAG:
    def __init__(self):
        self.embedding_pool = EmbeddingServicePool(
            endpoints=['embed-1:8080', 'embed-2:8080', 'embed-3:8080']
        )
        self.retrieval_shards = RetrievalShards(
            shards=8,
            replication_factor=3
        )
        self.cache = RedisCache(
            nodes=['redis-1:6379', 'redis-2:6379', 'redis-3:6379']
        )
    
    async def process_batch(self, queries):
        # 1. ë°°ì¹˜ë¥¼ ì—¬ëŸ¬ ì›Œì»¤ë¡œ ë¶„ì‚°
        chunks = self.split_batch(queries, num_workers=len(self.embedding_pool))
        
        # 2. ë³‘ë ¬ ì„ë² ë”© ì²˜ë¦¬
        embedding_tasks = [
            self.embedding_pool.encode_batch(chunk)
            for chunk in chunks
        ]
        embeddings = await asyncio.gather(*embedding_tasks)
        
        # 3. ìƒ¤ë“œë³„ ê²€ìƒ‰
        search_tasks = []
        for shard_id in range(self.retrieval_shards.num_shards):
            shard_embeddings = self.route_to_shard(embeddings, shard_id)
            task = self.retrieval_shards.search_shard(
                shard_id, 
                shard_embeddings
            )
            search_tasks.append(task)
        
        shard_results = await asyncio.gather(*search_tasks)
        
        # 4. ê²°ê³¼ ë³‘í•©
        return self.merge_shard_results(shard_results)
```

#### 4.2 ì‹¤ì‹œê°„ ì—…ë°ì´íŠ¸ì™€ ìŠ¤íŠ¸ë¦¬ë° (1ì‹œê°„ 30ë¶„)
```python
class StreamingRAG:
    def __init__(self):
        self.kafka_consumer = KafkaConsumer(
            'document-updates',
            bootstrap_servers=['kafka-1:9092', 'kafka-2:9092']
        )
        self.index_updater = IncrementalIndexer()
        self.version_manager = VersionManager()
    
    async def start_streaming(self):
        # ì‹¤ì‹œê°„ ë¬¸ì„œ ì—…ë°ì´íŠ¸ ì²˜ë¦¬
        async for message in self.kafka_consumer:
            document = json.loads(message.value)
            
            # 1. ë¬¸ì„œ ë²„ì „ ê´€ë¦¬
            version = self.version_manager.create_version(document)
            
            # 2. ì¦ë¶„ ì¸ë±ì‹±
            if document['operation'] == 'create':
                await self.index_updater.add_document(document)
            elif document['operation'] == 'update':
                await self.index_updater.update_document(document)
            elif document['operation'] == 'delete':
                await self.index_updater.delete_document(document['id'])
            
            # 3. ìºì‹œ ë¬´íš¨í™”
            self.invalidate_related_cache(document)
            
            # 4. êµ¬ë…ìì—ê²Œ ì•Œë¦¼
            await self.notify_subscribers({
                'type': 'index_update',
                'document_id': document['id'],
                'version': version,
                'timestamp': time.time()
            })
    
    def streaming_search(self, query):
        # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ
        async def generate():
            # 1. ì´ˆê¸° ê²€ìƒ‰ ê²°ê³¼
            initial_results = await self.search(query)
            yield {'type': 'initial', 'results': initial_results}
            
            # 2. ì ì§„ì  ê°œì„ 
            for refinement in self.progressive_refinement(query, initial_results):
                yield {'type': 'refinement', 'results': refinement}
            
            # 3. ì‹¤ì‹œê°„ ì—…ë°ì´íŠ¸ êµ¬ë…
            async for update in self.subscribe_to_updates(query):
                if self.is_relevant_update(update, query):
                    new_results = await self.incremental_search(query, update)
                    yield {'type': 'update', 'results': new_results}
        
        return generate()
```

#### 4.3 í”„ë¡œë•ì…˜ ëª¨ë‹ˆí„°ë§ê³¼ ìµœì í™” (1ì‹œê°„ 30ë¶„)
```python
class RAGMonitoring:
    def __init__(self):
        self.prometheus = PrometheusClient()
        self.grafana = GrafanaClient()
        self.alerting = AlertManager()
    
    def setup_metrics(self):
        # ë©”íŠ¸ë¦­ ì •ì˜
        self.metrics = {
            'query_latency': Histogram(
                'rag_query_latency_seconds',
                'Query latency in seconds',
                buckets=[0.1, 0.5, 1.0, 2.0, 5.0, 10.0]
            ),
            'retrieval_precision': Gauge(
                'rag_retrieval_precision',
                'Retrieval precision score'
            ),
            'cache_hit_rate': Counter(
                'rag_cache_hits_total',
                'Total number of cache hits'
            ),
            'embedding_throughput': Summary(
                'rag_embedding_throughput',
                'Embeddings processed per second'
            )
        }
    
    @self.metrics['query_latency'].time()
    def monitored_query(self, query):
        start_time = time.time()
        
        try:
            # ì¿¼ë¦¬ ì‹¤í–‰
            results = self.rag_system.query(query)
            
            # í’ˆì§ˆ ë©”íŠ¸ë¦­ ìˆ˜ì§‘
            precision = self.calculate_precision(results)
            self.metrics['retrieval_precision'].set(precision)
            
            # ì„±ê³µ ë¡œê¹…
            self.log_query_success({
                'query': query,
                'latency': time.time() - start_time,
                'result_count': len(results),
                'precision': precision
            })
            
            return results
            
        except Exception as e:
            # ì—ëŸ¬ ì¶”ì 
            self.log_query_error({
                'query': query,
                'error': str(e),
                'traceback': traceback.format_exc()
            })
            
            # ì•Œë¦¼ ë°œì†¡
            if self.is_critical_error(e):
                self.alerting.send_alert({
                    'severity': 'critical',
                    'message': f'RAG query failed: {str(e)}',
                    'query': query
                })
            
            raise
    
    def auto_optimize(self):
        # ì„±ëŠ¥ ë°ì´í„° ìˆ˜ì§‘
        perf_data = self.collect_performance_data()
        
        # ë³‘ëª© ì§€ì  ì‹ë³„
        bottlenecks = self.identify_bottlenecks(perf_data)
        
        # ìë™ ìµœì í™” ì ìš©
        for bottleneck in bottlenecks:
            if bottleneck['type'] == 'slow_embedding':
                self.scale_embedding_service()
            elif bottleneck['type'] == 'cache_miss':
                self.adjust_cache_policy()
            elif bottleneck['type'] == 'retrieval_latency':
                self.optimize_index_parameters()
```

## ğŸ› ï¸ ê³ ê¸‰ í”„ë¡œì íŠ¸

### í”„ë¡œì íŠ¸ 1: ì—”í„°í”„ë¼ì´ì¦ˆ RAG í”Œë«í¼
- 10TB+ ë¬¸ì„œ ì²˜ë¦¬
- ë‹¤ì¤‘ í…Œë„ŒíŠ¸ ì§€ì›
- RBAC ê¸°ë°˜ ì ‘ê·¼ ì œì–´
- ê°ì‚¬ ë¡œê¹… ë° ì»´í”Œë¼ì´ì–¸ìŠ¤

### í”„ë¡œì íŠ¸ 2: ì‹¤ì‹œê°„ ê¸ˆìœµ RAG ì‹œìŠ¤í…œ
- ìŠ¤íŠ¸ë¦¬ë° ë‰´ìŠ¤ ë°ì´í„° ì²˜ë¦¬
- ì´ˆì €ì§€ì—° ê²€ìƒ‰ (< 50ms)
- ê·œì œ ë¬¸ì„œ ìë™ ì—…ë°ì´íŠ¸
- ë‹¤êµ­ì–´ ê¸ˆìœµ ìš©ì–´ ì²˜ë¦¬

### í”„ë¡œì íŠ¸ 3: ì˜ë£Œ GraphRAG
- ì˜í•™ ì˜¨í†¨ë¡œì§€ í†µí•©
- ì•½ë¬¼ ìƒí˜¸ì‘ìš© ê·¸ë˜í”„
- ì„ìƒ ê°€ì´ë“œë¼ì¸ ì¶”ë¡ 
- HIPAA ì¤€ìˆ˜ ë³´ì•ˆ

## ğŸ“Š í‰ê°€ ê¸°ì¤€

### ê³ ê¸‰ ì—­ëŸ‰ ì²´í¬ë¦¬ìŠ¤íŠ¸
- [ ] GraphRAG ì‹œìŠ¤í…œ ì„¤ê³„ ë° êµ¬í˜„
- [ ] Multi-hop reasoning êµ¬í˜„
- [ ] Cross-lingual RAG êµ¬ì¶•
- [ ] ë¶„ì‚° ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜ ì„¤ê³„
- [ ] í”„ë¡œë•ì…˜ ë°°í¬ ë° ëª¨ë‹ˆí„°ë§

### ìµœì¢… í”„ë¡œì íŠ¸
- ì‹¤ì œ ê¸°ì—… í™˜ê²½ì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ RAG ì‹œìŠ¤í…œ êµ¬ì¶•
- ì¼ì¼ 100ë§Œ ì¿¼ë¦¬ ì²˜ë¦¬ ê°€ëŠ¥
- 99.9% ê°€ë™ë¥  ë‹¬ì„±
- ë‹¤êµ­ì–´ ë° ë©€í‹°ëª¨ë‹¬ ì§€ì›

## ğŸ¯ í•™ìŠµ ì„±ê³¼
- ìµœì²¨ë‹¨ RAG ê¸°ìˆ  ì™„ë²½ ì´í•´
- ëŒ€ê·œëª¨ ì‹œìŠ¤í…œ ì„¤ê³„ ë° ìš´ì˜ ëŠ¥ë ¥
- ë³µì¡í•œ ë¹„ì¦ˆë‹ˆìŠ¤ ìš”êµ¬ì‚¬í•­ í•´ê²°
- RAG ë¶„ì•¼ ì „ë¬¸ê°€ ìˆ˜ì¤€ ë‹¬ì„±

## ğŸ“š ì¶”ê°€ í•™ìŠµ ìë£Œ
- [GraphRAG: Microsoft Research](https://github.com/microsoft/graphrag)
- [Self-RAG Paper](https://arxiv.org/abs/2310.11511)
- [REALM: Retrieval-Augmented Language Model Pre-Training](https://arxiv.org/abs/2002.08909)
- [Atlas: Few-shot Learning with Retrieval Augmented Language Models](https://arxiv.org/abs/2208.03299)
- [LlamaIndex Advanced Tutorials](https://docs.llamaindex.ai/en/stable/examples/index.html)

## ğŸ† ì¸ì¦ ë° ê²½ë ¥ ê°œë°œ
- RAG ì‹œìŠ¤í…œ ì•„í‚¤í…íŠ¸ë¡œì„œì˜ í¬íŠ¸í´ë¦¬ì˜¤ êµ¬ì¶•
- ì˜¤í”ˆì†ŒìŠ¤ RAG í”„ë¡œì íŠ¸ ê¸°ì—¬
- ê¸°ìˆ  ë¸”ë¡œê·¸ ë° ì»¨í¼ëŸ°ìŠ¤ ë°œí‘œ
- ê¸°ì—… ì»¨ì„¤íŒ… ê¸°íšŒ